{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53290bd-2900-455f-b019-d1e729bcc5b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Simply say- We are going to learn...\n",
    "next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8af1d8-5b6d-4f7b-8b89-48267f0a0c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **1. Data Munging (Data Cleanup)**\n",
    "\n",
    "Data munging is the process of converting raw, messy data into a clean, structured, and usable format.  \n",
    "It prepares data so that it can be safely used for downstream activities such as data transformation, enrichment, analytics, reporting, and data science or AI applications.<br>\n",
    "When data is first collected (from files, databases, APIs, logs, etc.), it is usually:\n",
    "- Inconsistent\n",
    "- Incomplete\n",
    "- Incorrect\n",
    "- Hard to analyze directly\n",
    "- Data munging fixes this\n",
    "\n",
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13d429f-52f1-4553-815b-c7242acab4b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**Passive Data Munging** <br>\n",
    "Passive data munging = understanding data without changing it<br>\n",
    "Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) <br>\n",
    "(every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "It involves exploring the data to understand its structure, attributes, quality, and patterns without modifying the data.<br>\n",
    "---\n",
    "Before cleaning or transforming data, we first need to examine the data to understand what it contains.\n",
    "This step helps answer key questions such as:\n",
    "\n",
    "- What columns exist in the dataset?\n",
    "- What is the data type of each column?\n",
    "- How much data is missing or null?\n",
    "- Are there duplicate records?\n",
    "- Are there any obvious patterns, outliers, or anomalies in the data?\n",
    "\n",
    "---\n",
    "##**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)\n",
    "\n",
    "## **Active Data Munging**\n",
    "\n",
    "Active data munging refers to the process of **actively modifying and transforming data** to make it clean, structured, and usable for downstream systems and users.\n",
    "\n",
    "1. **Combining Data & Schema Structuring**  \n",
    "   Integrating data from multiple sources while handling schema evolution and schema merging to produce a consistent and well-defined structure.\n",
    "\n",
    "2. **Validation, Cleansing, and Scrubbing**  \n",
    "   - **Validation**: Enforcing data quality rules and business constraints.  \n",
    "   - **Cleansing**: Removing unwanted, irrelevant, or corrupt datasets.  \n",
    "   - **Scrubbing**: Converting raw and inconsistent data into a tidy, standardized format.\n",
    "\n",
    "3. **De-duplication & Standardization**  \n",
    "   Removing duplicate records and applying appropriate levels of standardization so the data is consistent and usable for data engineers and downstream consumers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a383d97b-e062-4956-920b-54f4a4babb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Types of Passive data munging\n",
    "- Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba1b404-34b9-49e4-9225-49c4efb7374f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "####1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality \n",
    "- - Null columns are there\n",
    "- - duplicate rows\n",
    "- - format issues are there (age is not in number format eg. 7-7)\n",
    "- - Uniformity issues (Artist, artist)\n",
    "- - Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- - Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0aefad-e875-47e7-882a-15082f922d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "####2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8686c6e6-a2e2-42e1-a5a2-b8331bba8684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False,inferSchema=True).toDF(\"ID\",\"FName\",\"LName\",\"Age\",\"profession\")\n",
    "#rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))\n",
    "display(rawdf1.sample(.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e5f3be-5877-4ddf-ac3f-3a89cd09abe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "print(rawdf1.printSchema())#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(rawdf1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(rawdf1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "for i in rawdf1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0]);\n",
    "\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f5d6b9-7d02-4180-aae7-b6064bc47cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format\n",
    "#After Identifying the structType and structFiled we can modify the scheam easily like below\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "structschema=StructType([StructField('ID', StringType(), True), StructField('FName', StringType(), True), StructField('LName', StringType(), True), StructField('Age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "rawdf2=spark.read.schema(structschema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False)\n",
    "print(rawdf2.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "450be28e-793f-402c-a81b-7d2700c8384d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Actual count of the data:\",rawdf1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75648d99-fbae-4a9b-a703-ce4aa4b4d31e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c54eb4c-d392-4927-96ef-91a8ffa8cd0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. rawdf1.distinct()<br>\n",
    "\n",
    "**What it does**<br>\n",
    "- Removes fully identical rows<br>\n",
    "- Considers all columns in the DataFrame<br>\n",
    "- Keeps only one copy of each complete row<br>\n",
    "---\n",
    "Key behavior<br>\n",
    "Two rows are considered duplicates only if every column value matches.<br>\n",
    "\n",
    "cust_id | name   | country|\n",
    "|-------|--------|--------|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | US|\n",
    "\n",
    "|ust_id | name   | country|\n",
    "|-------|--------|-----|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | US|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac10b5f3-6f2a-4a3d-9052-ce28804cd23e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"De-duplicated record count (all columns using distinct):\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15001a97-0626-49ad-9a14-f9d724b6b6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"De-duplicated record count (all columns using dropDuplicates):\",rawdf1.dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e79404-be1c-439f-979b-bb68a0209be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##How many unique rows are there in this DataFrame?‚Äù\n",
    "print(rawdf1.dropDuplicates().count())\n",
    "|ID | Name|\n",
    "|---|---|\n",
    "|1  | A|\n",
    "|1  | A|\n",
    "|2  | B|\n",
    "\n",
    "- Spark removes repeated rows\n",
    "\n",
    "|ID | Name|\n",
    "|---|---|\n",
    "|1  | A|\n",
    "|2  | B|\n",
    "\n",
    "**When you do not pass any column names to dropDuplicates(), it behaves the same as distinct()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcbfeba-f227-4677-9ee4-d3bea4640c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"de-duplicated given id column count:\",rawdf1.dropDuplicates(['id']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8de865-8419-437e-8a47-a8cd2cb038c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## `dropDuplicates(['id'])` Behavior in Spark\n",
    "\n",
    "### **What Spark Does:**\n",
    "- **Only looks at the `id` column** when checking for duplicates\n",
    "- If multiple rows have the same `id` value:\n",
    "  - Keeps **one** row (the first occurrence by default)\n",
    "  - Drops **all other** rows with that same `id`\n",
    "- **Other columns are completely ignored** for duplicate checking\n",
    "- Returns a DataFrame with unique `id` values\n",
    "\n",
    "---\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "**Input DataFrame:**\n",
    "```python\n",
    "|-----|-------|-------|\n",
    "| id  | name  | country|\n",
    "|-----|-------|-------|\n",
    "| 101 | Rahul | IN    |\n",
    "| 101 | Rahul | US    |  ‚Üê Different country, but same id\n",
    "| 101 | Amit  | UK    |  ‚Üê Different name, but same id\n",
    "| 102 | Priya | IN    |\n",
    "| 103 | Rahul | US    |\n",
    "\n",
    "\n",
    "\n",
    "|-----|-------|-------|\n",
    "| id  | name  | country|\n",
    "|-----|-------|-------|\n",
    "| 101 | Rahul | IN    |  ‚Üê First occurrence kept, others dropped\n",
    "| 102 | Priya | IN    |\n",
    "| 103 | Rahul | US    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fffb52-f7a1-4cf3-af16-9d79ef1f1a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Describe**\n",
    "\n",
    "| Statistic | Meaning                   |\n",
    "| --------- | ------------------------- |\n",
    "| `count`   | Number of non-null values |\n",
    "| `mean`    | Average value             |\n",
    "| `stddev`  | Standard deviation        |\n",
    "| `min`     | Minimum value             |\n",
    "| `max`     | Maximum value             |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5025a219-cdd7-4609-afbf-22e68b426f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9afdf71-b525-4130-b97b-3975f4ac4b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**summary() is similar to describe(), but more flexible and informative**\n",
    "\n",
    "| Statistic | Meaning                   |\n",
    "| --------- | ------------------------- |\n",
    "| `count`   | Number of non-null values |\n",
    "| `mean`    | Average                   |\n",
    "| `stddev`  | Standard deviation        |\n",
    "| `min`     | Minimum value             |\n",
    "| `25%`     | 1st quartile              |\n",
    "| `50%`     | Median                    |\n",
    "| `75%`     | 3rd quartile              |\n",
    "| `max`     | Maximum value             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540afe5a-28d3-4940-93d3-9fd985e6c582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d73b956-39c2-4eba-8e0b-bd35dcdce938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ccffe7-058a-4679-9f7a-76d693360c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession#15lakhs\n",
    "spark=SparkSession.builder.appName(\"WD36 - ETL Pipeline - Bread & Butter\").getOrCreate()#3 lakhs LOC by Databricks (for eg. display, delta, xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63eb20c8-091a-4c16-9e9d-bf4bf474edc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Combining Data + Schema Evolution/Merging (Structuring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8be7be0-62a4-4cee-8117-7f9111235969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\")\n",
    "#2. Multiple files (with same or different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/sub_paths/\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05326ec0-8318-4e9e-846b-78a04f194d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**When do we go for Schema Evolution?**<br>\n",
    "Over the time, if no. of col are keep added by source<br>\n",
    "Serialization  while writing+ mergeSchema while reading<br>\n",
    "**When do we go for Schema Merging?**<br>\n",
    "In a given day, If we get multiple files of related (not same) structure<br>\n",
    "After reading in dataframe format -> unionByName + allowMissingColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8341ac1-97ea-40bf-ac16-10bf1dc2a83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#When you go for Schema Merging/Melting and Schema Evolution?\n",
    "#Schema Merging/Melting (unionByName,allowMissingColumns)- If we get multiple files\n",
    "#Schema Evolution (orc/parquet with mergeSchema) - If no. of columns are keeps added by the source system\n",
    "#when we know structure of the file already - schema merge/ schema not known earlier  - schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002e3c56-c575-4263-9706-3dd1a243ae47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Fisrt Understand the difference between Schema Merging/Melting and Schema Evolution?**<br>\n",
    "\n",
    "In short we can say Take all columns from all files and create a superset schema<br>\n",
    "Example<br>\n",
    "FILE1:-<br>\n",
    "|id |name|\n",
    "|---|----|\n",
    "|1|John|\n",
    "\n",
    "FILE2:-\n",
    "|id|age|\n",
    "|--|---|\n",
    "|2|30|\n",
    "\n",
    "After schema merging<br>\n",
    "|id | name | age|\n",
    "|---|------|------|\n",
    "|1  | John | null|\n",
    "|2  | null | 30|\n",
    "\n",
    "Ask ONLY ONE QUESTION first:<br>\n",
    "üëâ Am I combining multiple DataFrames in my Spark code?<br>\n",
    "OR<br>\n",
    "üëâ Am I reading/writing data from storage (Parquet/ORC/Delta)?<br>\n",
    "\n",
    "1Ô∏è‚É£ If you are COMBINING DataFrames ‚Üí Schema Merging (unionByName)<br>\n",
    "\n",
    "Use this when:<br>\n",
    "- You already have two or more DataFrames<br>\n",
    "- Columns are not exactly the same<br>\n",
    "- You want to append rows<br>\n",
    "- Missing columns should become null<br>\n",
    "Example scenario:<br>\n",
    "Day 1 file ‚Üí id, name<br>\n",
    "Day 2 file ‚Üí id, name, salary<br>\n",
    "\n",
    "You read both separately and want one dataset.<br>\n",
    "\n",
    "USE:- df1.unionByName(df2, allowMissingColumns=True)<br>\n",
    "\n",
    "Think like this:<br>\n",
    "**I already have data in memory, now I want to merge rows**<br>\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ If you are READING / WRITING data from storage ‚Üí Schema Evolution<br>\n",
    "Use this when:<br>\n",
    "- Data is stored in Parquet / ORC / Delta<br>\n",
    "- New columns appear over time<br>\n",
    "- You are reading or appending data<br>\n",
    "- You want Spark to auto-handle schema change<br>\n",
    "\n",
    "Example scenario:<br>\n",
    "Day 1 parquet ‚Üí id, name<br>\n",
    "Day 10 parquet ‚Üí id, name, salary<br>\n",
    "\n",
    "You read from the same folder.<br>\n",
    "\n",
    "‚úÖ Use:spark.read.option(\"mergeSchema\", \"true\").parquet(path)<br>\n",
    "or during write:<br>\n",
    "df.write.option(\"mergeSchema\", \"true\").mode(\"append\").parquet(path)<br>\n",
    "Think like this:<br>\n",
    "\n",
    "‚ÄúData structure changed in storage over time.‚Äù<br>\n",
    "\n",
    "Interview Answer (Perfect & Short)<br>\n",
    "\n",
    "Use unionByName when combining multiple DataFrames with different columns.<br>\n",
    "Use mergeSchema when schema evolves over time in Parquet/ORC/Delta files<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351aa2ab-e54c-4c62-84dd-45607a3dc2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#COMBINING OR SCHEMA MERGING or SCHEMA MELTING of Data from different sources(Important interview question also as like schema evolution...)\n",
    "#4. Multiple files with different structure in multiple paths or sub paths\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/sub_paths/\"],recursiveFileLookup=True,pathGlobFilter='custsmodified_N*')\n",
    "print(rawdf1.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c20290-cc7d-42cc-97d5-d9f0a0f94c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/sub_paths/\"],recursiveFileLookup=True,pathGlobFilter='custsmodified_T*')\n",
    "rawdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665a2605-0e94-4c80-87ee-7957e9f29308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1)\n",
    "display(rawdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b5bedd-5e8b-4d0b-b6f9-fab2310854ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf_merged=rawdf1.union(rawdf2)#Use union only if the dataframes are having same columns in the same order with same datatype..\n",
    "rawdf_merged.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f31976d-5084-4df7-8f38-14c34a4d498e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Expected right approach to follow here is\n",
    "rawdf_merged_unionbynme=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged_unionbynme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223cc798-c37c-4965-a413-65931493494a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Here, we are merging two files because both are in CSV format. If one file is CSV and the other file is in a different format, what should we do in this scenario? it will be handled automatically\n",
    "#rawdf2.write.json(\"/Volumes/workspace/wd36schema/ingestion_volume/staging/csvjson\")\n",
    "rawdf3=spark.read.json(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_json/\")\n",
    "rawdf_json_csv=rawdf_merged_unionbynme.unionByName(rawdf3,allowMissingColumns=True)\n",
    "display(rawdf_json_csv)\n",
    "\n",
    "#Even if source files are in different formats like CSV and JSON, Spark first converts them into DataFrames. Once they are DataFrames, unionByName can be used to merge them based on column names. File format differences are handled during the read phase, not during union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5fd616-6a6f-4aaf-9a47-8acef8c11d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Just for the simple learning of schema evolution & schema merging/melting<br>\n",
    "Schema merging/melting<br>\n",
    "1,rajeshwari day1(source1)<br>\n",
    "1,rajeshwari,30 day1(source2)<br>\n",
    "\n",
    "Schema evolution<br>\n",
    "1,rajeshwari day1<br>\n",
    "1,rajeshwari,30 day2<br>\n",
    "\n",
    "Output is same in both cases...<br>\n",
    "| id | name        | age |\n",
    "|----|-------------|-----|\n",
    "| 1  | rajeshwari  | null|\n",
    "| 1  | rajeshwari  | 30  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fafc669-6980-486b-b2ce-5f87a2a43a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769f895b-13e8-4b7c-9ccf-7c18372cba63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rawdf1.printSchema())##I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "display(rawdf1.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941df628-48fa-4f6f-afc6-1d3a2e79eb6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validation by doing cleansing\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,ShortType\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleaneddf1=spark.read.schema(struttype1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')\n",
    "print(\"after keeping nulls on the wrong data format\",cleaneddf1.count())#all rows count\n",
    "display(cleaneddf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)\n",
    "\n",
    "#or\n",
    "#method2 - drop malformed rows\n",
    "cleaneddf2=spark.read.schema(struttype1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='dropmalformed')\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\",len(cleaneddf2.collect()))\n",
    "display(cleaneddf2)#We are removing the entire row, where ever data format mismatch is there (throwing away the entire potato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089be61d-d3ec-4eda-8281-afe7faeb9b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method3 best methodology of applying active data munging\n",
    "#Validation by doing cleansing (not at the time of creating Dataframe, rather we will clean and scrub subsequently)...\n",
    "struttype3=StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "rawdf4=spark.read.schema(struttype3).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')\n",
    "print(\"allow all data showing the real values\",rawdf4.count())#all rows count\n",
    "display(rawdf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12114213-81f3-4fa3-ab73-fd7bf5dddfb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#####Rejection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea49fd0-b76f-4ce7-bce7-251db59ec0c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "strcut1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "cleaneddf1=spark.read.schema(strcut1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')\n",
    "display(cleaneddf1)\n",
    "corrupted_data_set=cleaneddf1.where(\"corruptedrows is not NULL\")\n",
    "corrupted_data_set.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/rejected_data\",header=True,mode='overwrite')\n",
    "retained_data=cleaneddf1.where(\"corruptedrows is NULL\")\n",
    "print(\"Overall rows in the source data is \",len(cleaneddf1.collect()))\n",
    "print(\"Rejected rows in the source data is \",len(corrupted_data_set.collect()))\n",
    "print(\"Clean rows in the source data is \",len(retained_data.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58eebba-1c51-47a1-a0f3-10532360b269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing \n",
    "na.drop()<br>\n",
    "It is a process of cleaning/removing/deleting unwanted data\n",
    "Eg. I am purchasing potato from a shop, I am cutting down the debris/rotten portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9064eba-f634-4433-8e39-8c93f281146b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2\n",
    "#Important na functions we can use to do cleansing\n",
    "cleanseddf1=rawdf4.na.drop(how=\"any\")#It drops (removes) any row from rawdf1 that has at least one null value in any column ===== any\" means:Drop the row if ANY column in that row is null\n",
    "#cleanseddf = rawdf1.na.drop(how=\"all\")#Drop only if ALL columns are null\n",
    "#rawdf1.na.drop(subset=[\"id\", \"name\"])#Drop based on specific columns\n",
    "print(\"any one row in the rawdf4 with age null\")\n",
    "display(rawdf4.where(\"age is null\"))\n",
    "print(\"any one row in the cleansed df with age null\")\n",
    "display(cleanseddf1.where(\"age is null\"))\n",
    "print(\"After Drop the row if ANY column in that row is null:\", cleanseddf1.count())\n",
    "display(rawdf4.where('age is NULL')) #here age column has some NULL values\n",
    "display(cleanseddf1.where('age is null')) #Here No rows are returned because on the DF we arlready did na.drop()\n",
    "cleanseddf2=rawdf4.na.drop(how=\"any\",subset=['id','age'])\n",
    "print(\"After removing subset:\",len(cleanseddf2.collect()))\n",
    "cleanseddf3=rawdf4.na.drop(how=\"all\",subset=[\"lastname\",\"profession\"])\n",
    "print(\"After removing subset and all:\",len(cleanseddf3.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24fa975b-4a0d-499e-9e13-3d1c03acf4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Scrubbing \n",
    "na.fill() & na.replace()<br>\n",
    "It is a process of polishing/fine tuning/scrubbing/meaningful conversion the data in a usable format\n",
    "Eg. I am purchasing potato from a shop, I am scrubbing/washing mud/sand portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ec3c0e-ea2d-4954-a28a-96ffc07e4984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleandf5=rawdf4.na.fill('Not Provided',subset=[\"lastname\"])\n",
    "#display(cleandf5)\n",
    "df1=rawdf4.na.fill({\"firstname\":\"unkown\",\"lastname\":\"unkown\"})#Replace nulls with a fixed value.\n",
    "#display(df1)\n",
    "df2=rawdf4.fillna('NA',subset=[\"firstname\"])\n",
    "display(df2)\n",
    "find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "find_replace_values_dict2={'not provided':'NA'}\n",
    "scrubbeddf2=rawdf4.na.replace(find_replace_values_dict1,subset=[\"profession\"])#fill function is helping us find and replace the values\n",
    "#df.na.replace(to_replace, value, subset=None)\n",
    "display(scrubbeddf2)\n",
    "scrubbeddf3=rawdf4.na.replace(find_replace_values_dict2,subset=[\"lastname\"])\n",
    "display(scrubbeddf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7114abf8-bef3-4222-9456-c158b7ab8037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####DeDuplication\n",
    "Removal of duplicate rows/columns based on a priority or non priority\n",
    "distinct & dropDuplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86b8eff-0c38-47ba-936b-f836ddf15e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "strcut1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "cleaneddf1=spark.read.schema(strcut1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331887c0-f532-4031-a7b8-b561bbd2849a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767025171963}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaneddf1.where(\"id in (4000001)\"))#before row level dedup\n",
    "row_dup_removed=cleaneddf1.distinct()#It will remove the row level duplicates\n",
    "display(row_dup_removed.where(\"id in (4000001)\"))\n",
    "\n",
    "print(\"non prioritized deduplication, just remove the duplicates retaining only the first row\")\n",
    "#row_dup_removed=cleaneddf1.distinct():-At this stage, the DataFrame may be distributed across multiple partitions.\n",
    "#.coalesce(1) Reduces the number of partitions in the DataFrame to 1.coalesce(1) forces all data into a single partition.\n",
    "display(row_dup_removed.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "colmn_dup_removed=row_dup_removed.coalesce(1).dropDuplicates([\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(colmn_dup_removed.coalesce(1).where(\"id in ('4000003')\"))\n",
    "\n",
    "print(\"prioritized deduplication based on age\")\n",
    "display(row_dup_removed.coalesce(1).where(\"id in ('4000003')\"))\n",
    "colmn_dup_removed1=row_dup_removed.coalesce(1).orderBy(['id','age'],ascending=[True,False]).dropDuplicates(['id'])\n",
    "display(colmn_dup_removed1.coalesce(1).where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789c4e45-7753-4cd9-9cdc-5cc1ae51cc68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "####3. Standardization and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc10197b-82a1-48ab-a4b7-8703d237be2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#####Standardization - \n",
    "Making the data more standard by adding/removing/reordering columns as per the expected standard, unifying into expected format, converting the type as expected etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44eb5a03-693f-46c8-b1b0-801ed4ca6f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "######Standardization1 - Column Enrichment (Addition of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf9d4e8-a9b0-4395-90d0-db5c7f29e9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False,inferSchema=True).toDF(\"ID\",\"FName\",\"LName\",\"Age\",\"profession\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a22007b-bbaf-47b6-bf84-d86ba251cfd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "standarddf1=rawdf1.withColumn(\"sourcesystem\",lit(\"Retail\"))#This line adds a new column named sourcesystem to the DataFrame rawdf1, and assigns the constant value \"Retail\" to every row in that column.\n",
    "#display(standarddf1)\n",
    "#It converts a constant value into a Spark Column object so Spark can apply it to all rows.Without lit(), Spark would treat \"Retail\" as a column name and throw an error\n",
    "display(standarddf1.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4253b4-ba6a-4e42-b688-8b87a49dea67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "######Standardization2 - Column Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccf1d5d-1938-498c-9d47-56a201fdb96c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/uniformity/custsmodified.csv\",header=False,inferSchema=True).toDF(\"ID\",\"FName\",\"LName\",\"Age\",\"profession\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94367a63-f082-45cc-bc1a-7f99c46b7376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rawdf1.createOrReplaceTempView(\"sqlview\")#This line registers the DataFrame standarddf1 as a temporary SQL view named sqlview, so that you can query it using Spark SQL.After this, you can run SQL querie\n",
    "\n",
    "display(spark.sql(\"\"\"select profession,\n",
    "                  count(*) as ToatlCount\n",
    "                  from sqlview \n",
    "                  group by profession \n",
    "                  order by profession\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83920df4-4f7f-41d3-8b07-8c1899fc023d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "display(rawdf1.groupBy(\"profession\").count().orderBy(\"profession\", ascending=True))#DSL\n",
    "#Standardization2 - column uniformity\n",
    "Standardization2_df=rawdf1.withColumn(\"profession\",initcap(\"profession\"))#inicap or any other string function with columnOr name can accept either column or string type provided if the string is a column name for eg. profession/age/sourcesystem.\n",
    "#->Using column name as a string [initcap(\"profession\")]----Spark internally converts \"profession\" ‚Üí col(\"profession\")\n",
    "#and initcap(col(\"profession\"))---Both produce exactly the same result.\n",
    "#Spark SQL functions are designed to accept:\n",
    "\n",
    "#Column objects OR column names as strings (auto-resolved)\n",
    "#So internally this happens:initcap(\"profession\")  ‚Üí  initcap(col(\"profession\"))\n",
    "\n",
    "display(Standardization2_df.groupBy(\"profession\").count().orderBy(\"profession\", ascending=True))\n",
    "display(Standardization2_df.limit(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0e21bd-111b-402f-a7aa-678bbc639ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "######Standardization3 - Format Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03930901-8cc9-4a78-8bd7-ee3d41734f2b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ID\":142},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767117017258}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Did analysis to understand the format issues in our id and age columns\n",
    "display(Standardization2_df.where(\"id like 't%'\"))\n",
    "Standardization2_df.where(\"id rlike '[a-zA-Z]'\").show()#rlike is regular expression like function that help us identify any string data in our DF column\n",
    "Standardization2_df.where(\"age rlike '[^0-9]'\").show()#checking for any non number values in age column\n",
    "#standarddf3=Standardization2_df.withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66dd4c7d-ba32-4662-b83f-81b188eea503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,replace\n",
    "#Let's apply scrubbing features to our id column to replace ten with 10 (or we can think of using GenAI here)\n",
    "replaceval={'one':'1','two':'2','three':'3','four':'4','five':'5','six':'6','seven':'7','eight':'8','nine':'9','ten':'10'}\n",
    "Standardization3_df=Standardization2_df.na.replace(replaceval,['id'])\n",
    "\n",
    "#standarddf3=standarddf2.withColumn(\"id\",replace(\"id\",lit('ten'),\"10\"))\n",
    "#Here replace(\"id\", \"ten\", \"10\")\n",
    "#\"id\" ‚Üí column name\n",
    "#\"ten\" ‚Üí value to search\n",
    "#\"10\" ‚Üí replacement value\n",
    "standarddf4=Standardization3_df.withColumn(\"age\",regexp_replace(\"age\",'-',\"\"))\n",
    "display(standarddf4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e09fbe-2dcb-4603-b8e3-4b1d4bb22851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "######Standardization4 - Data Type Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9a8a47-cc80-4a30-ac9e-e5502908e7b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "print(standarddf4.printSchema())#Still id and age are string type, though it contains int data\n",
    "standarddf4=standarddf4.withColumn(\"id\",col(\"id\").cast(\"long\"))\n",
    "standarddf4=standarddf4.withColumn(\"id\",standarddf4[\"id\"].cast(\"long\"))\n",
    "standarddf4=standarddf4.withColumn(\"age\",col(\"age\").cast(\"short\"))\n",
    "print(standarddf4.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04b5f07-2610-4992-b061-b5f26f3c1f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "######Standardization5 - Naming Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bafad3ba-ce8b-48b0-8f39-49294ea95326",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767120358433}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf5=standarddf4.withColumnRenamed(\"id\",\"custid\")\n",
    "standarddf5=standarddf4.withColumnsRenamed({\"id\":\"custid\",\"age\":\"Age\"})\n",
    "display(standarddf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53218bbd-955a-48c9-a970-76731036d030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization6 - Reorder Standadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46038d80-665d-4ad2-83ab-2c7682bf6ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf6=standarddf5.select(\"custid\", \"Age\", \"FName\",\"LName\",\"profession\")\n",
    "#display(standarddf6)\n",
    "display(standarddf6.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5976e9b6-fc48-454c-a012-ddae1d9ed71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Before starting Data Enrichment or before sharing the data to the consumer, we have to do EDA/Exploration/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b0ef51-6e42-4aba-a459-f35807051a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf6.printSchema()\n",
    "display(standarddf6.take(20))\n",
    "display(\"total rows\",len(standarddf6.collect()))\n",
    "display(standarddf6.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f07461a-151c-4cc5-bdc6-a6c4bbee7ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##**2. Data Enrichment** - Detailing of data\n",
    "Makes your data rich and detailed <br>\n",
    "a. Add (), Derive (), Remove/Elimi), Rename (), Modify/replace () - very important spark sql functions <br>\n",
    "b. split, merge/Concat <br>\n",
    "c. Type Casting, reformat & Schema Migration <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58911383-3171-42a6-93e1-464a8936bb46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#####a. Add (), Derive (), Rename (), Modify/replace (), Remove/Eliminate () - very important spark sql DF functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393df97b-a256-4254-b210-b7112dd89ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Adding of columns\n",
    "Lets add datadt (date of the data orgniated from the source for eg. provided in the filename in a format of yy/dd/MM) and loaddt (date when we are loading the data into our system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f98dabf-20bb-4e46-a0c5-0b6a25a2b3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "derived_datadt='25/30/12'\n",
    "print(f\"hello '{derived_datadt}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f5293d-0f44-4eda-907e-1b344e3031db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,current_date\n",
    "original_filename='custsmodified_25/30/12.csv'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "derived_datadt=original_filename.split('_')[1].split('.')[0]\n",
    "print(\"the derived date is:\",derived_datadt)\n",
    "#derived_datadt='25/30/12'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "#or\n",
    "df1=standarddf6.withColumn(\"loaddt\",lit(\"25/30/12\")).withColumn(\"datadt\",current_date())#withColumn() is used to add or replace ONE column at a time.\n",
    "df1.printSchema()\n",
    "display(df1.take(5))\n",
    "#or\n",
    "df2=standarddf6.withColumns({\"loaddt\":lit(\"25/30/12\"),\"datadt\":current_date()})#withColumns() is used to add or replace multiple columns in a single call.\n",
    "display(df2.take(2))\n",
    "#or\n",
    "df3=standarddf6.select(\"*\",lit(derived_datadt).alias('datadt'),current_date().alias('loaddt'))#DSLs (FBP function)\n",
    "display(df3.take(3))\n",
    "#or\n",
    "df4=standarddf6.selectExpr(\"*\",\"'25/30/12' as loaddt\",\"current_date() as loaddt\")#)#DSL(select) + SQL expression\n",
    "display(df4.take(4))\n",
    "df5=standarddf6.selectExpr(\"*\",f\"'{derived_datadt}' as datadt\",\"current_date() as loaddt\")\n",
    "display(df5.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58eb0bf1-4354-410f-9b2c-65be4328fb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Deriving of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ae4f14-ccf5-44c3-aa10-ba084015cbd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df1=df5.withColumn(\"professionFlag\",substring(\"profession\",1,3))\n",
    "display(df1.take(2))\n",
    "df2=df5.select(\"*\",substring(\"profession\",1,1).alias(\"professionflag\"))\n",
    "display(df2.take(3))\n",
    "df3=df5.selectExpr(\"*\",\"substr(profession,1,1) as professionflag\")\n",
    "display(df3.take(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32aaf5ae-be19-42fd-8715-7f334336f2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "######Renaming of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a076df-5b08-4d9d-9862-7a7d476e48b8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767260005817}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Can we use withColumn to rename? not directly, its costly\n",
    "df3.printSchema()\n",
    "df1=df3.withColumn(\"load_date\",col(\"loaddt\"))\n",
    "df1=df1.drop(\"loaddt\").select(\"custid\",\"FName\",\"LName\",\"Age\",\"profession\",\"professionflag\",\"datadt\",\"load_date\")\n",
    "display(df1.take(3))\n",
    "\n",
    "#or\n",
    "\n",
    "df2=df3.select(\"custid\",\"FName\",\"LName\",\"Age\",\"profession\",\"professionflag\",\"datadt\",col(\"loaddt\").alias(\"load_date\"))\n",
    "display(df2.take(3))#costly too, since we have to choose all columns in the select\n",
    "\n",
    "#or\n",
    "\n",
    "df2=df3.selectExpr(\"custid\",\"FName\",\"LName\",\"Age\",\"profession\",\"professionflag\",\"datadt\",\"loaddt as load_date\")\n",
    "display(df2.take(3))#costly too, since we have to choose all columns in the select\n",
    "\n",
    "#or\n",
    "\n",
    "df2=df3.withColumnRenamed(\"loaddt\",\"load_data\")#Best function to rename the column(s)\n",
    "display(df2.take(3))\n",
    "df2=df3.withColumnsRenamed({\"loaddt\":\"load_data\",\"datadt\":\"dataload\"})\n",
    "display(df2.take(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d37ef8c-88cc-46ac-bfbe-43f31e4c87ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Modify/replace (withColumn, select/selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ed8b91-fa07-468f-bdb2-89f72c26ee84",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767261725333}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=df3.withColumn(\"FName\",col(\"LName\"))#This will replace the LName with Fname\n",
    "display(df3.take(3))\n",
    "display(df1.take(3))\n",
    "\n",
    "df1=df3.withColumn(\"profession\",concat(\"profession\",lit('-'),\"professionflag\"))#This will modify/enrich the profession column with sourcename\n",
    "display(df1.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a4fefe2-3369-4c94-af7a-acadffb4179a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "######Remove/Eliminate (drop,select,selectExpr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045b7142-41f4-434a-b61e-7cd64537e81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.printSchema()\n",
    "df2=df1.select(\"custid\",\"Age\",\"FName\",\"LName\",\"profession\",\"loaddt\",\"datadt\")\n",
    "display(df2.take(3))\n",
    "#or use selectExpr (yes, but costly)\n",
    "df2=df1.selectExpr(\"custid\",\"Age\",\"FName\",\"LName\",\"profession\",\"loaddt\",\"datadt\")\n",
    "display(df2.take(3))\n",
    "#or\n",
    "df2=df1.drop(\"professionflag\")\n",
    "display(df2.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a56eb184-16d8-40f1-8395-4de88bf42b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "######Conclusion/Best practices of using different column enrichment functions\n",
    "1. select is good to use if we want to perform - \n",
    "Good for ordering/reordering, only renaming column (not good), only reformatting/deriving a column (not good), **for all of these operation in a single iteration** such renaming, reordering, reformatting,deriving, dropping etc., (best to use)\n",
    "2. selectExpr is good to use if we want to perform - Same as select by using iso sql functionality (if we are not familiar in DSL) **for all of these operation in a single iteration**\n",
    "3. withColumn is good to use if we want to perform - \n",
    "**for adding/deriving/modifying/replacing in a single iteration**\n",
    "Adding/Deriving column(s) in the last (Good), Modifying/replacing (Good), Renaming (not good), Dropping(not possible)\n",
    "4. withColumnRenamed is good to use if we want to perform - only for renaming column (Good)\n",
    "5. drop is good to use if we want to perform - only dropping of columns in the given position (Good)\n",
    "\n",
    "----\n",
    "Big Picture First (Golden Rule)<br>\n",
    "üëâ Every DataFrame transformation creates a new DataFrame.<br>\n",
    "1Ô∏è‚É£ select()<br>\n",
    "Use select() when you want to:<br>\n",
    "- Reorder columns\n",
    "- Rename columns\n",
    "- Reformat columns\n",
    "- Derive new columns\n",
    "- Drop columns\n",
    "Do all of the above in a single iteration\n",
    "2Ô∏è‚É£ selectExpr()<br>\n",
    "üîπ What it is<br>\n",
    "Same power as select(), but written using SQL expressions instead of DSL.<br>\n",
    "üîπ Why use it<br>\n",
    "You are more comfortable with SQL<br>\n",
    "You want quick transformations using SQL syntax<br>\n",
    "\n",
    "3Ô∏è‚É£ withColumn() ‚Äî Add or Modify ONE Column\n",
    "üîπ What it is good for\n",
    "\n",
    "Adding a new column\n",
    "\n",
    "Modifying an existing column\n",
    "\n",
    "üîπ What it is NOT good for\n",
    "\n",
    "‚ùå Renaming many columns\n",
    "‚ùå Dropping columns\n",
    "‚ùå Heavy transformations (performance-wise)\n",
    "\n",
    "| Task                | Best Method           |\n",
    "| ------------------- | --------------------- |\n",
    "| Everything at once  | `select()`            |\n",
    "| SQL-style logic     | `selectExpr()`        |\n",
    "| Add / Modify column | `withColumn()`        |\n",
    "| Rename column       | `withColumnRenamed()` |\n",
    "| Drop column         | `drop()`              |\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_End_End_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
