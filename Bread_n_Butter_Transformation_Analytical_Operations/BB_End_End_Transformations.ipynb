{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53290bd-2900-455f-b019-d1e729bcc5b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Simply say- We are going to learn...\n",
    "next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8af1d8-5b6d-4f7b-8b89-48267f0a0c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **1. Data Munging (Data Cleanup)**\n",
    "\n",
    "Data munging is the process of converting raw, messy data into a clean, structured, and usable format.  \n",
    "It prepares data so that it can be safely used for downstream activities such as data transformation, enrichment, analytics, reporting, and data science or AI applications.<br>\n",
    "When data is first collected (from files, databases, APIs, logs, etc.), it is usually:\n",
    "- Inconsistent\n",
    "- Incomplete\n",
    "- Incorrect\n",
    "- Hard to analyze directly\n",
    "- Data munging fixes this\n",
    "\n",
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13d429f-52f1-4553-815b-c7242acab4b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**Passive Data Munging** <br>\n",
    "Passive data munging = understanding data without changing it<br>\n",
    "Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) <br>\n",
    "(every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "It involves exploring the data to understand its structure, attributes, quality, and patterns without modifying the data.<br>\n",
    "---\n",
    "Before cleaning or transforming data, we first need to examine the data to understand what it contains.\n",
    "This step helps answer key questions such as:\n",
    "\n",
    "- What columns exist in the dataset?\n",
    "- What is the data type of each column?\n",
    "- How much data is missing or null?\n",
    "- Are there duplicate records?\n",
    "- Are there any obvious patterns, outliers, or anomalies in the data?\n",
    "\n",
    "---\n",
    "##**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)\n",
    "\n",
    "## **Active Data Munging**\n",
    "\n",
    "Active data munging refers to the process of **actively modifying and transforming data** to make it clean, structured, and usable for downstream systems and users.\n",
    "\n",
    "1. **Combining Data & Schema Structuring**  \n",
    "   Integrating data from multiple sources while handling schema evolution and schema merging to produce a consistent and well-defined structure.\n",
    "\n",
    "2. **Validation, Cleansing, and Scrubbing**  \n",
    "   - **Validation**: Enforcing data quality rules and business constraints.  \n",
    "   - **Cleansing**: Removing unwanted, irrelevant, or corrupt datasets.  \n",
    "   - **Scrubbing**: Converting raw and inconsistent data into a tidy, standardized format.\n",
    "\n",
    "3. **De-duplication & Standardization**  \n",
    "   Removing duplicate records and applying appropriate levels of standardization so the data is consistent and usable for data engineers and downstream consumers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a383d97b-e062-4956-920b-54f4a4babb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Types of Passive data munging\n",
    "- Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba1b404-34b9-49e4-9225-49c4efb7374f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "####1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality \n",
    "- - Null columns are there\n",
    "- - duplicate rows\n",
    "- - format issues are there (age is not in number format eg. 7-7)\n",
    "- - Uniformity issues (Artist, artist)\n",
    "- - Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- - Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0aefad-e875-47e7-882a-15082f922d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "####2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8686c6e6-a2e2-42e1-a5a2-b8331bba8684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False,inferSchema=True).toDF(\"ID\",\"FName\",\"LName\",\"Age\",\"profession\")\n",
    "#rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))\n",
    "display(rawdf1.sample(.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e5f3be-5877-4ddf-ac3f-3a89cd09abe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "print(rawdf1.printSchema())#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(rawdf1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(rawdf1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "for i in rawdf1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0]);\n",
    "\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f5d6b9-7d02-4180-aae7-b6064bc47cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format\n",
    "#After Identifying the structType and structFiled we can modify the scheam easily like below\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "structschema=StructType([StructField('ID', StringType(), True), StructField('FName', StringType(), True), StructField('LName', StringType(), True), StructField('Age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "rawdf2=spark.read.schema(structschema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False)\n",
    "print(rawdf2.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "450be28e-793f-402c-a81b-7d2700c8384d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Actual count of the data:\",rawdf1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75648d99-fbae-4a9b-a703-ce4aa4b4d31e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c54eb4c-d392-4927-96ef-91a8ffa8cd0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. rawdf1.distinct()<br>\n",
    "\n",
    "**What it does**<br>\n",
    "- Removes fully identical rows<br>\n",
    "- Considers all columns in the DataFrame<br>\n",
    "- Keeps only one copy of each complete row<br>\n",
    "---\n",
    "Key behavior<br>\n",
    "Two rows are considered duplicates only if every column value matches.<br>\n",
    "\n",
    "cust_id | name   | country|\n",
    "|-------|--------|--------|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | US|\n",
    "\n",
    "|ust_id | name   | country|\n",
    "|-------|--------|-----|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | US|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac10b5f3-6f2a-4a3d-9052-ce28804cd23e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"De-duplicated record count (all columns using distinct):\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15001a97-0626-49ad-9a14-f9d724b6b6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"De-duplicated record count (all columns using dropDuplicates):\",rawdf1.dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e79404-be1c-439f-979b-bb68a0209be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##How many unique rows are there in this DataFrame?‚Äù\n",
    "print(rawdf1.dropDuplicates().count())\n",
    "|ID | Name|\n",
    "|---|---|\n",
    "|1  | A|\n",
    "|1  | A|\n",
    "|2  | B|\n",
    "\n",
    "- Spark removes repeated rows\n",
    "\n",
    "|ID | Name|\n",
    "|---|---|\n",
    "|1  | A|\n",
    "|2  | B|\n",
    "\n",
    "**When you do not pass any column names to dropDuplicates(), it behaves the same as distinct()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcbfeba-f227-4677-9ee4-d3bea4640c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"de-duplicated given id column count:\",rawdf1.dropDuplicates(['id']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8de865-8419-437e-8a47-a8cd2cb038c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## `dropDuplicates(['id'])` Behavior in Spark\n",
    "\n",
    "### **What Spark Does:**\n",
    "- **Only looks at the `id` column** when checking for duplicates\n",
    "- If multiple rows have the same `id` value:\n",
    "  - Keeps **one** row (the first occurrence by default)\n",
    "  - Drops **all other** rows with that same `id`\n",
    "- **Other columns are completely ignored** for duplicate checking\n",
    "- Returns a DataFrame with unique `id` values\n",
    "\n",
    "---\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "**Input DataFrame:**\n",
    "```python\n",
    "|-----|-------|-------+|\n",
    "| id  | name  | country|\n",
    "|-----|-------|-------|\n",
    "| 101 | Rahul | IN    |\n",
    "| 101 | Rahul | US    |  ‚Üê Different country, but same id\n",
    "| 101 | Amit  | UK    |  ‚Üê Different name, but same id\n",
    "| 102 | Priya | IN    |\n",
    "| 103 | Rahul | US    |\n",
    "\n",
    "\n",
    "\n",
    "|-----|-------|-------|\n",
    "| id  | name  | country|\n",
    "|-----|-------|-------|\n",
    "| 101 | Rahul | IN    |  ‚Üê First occurrence kept, others dropped\n",
    "| 102 | Priya | IN    |\n",
    "| 103 | Rahul | US    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fffb52-f7a1-4cf3-af16-9d79ef1f1a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Describe**\n",
    "\n",
    "| Statistic | Meaning                   |\n",
    "| --------- | ------------------------- |\n",
    "| `count`   | Number of non-null values |\n",
    "| `mean`    | Average value             |\n",
    "| `stddev`  | Standard deviation        |\n",
    "| `min`     | Minimum value             |\n",
    "| `max`     | Maximum value             |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5025a219-cdd7-4609-afbf-22e68b426f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9afdf71-b525-4130-b97b-3975f4ac4b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**summary() is similar to describe(), but more flexible and informative**\n",
    "\n",
    "| Statistic | Meaning                   |\n",
    "| --------- | ------------------------- |\n",
    "| `count`   | Number of non-null values |\n",
    "| `mean`    | Average                   |\n",
    "| `stddev`  | Standard deviation        |\n",
    "| `min`     | Minimum value             |\n",
    "| `25%`     | 1st quartile              |\n",
    "| `50%`     | Median                    |\n",
    "| `75%`     | 3rd quartile              |\n",
    "| `max`     | Maximum value             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540afe5a-28d3-4940-93d3-9fd985e6c582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d73b956-39c2-4eba-8e0b-bd35dcdce938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ccffe7-058a-4679-9f7a-76d693360c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession#15lakhs\n",
    "spark=SparkSession.builder.appName(\"WD36 - ETL Pipeline - Bread & Butter\").getOrCreate()#3 lakhs LOC by Databricks (for eg. display, delta, xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63eb20c8-091a-4c16-9e9d-bf4bf474edc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Combining Data + Schema Evolution/Merging (Structuring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8be7be0-62a4-4cee-8117-7f9111235969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\")\n",
    "#2. Multiple files (with same or different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/sub_paths/\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8341ac1-97ea-40bf-ac16-10bf1dc2a83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#When you go for Schema Merging/Melting and Schema Evolution?\n",
    "#Schema Merging/Melting (unionByName,allowMissingColumns)- If we get multiple files\n",
    "#Schema Evolution (orc/parquet with mergeSchema) - If no. of columns are keeps added by the source system\n",
    "#when we know structure of the file already - schema merge/ schema not known earlier  - schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002e3c56-c575-4263-9706-3dd1a243ae47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Fisrt Understand the difference between Schema Merging/Melting and Schema Evolution?**<br>\n",
    "\n",
    "In short we can say Take all columns from all files and create a superset schema<br>\n",
    "Example<br>\n",
    "FILE1:-<br>\n",
    "|id |name|\n",
    "|---|----|\n",
    "|1|John|\n",
    "\n",
    "FILE2:-\n",
    "|id|age|\n",
    "|--|---|\n",
    "|2|30|\n",
    "\n",
    "After schema merging<br>\n",
    "|id | name | age|\n",
    "|---|------|------|\n",
    "|1  | John | null|\n",
    "|2  | null | 30|\n",
    "\n",
    "Ask ONLY ONE QUESTION first:<br>\n",
    "üëâ Am I combining multiple DataFrames in my Spark code?<br>\n",
    "OR<br>\n",
    "üëâ Am I reading/writing data from storage (Parquet/ORC/Delta)?<br>\n",
    "\n",
    "1Ô∏è‚É£ If you are COMBINING DataFrames ‚Üí Schema Merging (unionByName)<br>\n",
    "\n",
    "Use this when:<br>\n",
    "- You already have two or more DataFrames<br>\n",
    "- Columns are not exactly the same<br>\n",
    "- You want to append rows<br>\n",
    "- Missing columns should become null<br>\n",
    "Example scenario:<br>\n",
    "Day 1 file ‚Üí id, name<br>\n",
    "Day 2 file ‚Üí id, name, salary<br>\n",
    "\n",
    "You read both separately and want one dataset.<br>\n",
    "\n",
    "USE:- df1.unionByName(df2, allowMissingColumns=True)<br>\n",
    "\n",
    "Think like this:<br>\n",
    "**I already have data in memory, now I want to merge rows**<br>\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ If you are READING / WRITING data from storage ‚Üí Schema Evolution<br>\n",
    "Use this when:<br>\n",
    "- Data is stored in Parquet / ORC / Delta<br>\n",
    "- New columns appear over time<br>\n",
    "- You are reading or appending data<br>\n",
    "- You want Spark to auto-handle schema change<br>\n",
    "\n",
    "Example scenario:<br>\n",
    "Day 1 parquet ‚Üí id, name<br>\n",
    "Day 10 parquet ‚Üí id, name, salary<br>\n",
    "\n",
    "You read from the same folder.<br>\n",
    "\n",
    "‚úÖ Use:spark.read.option(\"mergeSchema\", \"true\").parquet(path)<br>\n",
    "or during write:<br>\n",
    "df.write.option(\"mergeSchema\", \"true\").mode(\"append\").parquet(path)<br>\n",
    "Think like this:<br>\n",
    "\n",
    "‚ÄúData structure changed in storage over time.‚Äù<br>\n",
    "\n",
    "Interview Answer (Perfect & Short)<br>\n",
    "\n",
    "Use unionByName when combining multiple DataFrames with different columns.<br>\n",
    "Use mergeSchema when schema evolves over time in Parquet/ORC/Delta files<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351aa2ab-e54c-4c62-84dd-45607a3dc2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#COMBINING OR SCHEMA MERGING or SCHEMA MELTING of Data from different sources(Important interview question also as like schema evolution...)\n",
    "#4. Multiple files with different structure in multiple paths or sub paths\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/sub_paths/\"],recursiveFileLookup=True,pathGlobFilter='custsmodified_N*')\n",
    "print(rawdf1.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c20290-cc7d-42cc-97d5-d9f0a0f94c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/sub_paths/\"],recursiveFileLookup=True,pathGlobFilter='custsmodified_T*')\n",
    "rawdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665a2605-0e94-4c80-87ee-7957e9f29308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1)\n",
    "display(rawdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b5bedd-5e8b-4d0b-b6f9-fab2310854ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf_merged=rawdf1.union(rawdf2)#Use union only if the dataframes are having same columns in the same order with same datatype..\n",
    "rawdf_merged.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f31976d-5084-4df7-8f38-14c34a4d498e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Expected right approach to follow here is\n",
    "rawdf_merged_unionbynme=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged_unionbynme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223cc798-c37c-4965-a413-65931493494a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Here, we are merging two files because both are in CSV format. If one file is CSV and the other file is in a different format, what should we do in this scenario? it will be handled automatically\n",
    "#rawdf2.write.json(\"/Volumes/workspace/wd36schema/ingestion_volume/staging/csvjson\")\n",
    "rawdf3=spark.read.json(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_json/\")\n",
    "rawdf_json_csv=rawdf_merged_unionbynme.unionByName(rawdf3,allowMissingColumns=True)\n",
    "display(rawdf_json_csv)\n",
    "\n",
    "#Even if source files are in different formats like CSV and JSON, Spark first converts them into DataFrames. Once they are DataFrames, unionByName can be used to merge them based on column names. File format differences are handled during the read phase, not during union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5fd616-6a6f-4aaf-9a47-8acef8c11d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Just for the simple learning of schema evolution & schema merging/melting<br>\n",
    "Schema merging/melting<br>\n",
    "1,rajeshwari day1(source1)<br>\n",
    "1,rajeshwari,30 day1(source2)<br>\n",
    "\n",
    "Schema evolution<br>\n",
    "1,rajeshwari day1<br>\n",
    "1,rajeshwari,30 day2<br>\n",
    "\n",
    "Output is same in both cases...<br>\n",
    "| id | name        | age |\n",
    "|----|-------------|-----|\n",
    "| 1  | rajeshwari  | null|\n",
    "| 1  | rajeshwari  | 30  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fafc669-6980-486b-b2ce-5f87a2a43a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769f895b-13e8-4b7c-9ccf-7c18372cba63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rawdf1.printSchema())##I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "display(rawdf1.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941df628-48fa-4f6f-afc6-1d3a2e79eb6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validation by doing cleansing\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,ShortType\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleaneddf1=spark.read.schema(struttype1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')\n",
    "print(\"after keeping nulls on the wrong data format\",cleaneddf1.count())#all rows count\n",
    "display(cleaneddf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)\n",
    "\n",
    "#or\n",
    "#method2 - drop malformed rows\n",
    "cleaneddf2=spark.read.schema(struttype1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='dropmalformed')\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\",len(cleaneddf2.collect()))\n",
    "display(cleaneddf2)#We are removing the entire row, where ever data format mismatch is there (throwing away the entire potato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089be61d-d3ec-4eda-8281-afe7faeb9b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method3 best methodology of applying active data munging\n",
    "#Validation by doing cleansing (not at the time of creating Dataframe, rather we will clean and scrub subsequently)...\n",
    "struttype3=StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "rawdf4=spark.read.schema(struttype3).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')\n",
    "print(\"allow all data showing the real values\",rawdf4.count())#all rows count\n",
    "display(rawdf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12114213-81f3-4fa3-ab73-fd7bf5dddfb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#####Rejection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea49fd0-b76f-4ce7-bce7-251db59ec0c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "strcut1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "cleaneddf1=spark.read.schema(strcut1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')\n",
    "display(cleaneddf1)\n",
    "corrupted_data_set=cleaneddf1.where(\"corruptedrows is not NULL\")\n",
    "corrupted_data_set.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/rejected_data\",header=True,mode='overwrite')\n",
    "retained_data=cleaneddf1.where(\"corruptedrows is NULL\")\n",
    "print(\"Overall rows in the source data is \",len(cleaneddf1.collect()))\n",
    "print(\"Rejected rows in the source data is \",len(corrupted_data_set.collect()))\n",
    "print(\"Clean rows in the source data is \",len(retained_data.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58eebba-1c51-47a1-a0f3-10532360b269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing \n",
    "na.drop()<br>\n",
    "It is a process of cleaning/removing/deleting unwanted data\n",
    "Eg. I am purchasing potato from a shop, I am cutting down the debris/rotten portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9064eba-f634-4433-8e39-8c93f281146b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2\n",
    "#Important na functions we can use to do cleansing\n",
    "cleanseddf1=rawdf4.na.drop(how=\"any\")#It drops (removes) any row from rawdf1 that has at least one null value in any column ===== any\" means:Drop the row if ANY column in that row is null\n",
    "#cleanseddf = rawdf1.na.drop(how=\"all\")#Drop only if ALL columns are null\n",
    "#rawdf1.na.drop(subset=[\"id\", \"name\"])#Drop based on specific columns\n",
    "print(\"any one row in the rawdf4 with age null\")\n",
    "display(rawdf4.where(\"age is null\"))\n",
    "print(\"any one row in the cleansed df with age null\")\n",
    "display(cleanseddf1.where(\"age is null\"))\n",
    "print(\"After Drop the row if ANY column in that row is null:\", cleanseddf1.count())\n",
    "display(rawdf4.where('age is NULL')) #here age column has some NULL values\n",
    "display(cleanseddf1.where('age is null')) #Here No rows are returned because on the DF we arlready did na.drop()\n",
    "cleanseddf2=rawdf4.na.drop(how=\"any\",subset=['id','age'])\n",
    "print(\"After removing subset:\",len(cleanseddf2.collect()))\n",
    "cleanseddf3=rawdf4.na.drop(how=\"all\",subset=[\"lastname\",\"profession\"])\n",
    "print(\"After removing subset and all:\",len(cleanseddf3.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24fa975b-4a0d-499e-9e13-3d1c03acf4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Scrubbing \n",
    "na.fill() & na.replace()<br>\n",
    "It is a process of polishing/fine tuning/scrubbing/meaningful conversion the data in a usable format\n",
    "Eg. I am purchasing potato from a shop, I am scrubbing/washing mud/sand portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ec3c0e-ea2d-4954-a28a-96ffc07e4984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleandf5=rawdf4.na.fill('Not Provided',subset=[\"lastname\"])\n",
    "#display(cleandf5)\n",
    "df1=rawdf4.na.fill({\"firstname\":\"unkown\",\"lastname\":\"unkown\"})#Replace nulls with a fixed value.\n",
    "#display(df1)\n",
    "df2=rawdf4.fillna('NA',subset=[\"firstname\"])\n",
    "display(df2)\n",
    "find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "find_replace_values_dict2={'not provided':'NA'}\n",
    "scrubbeddf2=rawdf4.na.replace(find_replace_values_dict1,subset=[\"profession\"])#fill function is helping us find and replace the values\n",
    "display(scrubbeddf2)\n",
    "scrubbeddf3=rawdf4.na.replace(find_replace_values_dict2,subset=[\"lastname\"])\n",
    "display(scrubbeddf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7114abf8-bef3-4222-9456-c158b7ab8037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####DeDuplication\n",
    "Removal of duplicate rows/columns based on a priority or non priority\n",
    "distinct & dropDuplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86b8eff-0c38-47ba-936b-f836ddf15e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "strcut1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "cleaneddf1=spark.read.schema(strcut1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331887c0-f532-4031-a7b8-b561bbd2849a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767025171963}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaneddf1.where(\"id in (4000001)\"))#before row level dedup\n",
    "row_dup_removed=cleaneddf1.distinct()#It will remove the row level duplicates\n",
    "display(row_dup_removed.where(\"id in (4000001)\"))\n",
    "\n",
    "print(\"non prioritized deduplication, just remove the duplicates retaining only the first row\")\n",
    "#row_dup_removed=cleaneddf1.distinct():-At this stage, the DataFrame may be distributed across multiple partitions.\n",
    "#.coalesce(1) Reduces the number of partitions in the DataFrame to 1.coalesce(1) forces all data into a single partition.\n",
    "display(row_dup_removed.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "colmn_dup_removed=row_dup_removed.coalesce(1).dropDuplicates([\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(colmn_dup_removed.coalesce(1).where(\"id in ('4000003')\"))\n",
    "\n",
    "print(\"prioritized deduplication based on age\")\n",
    "display(row_dup_removed.coalesce(1).where(\"id in ('4000003')\"))\n",
    "colmn_dup_removed1=row_dup_removed.coalesce(1).orderBy(['id','age'],ascending=[True,False]).dropDuplicates(['id'])\n",
    "display(colmn_dup_removed1.coalesce(1).where(\"id in ('4000003')\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_End_End_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
