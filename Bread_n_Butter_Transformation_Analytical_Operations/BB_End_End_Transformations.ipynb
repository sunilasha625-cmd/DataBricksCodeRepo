{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d53290bd-2900-455f-b019-d1e729bcc5b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Simply say- We are going to learn...\n",
    "next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb8af1d8-5b6d-4f7b-8b89-48267f0a0c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **1. Data Munging (Data Cleanup)**\n",
    "\n",
    "Data munging is the process of converting raw, messy data into a clean, structured, and usable format.  \n",
    "It prepares data so that it can be safely used for downstream activities such as data transformation, enrichment, analytics, reporting, and data science or AI applications.<br>\n",
    "When data is first collected (from files, databases, APIs, logs, etc.), it is usually:\n",
    "- Inconsistent\n",
    "- Incomplete\n",
    "- Incorrect\n",
    "- Hard to analyze directly\n",
    "- Data munging fixes this\n",
    "\n",
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f13d429f-52f1-4553-815b-c7242acab4b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**Passive Data Munging** <br>\n",
    "Passive data munging = understanding data without changing it<br>\n",
    "Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) <br>\n",
    "(every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "It involves exploring the data to understand its structure, attributes, quality, and patterns without modifying the data.<br>\n",
    "---\n",
    "Before cleaning or transforming data, we first need to examine the data to understand what it contains.\n",
    "This step helps answer key questions such as:\n",
    "\n",
    "- What columns exist in the dataset?\n",
    "- What is the data type of each column?\n",
    "- How much data is missing or null?\n",
    "- Are there duplicate records?\n",
    "- Are there any obvious patterns, outliers, or anomalies in the data?\n",
    "\n",
    "---\n",
    "##**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)\n",
    "\n",
    "## **Active Data Munging**\n",
    "\n",
    "Active data munging refers to the process of **actively modifying and transforming data** to make it clean, structured, and usable for downstream systems and users.\n",
    "\n",
    "1. **Combining Data & Schema Structuring**  \n",
    "   Integrating data from multiple sources while handling schema evolution and schema merging to produce a consistent and well-defined structure.\n",
    "\n",
    "2. **Validation, Cleansing, and Scrubbing**  \n",
    "   - **Validation**: Enforcing data quality rules and business constraints.  \n",
    "   - **Cleansing**: Removing unwanted, irrelevant, or corrupt datasets.  \n",
    "   - **Scrubbing**: Converting raw and inconsistent data into a tidy, standardized format.\n",
    "\n",
    "3. **De-duplication & Standardization**  \n",
    "   Removing duplicate records and applying appropriate levels of standardization so the data is consistent and usable for data engineers and downstream consumers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a383d97b-e062-4956-920b-54f4a4babb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Types of Passive data munging\n",
    "- Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ba1b404-34b9-49e4-9225-49c4efb7374f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "####1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality \n",
    "- - Null columns are there\n",
    "- - duplicate rows\n",
    "- - format issues are there (age is not in number format eg. 7-7)\n",
    "- - Uniformity issues (Artist, artist)\n",
    "- - Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- - Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c0aefad-e875-47e7-882a-15082f922d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "####2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8686c6e6-a2e2-42e1-a5a2-b8331bba8684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False,inferSchema=True).toDF(\"ID\",\"FName\",\"LName\",\"Age\",\"profession\")\n",
    "#rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))\n",
    "display(rawdf1.sample(.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e5f3be-5877-4ddf-ac3f-3a89cd09abe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "print(rawdf1.printSchema())#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(rawdf1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(rawdf1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "for i in rawdf1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0]);\n",
    "\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f5d6b9-7d02-4180-aae7-b6064bc47cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format\n",
    "#After Identifying the structType and structFiled we can modify the scheam easily like below\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "structschema=StructType([StructField('ID', StringType(), True), StructField('FName', StringType(), True), StructField('LName', StringType(), True), StructField('Age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "rawdf2=spark.read.schema(structschema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False)\n",
    "print(rawdf2.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "450be28e-793f-402c-a81b-7d2700c8384d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Actual count of the data:\",rawdf1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75648d99-fbae-4a9b-a703-ce4aa4b4d31e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c54eb4c-d392-4927-96ef-91a8ffa8cd0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. rawdf1.distinct()<br>\n",
    "\n",
    "**What it does**<br>\n",
    "- Removes fully identical rows<br>\n",
    "- Considers all columns in the DataFrame<br>\n",
    "- Keeps only one copy of each complete row<br>\n",
    "---\n",
    "Key behavior<br>\n",
    "Two rows are considered duplicates only if every column value matches.<br>\n",
    "\n",
    "cust_id | name   | country|\n",
    "|-------|--------|--------|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | US|\n",
    "\n",
    "|ust_id | name   | country|\n",
    "|-------|--------|-----|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | US|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac10b5f3-6f2a-4a3d-9052-ce28804cd23e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"De-duplicated record count (all columns using distinct):\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15001a97-0626-49ad-9a14-f9d724b6b6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"De-duplicated record count (all columns using dropDuplicates):\",rawdf1.dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e79404-be1c-439f-979b-bb68a0209be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#How many unique rows are there in this DataFrame?”\n",
    "print(rawdf1.dropDuplicates().count())\n",
    "|ID | Name|\n",
    "|---|---|\n",
    "|1  | A|\n",
    "|1  | A|\n",
    "|2  | B|\n",
    "\n",
    "- Spark removes repeated rows\n",
    "\n",
    "|ID | Name|\n",
    "|---|---|\n",
    "|1  | A|\n",
    "|2  | B|\n",
    "\n",
    "**When you do not pass any column names to dropDuplicates(), it behaves the same as distinct()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcbfeba-f227-4677-9ee4-d3bea4640c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"de-duplicated given id column count:\",rawdf1.dropDuplicates(['id']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb8de865-8419-437e-8a47-a8cd2cb038c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## `dropDuplicates(['id'])` Behavior in Spark\n",
    "\n",
    "### **What Spark Does:**\n",
    "- **Only looks at the `id` column** when checking for duplicates\n",
    "- If multiple rows have the same `id` value:\n",
    "  - Keeps **one** row (the first occurrence by default)\n",
    "  - Drops **all other** rows with that same `id`\n",
    "- **Other columns are completely ignored** for duplicate checking\n",
    "- Returns a DataFrame with unique `id` values\n",
    "\n",
    "---\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "**Input DataFrame:**\n",
    "```python\n",
    "|-----|-------|-------+|\n",
    "| id  | name  | country|\n",
    "|-----|-------|-------|\n",
    "| 101 | Rahul | IN    |\n",
    "| 101 | Rahul | US    |  ← Different country, but same id\n",
    "| 101 | Amit  | UK    |  ← Different name, but same id\n",
    "| 102 | Priya | IN    |\n",
    "| 103 | Rahul | US    |\n",
    "\n",
    "\n",
    "\n",
    "|-----|-------|-------|\n",
    "| id  | name  | country|\n",
    "|-----|-------|-------|\n",
    "| 101 | Rahul | IN    |  ← First occurrence kept, others dropped\n",
    "| 102 | Priya | IN    |\n",
    "| 103 | Rahul | US    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7fffb52-f7a1-4cf3-af16-9d79ef1f1a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Describe**\n",
    "\n",
    "| Statistic | Meaning                   |\n",
    "| --------- | ------------------------- |\n",
    "| `count`   | Number of non-null values |\n",
    "| `mean`    | Average value             |\n",
    "| `stddev`  | Standard deviation        |\n",
    "| `min`     | Minimum value             |\n",
    "| `max`     | Maximum value             |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5025a219-cdd7-4609-afbf-22e68b426f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9afdf71-b525-4130-b97b-3975f4ac4b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**summary() is similar to describe(), but more flexible and informative**\n",
    "\n",
    "| Statistic | Meaning                   |\n",
    "| --------- | ------------------------- |\n",
    "| `count`   | Number of non-null values |\n",
    "| `mean`    | Average                   |\n",
    "| `stddev`  | Standard deviation        |\n",
    "| `min`     | Minimum value             |\n",
    "| `25%`     | 1st quartile              |\n",
    "| `50%`     | Median                    |\n",
    "| `75%`     | 3rd quartile              |\n",
    "| `max`     | Maximum value             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540afe5a-28d3-4940-93d3-9fd985e6c582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1.summary())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_End_End_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
