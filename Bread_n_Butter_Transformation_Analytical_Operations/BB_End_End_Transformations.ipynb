{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53290bd-2900-455f-b019-d1e729bcc5b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Simply say- We are going to learn...\n",
    "next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae4557a-86a0-4b80-99ce-113351583d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Stage1](./stage1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8af1d8-5b6d-4f7b-8b89-48267f0a0c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **1. Data Munging (Data Cleanup)**\n",
    "\n",
    "Data munging is the process of converting raw, messy data into a clean, structured, and usable format.  \n",
    "It prepares data so that it can be safely used for downstream activities such as data transformation, enrichment, analytics, reporting, and data science or AI applications.<br>\n",
    "When data is first collected (from files, databases, APIs, logs, etc.), it is usually:\n",
    "- Inconsistent\n",
    "- Incomplete\n",
    "- Incorrect\n",
    "- Hard to analyze directly\n",
    "- Data munging fixes this\n",
    "\n",
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13d429f-52f1-4553-815b-c7242acab4b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###A.**Passive Data Munging** <br>\n",
    "Passive data munging = understanding data without changing it<br>\n",
    "Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) <br>\n",
    "(every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "It involves exploring the data to understand its structure, attributes, quality, and patterns without modifying the data.<br>\n",
    "---\n",
    "Before cleaning or transforming data, we first need to examine the data to understand what it contains.\n",
    "This step helps answer key questions such as:\n",
    "\n",
    "- What columns exist in the dataset?\n",
    "- What is the data type of each column?\n",
    "- How much data is missing or null?\n",
    "- Are there duplicate records?\n",
    "- Are there any obvious patterns, outliers, or anomalies in the data?\n",
    "\n",
    "---\n",
    "##**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)\n",
    "\n",
    "## **Active Data Munging**\n",
    "\n",
    "Active data munging refers to the process of **actively modifying and transforming data** to make it clean, structured, and usable for downstream systems and users.\n",
    "\n",
    "1. **Combining Data & Schema Structuring**  \n",
    "   Integrating data from multiple sources while handling schema evolution and schema merging to produce a consistent and well-defined structure.\n",
    "\n",
    "2. **Validation, Cleansing, and Scrubbing**  \n",
    "   - **Validation**: Enforcing data quality rules and business constraints.  \n",
    "   - **Cleansing**: Removing unwanted, irrelevant, or corrupt datasets.  \n",
    "   - **Scrubbing**: Converting raw and inconsistent data into a tidy, standardized format.\n",
    "\n",
    "3. **De-duplication & Standardization**  \n",
    "   Removing duplicate records and applying appropriate levels of standardization so the data is consistent and usable for data engineers and downstream consumers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a383d97b-e062-4956-920b-54f4a4babb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Types of Passive data munging\n",
    "- Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ba1b404-34b9-49e4-9225-49c4efb7374f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality \n",
    "- - Null columns and null records are there\n",
    "- - duplicate rows & Duplicate id keys\n",
    "- - format issues are there (age is not in number format eg. 7-7)\n",
    "- - Uniformity issues (Artist, artist)\n",
    "- - Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- - Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0aefad-e875-47e7-882a-15082f922d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8686c6e6-a2e2-42e1-a5a2-b8331bba8684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False,inferSchema=True).toDF(\"ID\",\"FName\",\"LName\",\"Age\",\"profession\")\n",
    "#rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))\n",
    "display(rawdf1.sample(.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e5f3be-5877-4ddf-ac3f-3a89cd09abe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "print(rawdf1.printSchema())#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(rawdf1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(rawdf1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "for i in rawdf1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0]);\n",
    "\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f5d6b9-7d02-4180-aae7-b6064bc47cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format\n",
    "#After Identifying the structType and structFiled we can modify the scheam easily like below\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "structschema=StructType([StructField('ID', StringType(), True), StructField('FName', StringType(), True), StructField('LName', StringType(), True), StructField('Age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "rawdf2=spark.read.schema(structschema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False)\n",
    "print(rawdf2.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "450be28e-793f-402c-a81b-7d2700c8384d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Actual count of the data:\",rawdf1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75648d99-fbae-4a9b-a703-ce4aa4b4d31e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c54eb4c-d392-4927-96ef-91a8ffa8cd0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. rawdf1.distinct()<br>\n",
    "\n",
    "**What it does**<br>\n",
    "- Removes fully identical rows<br>\n",
    "- Considers all columns in the DataFrame<br>\n",
    "- Keeps only one copy of each complete row<br>\n",
    "---\n",
    "Key behavior<br>\n",
    "Two rows are considered duplicates only if every column value matches.<br>\n",
    "\n",
    "cust_id | name   | country|\n",
    "|-------|--------|--------|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | US|\n",
    "\n",
    "|ust_id | name   | country|\n",
    "|-------|--------|-----|\n",
    "|101     | Rahul  | IN|\n",
    "|101     | Rahul  | US|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac10b5f3-6f2a-4a3d-9052-ce28804cd23e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"De-duplicated record count (all columns using distinct):\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15001a97-0626-49ad-9a14-f9d724b6b6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"De-duplicated record count (all columns using dropDuplicates):\",rawdf1.dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e79404-be1c-439f-979b-bb68a0209be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####How many unique rows are there in this DataFrame?‚Äù\n",
    "print(rawdf1.dropDuplicates().count())\n",
    "|ID | Name|\n",
    "|---|---|\n",
    "|1  | A|\n",
    "|1  | A|\n",
    "|2  | B|\n",
    "\n",
    "- Spark removes repeated rows\n",
    "\n",
    "|ID | Name|\n",
    "|---|---|\n",
    "|1  | A|\n",
    "|2  | B|\n",
    "\n",
    "**When you do not pass any column names to dropDuplicates(), it behaves the same as distinct()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcbfeba-f227-4677-9ee4-d3bea4640c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"de-duplicated given id column count:\",rawdf1.dropDuplicates(['id']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8de865-8419-437e-8a47-a8cd2cb038c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### `dropDuplicates(['id'])` Behavior in Spark\n",
    "\n",
    "### **What Spark Does:**\n",
    "- **Only looks at the `id` column** when checking for duplicates\n",
    "- If multiple rows have the same `id` value:\n",
    "  - Keeps **one** row (the first occurrence by default)\n",
    "  - Drops **all other** rows with that same `id`\n",
    "- **Other columns are completely ignored** for duplicate checking\n",
    "- Returns a DataFrame with unique `id` values\n",
    "\n",
    "---\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "**Input DataFrame:**\n",
    "```python\n",
    "|-----|-------|-------|\n",
    "| id  | name  | country|\n",
    "|-----|-------|-------|\n",
    "| 101 | Rahul | IN    |\n",
    "| 101 | Rahul | US    |  ‚Üê Different country, but same id\n",
    "| 101 | Amit  | UK    |  ‚Üê Different name, but same id\n",
    "| 102 | Priya | IN    |\n",
    "| 103 | Rahul | US    |\n",
    "\n",
    "\n",
    "\n",
    "|-----|-------|-------|\n",
    "| id  | name  | country|\n",
    "|-----|-------|-------|\n",
    "| 101 | Rahul | IN    |  ‚Üê First occurrence kept, others dropped\n",
    "| 102 | Priya | IN    |\n",
    "| 103 | Rahul | US    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fffb52-f7a1-4cf3-af16-9d79ef1f1a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Describe**\n",
    "\n",
    "| Statistic | Meaning                   |\n",
    "| --------- | ------------------------- |\n",
    "| `count`   | Number of non-null values |\n",
    "| `mean`    | Average value             |\n",
    "| `stddev`  | Standard deviation        |\n",
    "| `min`     | Minimum value             |\n",
    "| `max`     | Maximum value             |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5025a219-cdd7-4609-afbf-22e68b426f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9afdf71-b525-4130-b97b-3975f4ac4b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**summary() is similar to describe(), but more flexible and informative**\n",
    "\n",
    "| Statistic | Meaning                   |\n",
    "| --------- | ------------------------- |\n",
    "| `count`   | Number of non-null values |\n",
    "| `mean`    | Average                   |\n",
    "| `stddev`  | Standard deviation        |\n",
    "| `min`     | Minimum value             |\n",
    "| `25%`     | 1st quartile              |\n",
    "| `50%`     | Median                    |\n",
    "| `75%`     | 3rd quartile              |\n",
    "| `max`     | Maximum value             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540afe5a-28d3-4940-93d3-9fd985e6c582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d73b956-39c2-4eba-8e0b-bd35dcdce938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**B. Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ccffe7-058a-4679-9f7a-76d693360c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession#15lakhs\n",
    "spark=SparkSession.builder.appName(\"WD36 - ETL Pipeline - Bread & Butter\").getOrCreate()#3 lakhs LOC by Databricks (for eg. display, delta, xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63eb20c8-091a-4c16-9e9d-bf4bf474edc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. **Structuring** - Combining Data + Schema Evolution/Merging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa3036d-28d3-49b2-9d69-20f8867537bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When do we go for Schema Evolution?<br>\n",
    "Over the time, if no. of col are keep added by source<br>\n",
    "Serialization  while writing+ mergeSchema while reading<br>\n",
    "When do we go for Schema Merging?<br>\n",
    "In a given day, If we get multiple files of related (not same) structure<br>\n",
    "After reading in dataframe format -> unionByName + allowMissingColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8be7be0-62a4-4cee-8117-7f9111235969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\")\n",
    "#2. Multiple files (with same or different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/sub_paths/\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05326ec0-8318-4e9e-846b-78a04f194d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**When do we go for Schema Evolution?**<br>\n",
    "Over the time, if no. of col are keep added by source<br>\n",
    "Serialization  while writing+ mergeSchema while reading<br>\n",
    "**When do we go for Schema Merging?**<br>\n",
    "In a given day, If we get multiple files of related (not same) structure<br>\n",
    "After reading in dataframe format -> unionByName + allowMissingColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8341ac1-97ea-40bf-ac16-10bf1dc2a83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#When you go for Schema Merging/Melting and Schema Evolution?\n",
    "#Schema Merging/Melting (unionByName,allowMissingColumns)- If we get multiple files\n",
    "#Schema Evolution (orc/parquet with mergeSchema) - If no. of columns are keeps added by the source system\n",
    "#when we know structure of the file already - schema merge/ schema not known earlier  - schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002e3c56-c575-4263-9706-3dd1a243ae47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Fisrt Understand the difference between Schema Merging/Melting and Schema Evolution?**<br>\n",
    "\n",
    "In short we can say Take all columns from all files and create a superset schema<br>\n",
    "Example<br>\n",
    "FILE1:-<br>\n",
    "|id |name|\n",
    "|---|----|\n",
    "|1|John|\n",
    "\n",
    "FILE2:-\n",
    "|id|age|\n",
    "|--|---|\n",
    "|2|30|\n",
    "\n",
    "After schema merging<br>\n",
    "|id | name | age|\n",
    "|---|------|------|\n",
    "|1  | John | null|\n",
    "|2  | null | 30|\n",
    "\n",
    "Ask ONLY ONE QUESTION first:<br>\n",
    "üëâ Am I combining multiple DataFrames in my Spark code?<br>\n",
    "OR<br>\n",
    "üëâ Am I reading/writing data from storage (Parquet/ORC/Delta)?<br>\n",
    "\n",
    "1Ô∏è‚É£ If you are COMBINING DataFrames ‚Üí Schema Merging (unionByName)<br>\n",
    "\n",
    "Use this when:<br>\n",
    "- You already have two or more DataFrames<br>\n",
    "- Columns are not exactly the same<br>\n",
    "- You want to append rows<br>\n",
    "- Missing columns should become null<br>\n",
    "Example scenario:<br>\n",
    "Day 1 file ‚Üí id, name<br>\n",
    "Day 2 file ‚Üí id, name, salary<br>\n",
    "\n",
    "You read both separately and want one dataset.<br>\n",
    "\n",
    "USE:- df1.unionByName(df2, allowMissingColumns=True)<br>\n",
    "\n",
    "Think like this:<br>\n",
    "**I already have data in memory, now I want to merge rows**<br>\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ If you are READING / WRITING data from storage ‚Üí Schema Evolution<br>\n",
    "Use this when:<br>\n",
    "- Data is stored in Parquet / ORC / Delta<br>\n",
    "- New columns appear over time<br>\n",
    "- You are reading or appending data<br>\n",
    "- You want Spark to auto-handle schema change<br>\n",
    "\n",
    "Example scenario:<br>\n",
    "Day 1 parquet ‚Üí id, name<br>\n",
    "Day 10 parquet ‚Üí id, name, salary<br>\n",
    "\n",
    "You read from the same folder.<br>\n",
    "\n",
    "‚úÖ Use:spark.read.option(\"mergeSchema\", \"true\").parquet(path)<br>\n",
    "or during write:<br>\n",
    "df.write.option(\"mergeSchema\", \"true\").mode(\"append\").parquet(path)<br>\n",
    "Think like this:<br>\n",
    "\n",
    "‚ÄúData structure changed in storage over time.‚Äù<br>\n",
    "\n",
    "Interview Answer (Perfect & Short)<br>\n",
    "\n",
    "Use unionByName when combining multiple DataFrames with different columns.<br>\n",
    "Use mergeSchema when schema evolves over time in Parquet/ORC/Delta files<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351aa2ab-e54c-4c62-84dd-45607a3dc2ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#COMBINING OR SCHEMA MERGING or SCHEMA MELTING of Data from different sources(Important interview question also as like schema evolution...)\n",
    "#4. Multiple files with different structure in multiple paths or sub paths\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/sub_paths/\"],recursiveFileLookup=True,pathGlobFilter='custsmodified_N*')\n",
    "print(rawdf1.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c20290-cc7d-42cc-97d5-d9f0a0f94c3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/sub_paths/\"],recursiveFileLookup=True,pathGlobFilter='custsmodified_T*')\n",
    "rawdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665a2605-0e94-4c80-87ee-7957e9f29308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rawdf1)\n",
    "display(rawdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b5bedd-5e8b-4d0b-b6f9-fab2310854ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf_merged=rawdf1.union(rawdf2)#Use union only if the dataframes are having same columns in the same order with same datatype..\n",
    "rawdf_merged.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f31976d-5084-4df7-8f38-14c34a4d498e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Expected right approach to follow here is\n",
    "rawdf_merged_unionbynme=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged_unionbynme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223cc798-c37c-4965-a413-65931493494a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Here, we are merging two files because both are in CSV format. If one file is CSV and the other file is in a different format, what should we do in this scenario? it will be handled automatically\n",
    "#rawdf2.write.json(\"/Volumes/workspace/wd36schema/ingestion_volume/staging/csvjson\")\n",
    "rawdf3=spark.read.json(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_json/\")\n",
    "rawdf_json_csv=rawdf_merged_unionbynme.unionByName(rawdf3,allowMissingColumns=True)\n",
    "display(rawdf_json_csv)\n",
    "\n",
    "#Even if source files are in different formats like CSV and JSON, Spark first converts them into DataFrames. Once they are DataFrames, unionByName can be used to merge them based on column names. File format differences are handled during the read phase, not during union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5fd616-6a6f-4aaf-9a47-8acef8c11d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Just for the simple learning of schema evolution & schema merging/melting<br>\n",
    "Schema merging/melting<br>\n",
    "1,rajeshwari day1(source1)<br>\n",
    "1,rajeshwari,30 day1(source2)<br>\n",
    "\n",
    "Schema evolution<br>\n",
    "1,rajeshwari day1<br>\n",
    "1,rajeshwari,30 day2<br>\n",
    "\n",
    "Output is same in both cases...<br>\n",
    "| id | name        | age |\n",
    "|----|-------------|-----|\n",
    "| 1  | rajeshwari  | null|\n",
    "| 1  | rajeshwari  | 30  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fafc669-6980-486b-b2ce-5f87a2a43a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769f895b-13e8-4b7c-9ccf-7c18372cba63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rawdf1.printSchema())##I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "display(rawdf1.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941df628-48fa-4f6f-afc6-1d3a2e79eb6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validation by doing cleansing\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,ShortType\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleaneddf1=spark.read.schema(struttype1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')\n",
    "print(\"after keeping nulls on the wrong data format\",cleaneddf1.count())#all rows count\n",
    "display(cleaneddf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)\n",
    "\n",
    "#or\n",
    "#method2 - drop malformed rows\n",
    "cleaneddf2=spark.read.schema(struttype1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='dropmalformed')\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\",len(cleaneddf2.collect()))\n",
    "display(cleaneddf2)#We are removing the entire row, where ever data format mismatch is there (throwing away the entire potato)\n",
    "\n",
    "print(cleaneddf2.count())#count will return the original count of the raw data\n",
    "print(len(cleaneddf2.collect()))#collect+len will return the dropmalformed count of the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37bc578f-54ca-40ff-8c11-9f553a52a29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089be61d-d3ec-4eda-8281-afe7faeb9b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method3 best methodology of applying active data munging\n",
    "#Validation by doing cleansing (not at the time of creating Dataframe, rather we will clean and scrub subsequently)...\n",
    "struttype3=StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "rawdf4=spark.read.schema(struttype3).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')\n",
    "print(\"allow all data showing the real values\",rawdf4.count())#all rows count\n",
    "display(rawdf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12114213-81f3-4fa3-ab73-fd7bf5dddfb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Rejection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea49fd0-b76f-4ce7-bce7-251db59ec0c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "strcut1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "cleaneddf1=spark.read.schema(strcut1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')\n",
    "display(cleaneddf1)\n",
    "corrupted_data_set=cleaneddf1.where(\"corruptedrows is not NULL\")\n",
    "corrupted_data_set.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/rejected_data\",header=True,mode='overwrite')\n",
    "retained_data=cleaneddf1.where(\"corruptedrows is NULL\")\n",
    "print(\"Overall rows in the source data is \",len(cleaneddf1.collect()))\n",
    "print(\"Rejected rows in the source data is \",len(corrupted_data_set.collect()))\n",
    "print(\"Clean rows in the source data is \",len(retained_data.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58eebba-1c51-47a1-a0f3-10532360b269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing \n",
    "na.drop()<br>\n",
    "It is a process of cleaning/removing/deleting unwanted data\n",
    "Eg. I am purchasing potato from a shop, I am cutting down the debris/rotten portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9064eba-f634-4433-8e39-8c93f281146b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2\n",
    "#Important na functions we can use to do cleansing\n",
    "cleanseddf1=rawdf4.na.drop(how=\"any\")#It drops (removes) any row from rawdf1 that has at least one null value in any column ===== any\" means:Drop the row if ANY column in that row is null\n",
    "#cleanseddf = rawdf1.na.drop(how=\"all\")#Drop only if ALL columns are null\n",
    "#rawdf1.na.drop(subset=[\"id\", \"name\"])#Drop based on specific columns\n",
    "print(\"any one row in the rawdf4 with age null\")\n",
    "display(rawdf4.where(\"age is null\"))\n",
    "print(\"any one row in the cleansed df with age null\")\n",
    "display(cleanseddf1.where(\"age is null\"))\n",
    "print(\"After Drop the row if ANY column in that row is null:\", cleanseddf1.count())\n",
    "display(rawdf4.where('age is NULL')) #here age column has some NULL values\n",
    "display(cleanseddf1.where('age is null')) #Here No rows are returned because on the DF we arlready did na.drop()\n",
    "cleanseddf2=rawdf4.na.drop(how=\"any\",subset=['id','age'])\n",
    "print(\"After removing subset:\",len(cleanseddf2.collect()))\n",
    "cleanseddf3=rawdf4.na.drop(how=\"all\",subset=[\"lastname\",\"profession\"])\n",
    "print(\"After removing subset and all:\",len(cleanseddf3.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24fa975b-4a0d-499e-9e13-3d1c03acf4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Scrubbing \n",
    "na.fill() & na.replace()<br>\n",
    "It is a process of polishing/fine tuning/scrubbing/meaningful conversion the data in a usable format\n",
    "Eg. I am purchasing potato from a shop, I am scrubbing/washing mud/sand portion of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ec3c0e-ea2d-4954-a28a-96ffc07e4984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleandf5=rawdf4.na.fill('Not Provided',subset=[\"lastname\"])\n",
    "#display(cleandf5)\n",
    "df1=rawdf4.na.fill({\"firstname\":\"unkown\",\"lastname\":\"unkown\"})#Replace nulls with a fixed value.\n",
    "#display(df1)\n",
    "df2=rawdf4.fillna('NA',subset=[\"firstname\"])\n",
    "display(df2)\n",
    "find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "find_replace_values_dict2={'not provided':'NA'}\n",
    "scrubbeddf2=rawdf4.na.replace(find_replace_values_dict1,subset=[\"profession\"])#fill function is helping us find and replace the values\n",
    "#df.na.replace(to_replace, value, subset=None)\n",
    "display(scrubbeddf2)\n",
    "scrubbeddf3=rawdf4.na.replace(find_replace_values_dict2,subset=[\"lastname\"])\n",
    "display(scrubbeddf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7114abf8-bef3-4222-9456-c158b7ab8037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####DeDuplication\n",
    "Removal of duplicate rows/columns based on a priority or non priority\n",
    "distinct & dropDuplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b86b8eff-0c38-47ba-936b-f836ddf15e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "strcut1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "cleaneddf1=spark.read.schema(strcut1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",mode='permissive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331887c0-f532-4031-a7b8-b561bbd2849a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767025171963}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaneddf1.where(\"id in (4000001)\"))#before row level dedup\n",
    "row_dup_removed=cleaneddf1.distinct()#It will remove the row level duplicates\n",
    "display(row_dup_removed.where(\"id in (4000001)\"))\n",
    "\n",
    "print(\"non prioritized deduplication, just remove the duplicates retaining only the first row\")\n",
    "#row_dup_removed=cleaneddf1.distinct():-At this stage, the DataFrame may be distributed across multiple partitions.\n",
    "#.coalesce(1) Reduces the number of partitions in the DataFrame to 1.coalesce(1) forces all data into a single partition.\n",
    "display(row_dup_removed.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "colmn_dup_removed=row_dup_removed.coalesce(1).dropDuplicates([\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(colmn_dup_removed.coalesce(1).where(\"id in ('4000003')\"))\n",
    "\n",
    "print(\"prioritized deduplication based on age\")\n",
    "display(row_dup_removed.coalesce(1).where(\"id in ('4000003')\"))\n",
    "colmn_dup_removed1=row_dup_removed.coalesce(1).orderBy(['id','age'],ascending=[True,False]).dropDuplicates(['id'])\n",
    "display(colmn_dup_removed1.coalesce(1).where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789c4e45-7753-4cd9-9cdc-5cc1ae51cc68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc10197b-82a1-48ab-a4b7-8703d237be2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "#####Standardization - \n",
    "Making the data more standard by adding/removing/reordering columns as per the expected standard, unifying into expected format, converting the type as expected etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44eb5a03-693f-46c8-b1b0-801ed4ca6f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization1 - Column Enrichment (Addition of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf9d4e8-a9b0-4395-90d0-db5c7f29e9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False,inferSchema=True).toDF(\"ID\",\"FName\",\"LName\",\"Age\",\"profession\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a22007b-bbaf-47b6-bf84-d86ba251cfd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "standarddf1=rawdf1.withColumn(\"sourcesystem\",lit(\"Retail\"))#This line adds a new column named sourcesystem to the DataFrame rawdf1, and assigns the constant value \"Retail\" to every row in that column.\n",
    "#display(standarddf1)\n",
    "#It converts a constant value into a Spark Column object so Spark can apply it to all rows.Without lit(), Spark would treat \"Retail\" as a column name and throw an error\n",
    "display(standarddf1.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4253b4-ba6a-4e42-b688-8b87a49dea67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization2 - Column Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccf1d5d-1938-498c-9d47-56a201fdb96c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/uniformity/custsmodified.csv\",header=False,inferSchema=True).toDF(\"ID\",\"FName\",\"LName\",\"Age\",\"profession\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94367a63-f082-45cc-bc1a-7f99c46b7376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rawdf1.createOrReplaceTempView(\"sqlview\")#This line registers the DataFrame standarddf1 as a temporary SQL view named sqlview, so that you can query it using Spark SQL.After this, you can run SQL querie\n",
    "\n",
    "display(spark.sql(\"\"\"select profession,\n",
    "                  count(*) as ToatlCount\n",
    "                  from sqlview \n",
    "                  group by profession \n",
    "                  order by profession\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83920df4-4f7f-41d3-8b07-8c1899fc023d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "display(rawdf1.groupBy(\"profession\").count().orderBy(\"profession\", ascending=True))#DSL\n",
    "#Standardization2 - column uniformity\n",
    "Standardization2_df=rawdf1.withColumn(\"profession\",initcap(\"profession\"))#inicap or any other string function with columnOr name can accept either column or string type provided if the string is a column name for eg. profession/age/sourcesystem.\n",
    "#->Using column name as a string [initcap(\"profession\")]----Spark internally converts \"profession\" ‚Üí col(\"profession\")\n",
    "#and initcap(col(\"profession\"))---Both produce exactly the same result.\n",
    "#Spark SQL functions are designed to accept:\n",
    "\n",
    "#Column objects OR column names as strings (auto-resolved)\n",
    "#So internally this happens:initcap(\"profession\")  ‚Üí  initcap(col(\"profession\"))\n",
    "\n",
    "display(Standardization2_df.groupBy(\"profession\").count().orderBy(\"profession\", ascending=True))\n",
    "display(Standardization2_df.limit(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0e21bd-111b-402f-a7aa-678bbc639ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization3 - Format Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03930901-8cc9-4a78-8bd7-ee3d41734f2b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ID\":142},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767117017258}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Did analysis to understand the format issues in our id and age columns\n",
    "display(Standardization2_df.where(\"id like 't%'\"))\n",
    "Standardization2_df.where(\"id rlike '[a-zA-Z]'\").show()#rlike is regular expression like function that help us identify any string data in our DF column\n",
    "Standardization2_df.where(\"age rlike '[^0-9]'\").show()#checking for any non number values in age column\n",
    "#standarddf3=Standardization2_df.withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66dd4c7d-ba32-4662-b83f-81b188eea503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,replace\n",
    "#Let's apply scrubbing features to our id column to replace ten with 10 (or we can think of using GenAI here)\n",
    "replaceval={'one':'1','two':'2','three':'3','four':'4','five':'5','six':'6','seven':'7','eight':'8','nine':'9','ten':'10'}\n",
    "Standardization3_df=Standardization2_df.na.replace(replaceval,['id'])\n",
    "\n",
    "#standarddf3=standarddf2.withColumn(\"id\",replace(\"id\",lit('ten'),\"10\"))\n",
    "#Here replace(\"id\", \"ten\", \"10\")\n",
    "#\"id\" ‚Üí column name\n",
    "#\"ten\" ‚Üí value to search\n",
    "#\"10\" ‚Üí replacement value\n",
    "standarddf4=Standardization3_df.withColumn(\"age\",regexp_replace(\"age\",'-',\"\"))\n",
    "display(standarddf4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e09fbe-2dcb-4603-b8e3-4b1d4bb22851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization4 - Data Type Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9a8a47-cc80-4a30-ac9e-e5502908e7b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "print(standarddf4.printSchema())#Still id and age are string type, though it contains int data\n",
    "standarddf4=standarddf4.withColumn(\"id\",col(\"id\").cast(\"long\"))\n",
    "standarddf4=standarddf4.withColumn(\"id\",standarddf4[\"id\"].cast(\"long\"))\n",
    "standarddf4=standarddf4.withColumn(\"age\",col(\"age\").cast(\"short\"))\n",
    "print(standarddf4.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04b5f07-2610-4992-b061-b5f26f3c1f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization5 - Naming Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bafad3ba-ce8b-48b0-8f39-49294ea95326",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767120358433}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf5=standarddf4.withColumnRenamed(\"id\",\"custid\")\n",
    "standarddf5=standarddf4.withColumnsRenamed({\"id\":\"custid\",\"age\":\"Age\"})\n",
    "display(standarddf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53218bbd-955a-48c9-a970-76731036d030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization6 - Reorder Standadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46038d80-665d-4ad2-83ab-2c7682bf6ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf6=standarddf5.select(\"custid\", \"Age\", \"FName\",\"LName\",\"profession\")\n",
    "#display(standarddf6)\n",
    "display(standarddf6.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5976e9b6-fc48-454c-a012-ddae1d9ed71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Before starting Data Enrichment or before sharing the data to the consumer, we have to do EDA/Exploration/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b0ef51-6e42-4aba-a459-f35807051a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf6.printSchema()\n",
    "display(standarddf6.take(20))\n",
    "display(\"total rows\",len(standarddf6.collect()))\n",
    "display(standarddf6.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f07461a-151c-4cc5-bdc6-a6c4bbee7ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**2. Data Enrichment** - Detailing of data\n",
    "Makes your data rich and detailed <br>\n",
    "a. Add (withColumn,select,selectExpr), Derive (withColumn,select,selectExpr), Remove(drop,select,selectExpr), Rename (withColumnRenamed,select,selectExpr), Modify/replace (withColumn,select,selectExpr) - very important spark sql functions <br>\n",
    "b. split, merge/Concat <br>\n",
    "c. Type Casting, reformat & Schema Migration <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4155b517-2353-4d00-992d-93f7e7894844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![stage2.png](./stage2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58911383-3171-42a6-93e1-464a8936bb46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####a. Add (), Derive (), Rename (), Modify/replace (), Remove/Eliminate () - very important spark sql DF functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393df97b-a256-4254-b210-b7112dd89ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Adding of columns\n",
    "Lets add datadt (date of the data orgniated from the source for eg. provided in the filename in a format of yy/dd/MM) and loaddt (date when we are loading the data into our system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f98dabf-20bb-4e46-a0c5-0b6a25a2b3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "derived_datadt='25/30/12'\n",
    "print(f\"hello '{derived_datadt}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f5293d-0f44-4eda-907e-1b344e3031db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,current_date\n",
    "original_filename='custsmodified_25/30/12.csv'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "derived_datadt=original_filename.split('_')[1].split('.')[0]\n",
    "print(\"the derived date is:\",derived_datadt)\n",
    "#derived_datadt='25/30/12'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "#or\n",
    "df1=standarddf6.withColumn(\"loaddt\",lit(\"25/30/12\")).withColumn(\"datadt\",current_date())#withColumn() is used to add or replace ONE column at a time.\n",
    "df1.printSchema()\n",
    "display(df1.take(5))\n",
    "#or\n",
    "df2=standarddf6.withColumns({\"loaddt\":lit(\"25/30/12\"),\"datadt\":current_date()})#withColumns() is used to add or replace multiple columns in a single call.\n",
    "display(df2.take(2))\n",
    "#or\n",
    "df3=standarddf6.select(\"*\",lit(derived_datadt).alias('datadt'),current_date().alias('loaddt'))#DSLs (FBP function)\n",
    "display(df3.take(3))\n",
    "#or\n",
    "df4=standarddf6.selectExpr(\"*\",\"'25/30/12' as loaddt\",\"current_date() as loaddt\")#)#DSL(select) + SQL expression\n",
    "display(df4.take(4))\n",
    "df5=standarddf6.selectExpr(\"*\",f\"'{derived_datadt}' as datadt\",\"current_date() as loaddt\")\n",
    "display(df5.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58eb0bf1-4354-410f-9b2c-65be4328fb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Deriving of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2ae4f14-ccf5-44c3-aa10-ba084015cbd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df1=df5.withColumn(\"professionFlag\",substring(\"profession\",1,3))\n",
    "display(df1.take(2))\n",
    "df2=df5.select(\"*\",substring(\"profession\",1,1).alias(\"professionflag\"))\n",
    "display(df2.take(3))\n",
    "df3=df5.selectExpr(\"*\",\"substr(profession,1,1) as professionflag\")\n",
    "display(df3.take(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aaf5ae-be19-42fd-8715-7f334336f2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "######Renaming of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a076df-5b08-4d9d-9862-7a7d476e48b8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767260005817}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Can we use withColumn to rename? not directly, its costly\n",
    "df3.printSchema()\n",
    "df1=df3.withColumn(\"load_date\",col(\"loaddt\"))\n",
    "df1=df1.drop(\"loaddt\").select(\"custid\",\"FName\",\"LName\",\"Age\",\"profession\",\"professionflag\",\"datadt\",\"load_date\")\n",
    "display(df1.take(3))\n",
    "\n",
    "#or\n",
    "\n",
    "df2=df3.select(\"custid\",\"FName\",\"LName\",\"Age\",\"profession\",\"professionflag\",\"datadt\",col(\"loaddt\").alias(\"load_date\"))\n",
    "display(df2.take(3))#costly too, since we have to choose all columns in the select\n",
    "\n",
    "#or\n",
    "\n",
    "df2=df3.selectExpr(\"custid\",\"FName\",\"LName\",\"Age\",\"profession\",\"professionflag\",\"datadt\",\"loaddt as load_date\")\n",
    "display(df2.take(3))#costly too, since we have to choose all columns in the select\n",
    "\n",
    "#or\n",
    "\n",
    "df2=df3.withColumnRenamed(\"loaddt\",\"load_data\")#Best function to rename the column(s)\n",
    "display(df2.take(3))\n",
    "df2=df3.withColumnsRenamed({\"loaddt\":\"load_data\",\"datadt\":\"dataload\"})\n",
    "display(df2.take(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d37ef8c-88cc-46ac-bfbe-43f31e4c87ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Modify/replace (withColumn, select/selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ed8b91-fa07-468f-bdb2-89f72c26ee84",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767261725333}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=df3.withColumn(\"FName\",col(\"LName\"))#This will replace the LName with Fname\n",
    "display(df3.take(3))\n",
    "display(df1.take(3))\n",
    "\n",
    "df1=df3.withColumn(\"profession\",concat(\"profession\",lit('-'),\"professionflag\"))#This will modify/enrich the profession column with sourcename\n",
    "display(df1.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4fefe2-3369-4c94-af7a-acadffb4179a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Remove/Eliminate (drop,select,selectExpr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045b7142-41f4-434a-b61e-7cd64537e81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.printSchema()\n",
    "df2=df1.select(\"custid\",\"Age\",\"FName\",\"LName\",\"profession\",\"loaddt\",\"datadt\")\n",
    "display(df2.take(3))\n",
    "#or use selectExpr (yes, but costly)\n",
    "df2=df1.selectExpr(\"custid\",\"Age\",\"FName\",\"LName\",\"profession\",\"loaddt\",\"datadt\")\n",
    "display(df2.take(3))\n",
    "#or\n",
    "df2=df1.drop(\"professionflag\")\n",
    "display(df2.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a56eb184-16d8-40f1-8395-4de88bf42b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Conclusion/Best practices of using different column enrichment functions\n",
    "1. **select** is good to use if we want to perform - \n",
    "Good for ordering/reordering of columns, only renaming column (not good), only reformatting/deriving a column (not good), **for all of these operation in a single iteration** such renaming, reordering, reformatting,deriving, dropping etc., (best to use)\n",
    "2. **selectExpr** is good to use if we want to perform - Same as select by using ISO/ANSI SQL functionality (if we are not familiar in DSL FBP) **for all of these operation in a single iteration**\n",
    "3. **withColumn** is good to use if we want to perform - \n",
    "**for adding/deriving/modifying/replacing in a single iteration**\n",
    "Adding/Deriving column(s) in the last (Good), Modifying/replacing (Good), Renaming (not good), Dropping(not possible), reordering(not good)\n",
    "4. **withColumnRenamed** is good to use if we want to perform - only for renaming column (Good)\n",
    "5. **drop** is good to use if we want to perform - only dropping of columns in the given position (Good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12ee369-9031-40a5-b489-c078c50df5e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####b. Splitting & Merging/Melting of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f54569-f631-40ab-901c-da6164df8783",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767370992871}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Splitting of column\n",
    "display(df2.take(5))\n",
    "df3=df2.withColumn(\"Professio\",split(\"profession\",\"-\")[0])\n",
    "display(df3.take(5))\n",
    "#or\n",
    "df4=df3.withColumn(\"ShortProf\",upper(substring(col(\"profession\"),1,3)))\n",
    "display(df4.take(5))\n",
    "#Merging of column\n",
    "df5=df4.select(\"custid\",\"FName\",\"LName\",\"Age\",concat_ws(\" \",\"FName\",\"LName\").alias(\"FullName\"),\"Professio\",\"datadt\",\"loaddt\")\n",
    "display(df5.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62516570-3aad-410c-96c6-cac2b59d7b61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d52f8a-2d20-46a4-a4a3-265db9b9bf39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####c. Formatting & Typecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8291203f-6f64-482e-8d38-ea5b8dd210a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df6=df5.withColumn(\"datadt\",col(\"datadt\").cast(\"Date\"))#This will not work beoz Spark expects the date to be in one of its default formats, typically:yyyy-MM-dd or yyyy/MM/dd\n",
    "df6.printSchema()\n",
    "df7=df5.withColumn(\"datadt\",to_date(col(\"datadt\"),\"yy/dd/MM\"))#25/30/12 -> 2025-12-30\n",
    "df7.printSchema()\n",
    "display(df7.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e335183-d09f-4ed8-9e63-b96fe5120b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Customization - Application of Tailored Business specific Rules <br>\n",
    "a. User Defined Functions <br>\n",
    "b. Building of Frameworks & Reusable Functions (We will learn very next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "003e67a7-e2fe-4be4-bce8-14233a986472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![stage3.png](./stage3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "828e7a2e-7f70-4d51-867c-15435e258e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If Spark does not already provide a built-in function for what you want to do, then you have two options:<br>\n",
    "\n",
    "Look for an existing open-source solution (e.g., GitHub, StackOverflow)<br>\n",
    "\n",
    "Write your own custom function (UDF)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca0ce44-9605-4458-ad3e-bfcd2a6cf15f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#formatteddf2=formatteddf.withColumn(\"sourcename\",upper(\"sourcename\"))\n",
    "#formatteddf2.show(2)\n",
    "#Caveat - If there is no upper() function is available already in spark dsl/sql, we can either search for some functions in the online opensource platform or we have to create one (custom functions)\n",
    "#from org.apache.sql.functions import upperodd\n",
    "\n",
    "def upperodd(colname_containsvalue):\n",
    "    convertedcolvalue=colname_containsvalue.upper()\n",
    "    return convertedcolvalue\n",
    "print(upperodd(\"sunil\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e25563-8e43-47f1-a29f-5c70583de68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8=df7.withColumn(\"FirstName\",upper(col(\"FName\")))#we can't run python function as it is\n",
    "df8.explain() #== Photon Explanation ==The query is fully supported by Photon.\n",
    "df9=df7.withColumn(\"FirstName\",udf(upperodd)(\"FName\"))#promote normal python function to spark ready udf\n",
    "df9.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6d1557-8916-4eff-b28f-6beb56948826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Create Python Custom Function with complex logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e32438-7f64-4017-b94b-f14b5bbc6c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculating age category from the given age of the customer\n",
    "def pythonAgeCat(dfcol):\n",
    "    if dfcol is None:\n",
    "        return \"Unknown\"\n",
    "    elif dfcol<=10:\n",
    "        return \"child\"\n",
    "    elif dfcol>10 and dfcol<=18:\n",
    "        return \"teenager\"\n",
    "    elif dfcol>18 and dfcol<=30:\n",
    "        return \"young\"\n",
    "    elif dfcol>30 and dfcol<=50:\n",
    "        return \"middleaged\"\n",
    "    else:\n",
    "        return \"senior\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f288517d-90ed-442d-93d3-571f8f49b99b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Import udf library, Convert to UDF, Apply in the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4f1d57-67d7-43d3-89a7-ca8ca3b36c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "df10=df7.withColumn(\"AgeCat\",udf(pythonAgeCat)(\"Age\"))#== Photon Explanation ==Photon does not fully support the query because:\n",
    "df10.explain()\n",
    "display(df10.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab42d39a-bf4e-41a8-afbf-c0d54620291a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Data Curation/Processing (Pre Wrangling Stage) - Applying different levels of<br> business logics, transformation, filtering, grouping, aggregation and limits applying<br> different transformation functions\n",
    "1. Select, Filter\n",
    "2. Derive flags & Columns\n",
    "3. Format\n",
    "4. Group & Aggregate\n",
    "5. Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c992d9a7-60c9-4100-a996-46728d1df812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](./stage5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04390613-5810-48d4-a715-96e31d4186a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Select, Filter\n",
    "In terms of Performance Optimzation - I ensured to do Push Down Optimization by doing select(project) & Filter(predicate) of what ever the expected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23f788b-c38e-4261-a88e-ef776dfce324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once data is already in memory, push-down optimization NO LONGER applies.<br>\n",
    "Pushdown works only during READ<br>\n",
    "After that, it is in-memory execution optimization.<br>\n",
    "\n",
    "df10 = spark.read.parquet(\"/data/customers\")<br>\n",
    "At this point:<br>\n",
    "df10 is not loaded yet<br>\n",
    "It is only a logical plan<br>\n",
    "Spark is lazy.<br>\n",
    "\n",
    "Data is loaded only when an ACTION is called:<br>\n",
    "selectdf.show()<br>\n",
    "selectdf.count()<br>\n",
    "\n",
    "At that moment:<br>\n",
    "Spark reads data from storage<br>\n",
    "Pushdown may happen<br>\n",
    "\n",
    "Case 1: Push Down (During Read)<bt>\n",
    "df = spark.read.parquet(\"/data/customers\") \\\n",
    "        .select(\"custid\", \"Age\") \\\n",
    "        .where(\"Age > 30\")\n",
    "\n",
    "üëâ Spark pushes:<bt>\n",
    "column selection<bt>\n",
    "filter condition<br>\n",
    "\n",
    "üëâ Less data enters memory<br>\n",
    "This is true push-down optimization<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c142837-74d0-4f83-ad00-0933e7eee96c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Select\n",
    "#select, functions, case, literal ,from,where,group by, having, order by, limit...\n",
    "#Select few columns by filtering few rows\n",
    "selectdf=df10.select(\"custid\",\"Age\",\"AgeCat\",col(\"Professio\").alias(\"prof\"))#DSL\n",
    "selectdf.show(5)\n",
    "selectdf1=df10.selectExpr(\"custid\",\"Age\",\"AgeCat\",\"Professio as prof\")#SQLSelct\n",
    "selectdf1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d77fd75a-a8c1-458f-9255-9aa700b7a2b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Filter/Where - both are literally same (filter will be used by FBP developers & where will be used by SQL developers)\n",
    "filterdf=selectdf1.filter((col(\"Age\")>40) & (col(\"Age\")<=60))#DSL operation\n",
    "filterdf.show(5)\n",
    "filterdf=selectdf1.where((col(\"Age\")>40) & (col(\"Age\")<=60))#DSP operation\n",
    "filterdf.show(5)\n",
    "\n",
    "filterdf=selectdf1.filter(\"Age>40 and Age<=60\")#SQL where operation\n",
    "filterdf.show(5)\n",
    "filterdf=selectdf1.where(\"Age>40 and Age<=60\")#SQL where operation\n",
    "filterdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ef5085-df6c-4a05-9a77-2a2527d4c611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Derive flags & Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79cdb8d-27b3-4045-a17a-1954277dffe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#We have created agecat using UDF (which is supposed to use only if it is inevitable)\n",
    "#But we can do the same using DSL When.otherwise or SQL CASE WHEN\n",
    "#Deriving Flag\n",
    "#Syntax in DSL: when(conditions,\"value\").when(conditions,\"value2\").otherwise(\"valuen\").alis(\"colname\")\n",
    "derivdf=df10.select(\"*\",when(col(\"Age\").isNull(),'U').when(col(\"Age\")<=10,'C').when((col(\"Age\")>10) & (col(\"Age\")<=18),'T').when((col(\"Age\")>18) & (col(\"Age\")<=30),\"Y\").when((col(\"Age\")>30) & (col(\"Age\")<=50),\"M\").otherwise(\"S\").alias(\"agecatflag\"))\n",
    "#Suggessted than using UDFs\n",
    "display(derivdf.take(10))\n",
    "\n",
    "#Deriving Column\n",
    "#Syntax in SQL: case when conditions then value when conditions then value2 else valuen end as colname\n",
    "derivdf=derivdf.drop('Agecat').selectExpr(\"*\",\"\"\"case when Age is NULL then 'Unkown'\n",
    "                                    when Age<10 then 'Child'\n",
    "                                    when Age>10 AND Age<=18 then 'teenager'\n",
    "                                    When Age>18 AND Age<=30 then 'young'\n",
    "                                    when Age>30 AND Age<=50 then 'middleaged'\n",
    "                                    else 'oldaged' end as agecat\n",
    "                             \"\"\")#Suggessted than using UDFs\n",
    "display(derivdf.take(10))\n",
    "\n",
    "#Interview Answer of how you optimized the existing spark code developed by your ex team members?\n",
    "#I analysed the existing udfs used in my project and seeked for opurtunities to convert them into SQL/dsl based programs by implementing the udf logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb41d9d-7f55-4987-8e18-3a233c1fcbb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''def pythonAgeCat(dfcol):\n",
    "    if dfcol is None:\n",
    "        return \"Unknown\"\n",
    "    elif dfcol<=10:\n",
    "        return \"child\"\n",
    "    elif dfcol>10 and dfcol<=18:\n",
    "        return \"teenager\"\n",
    "    elif dfcol>18 and dfcol<=30:\n",
    "        return \"young\"\n",
    "    elif dfcol>30 and dfcol<=50:\n",
    "        return \"middleaged\"\n",
    "    else:\n",
    "        return \"senior\"'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec4441a-5e04-4824-ad51-164ac680989f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####3.Format (Deriving Columns with different format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7568960-7a42-467c-94d9-11bb1c8728bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We can use different functions - string or number or date function for format modeling\n",
    "from pyspark.sql.functions import col, datediff, year, month, initcap\n",
    "derivdf1=derivdf.select(\"*\",datediff(col(\"datadt\"), col(\"loaddt\")).alias(\"delaydays\"), year(col(\"datadt\")).alias(\"DataYear\"), month(col(\"datadt\")).alias(\"datamonth\")).withColumn(\"agecat\", initcap(col(\"agecat\")))\n",
    "\n",
    "display(derivdf1.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "538d660a-6c78-41ef-80ae-92e087baea61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####4. Group & Aggregate\n",
    "Before performing grouping or aggr, consider the below factors from the dataset....<br>\n",
    "identifier?\tcid (high in cardinality/difference) (surrogate/naturalkey)<br>\n",
    "descriptive?\tname<br>\n",
    "metric?\tavg(age),count(distinct cid),max(age),min(age)<br>\n",
    "measure?\tage,cid<br>\n",
    "grouping?\tage,prof - low in cardinality/difference<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7091e82d-dba7-4ce7-aac2-e25ea472a168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions.aggregate import avg,count,initcap,last_day,datediff,year,month,agg\n",
    "#What is the total number of customers we have?\n",
    "print(derivdf1.count())\n",
    "#What is the total number of customers we have in each profession?\n",
    "curateddf1=derivdf1.groupBy(\"Professio\").count()\n",
    "display(curateddf1.take(5))\n",
    "#Multiple Aggregation with one grouping - What is the total number of customers,average age of those customers we have in each profession?\n",
    "curateddf1=derivdf1.groupBy(\"Professio\").avg(\"Age\").withColumnRenamed(\"avg(Age)\",\"AvgAge\")\n",
    "display(curateddf1.take(5))\n",
    "\n",
    "#To calculate multiple aggregation, we need to use a function called agg function\n",
    "curateddf4=derivdf1.groupBy(\"Professio\").agg(count(\"custid\").alias(\"custcount\"),avg(\"Age\").alias(\"avgage\"))\n",
    "display(curateddf4.take(5))\n",
    "#curateddf4 this dataframe we materialize/store in some tables/files later\n",
    "\n",
    "#Multiple Aggregation with multiple grouping - What is the total number of customers,average age of those customers we have in each profession?\n",
    "curateddf4=derivdf1.groupBy(\"Professio\",\"agecat\").agg(count(\"custid\").alias(\"custcount\"),avg(\"Age\").alias(\"avgage\"))\n",
    "display(curateddf4.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11772989-2b18-47a3-b209-78d4b791361c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####5. Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00f3c1f-ed67-47bf-add3-ee5c527cadec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curateddf5=curateddf4.orderBy(\"Professio\",\"agecat\")\n",
    "display(curateddf5.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb3f755-6609-4eed-9341-9462f6f7e578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####6. Limit\n",
    "Let us take an oppurtunity to understand different data limiting/restricting functions<br>\n",
    "**Limit** is a dataframe TRANSFORMATION functions used to limit the number of rows returned in a spark dataframe FORMAT<br>\n",
    "---\n",
    "**Take** is a dataframe/RDD ACTION functions used to limit the number of rows returned in a python list FORMAT<br>\n",
    "---\n",
    "**display** is a standard output databricks specific function used to produce entire DF/List output in a notebook view with multiple options <br>\n",
    "---\n",
    "**show** is a standard output spark dataframe specific function used to produce default 20 rows of a DF output in a notebook/REPL/IDE view <br>\n",
    "\n",
    "**collect** is a spark action that help us collect the DF/RDD data into the driver environment in a form of python list <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23cc794-cd5c-4f55-b414-c6984d18ab64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#anything can be used under display()\n",
    "print(\"limit output\")\n",
    "curateddf5.limit(20)#.show(10)\n",
    "print(\"take output\")\n",
    "curateddf5.take(20)\n",
    "#I have to filter some data in a limited dataset of 100 rows\n",
    "curateddf5.limit(100).filter(\"professio='Accountant'\").show()\n",
    "#curateddf5.take(100).filter(\"professio='Accountant'\").show()--it returns list so filter can not be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c4b017-3195-42cb-b582-d74a5febd021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Data Wrangling - More of Analytics + Transformation<br>\n",
    "Taking raw, scattered, incomplete data and transforming it into a clean, enriched, analytics-ready dataset\n",
    "  1. Joins - Relation/Connection established between one or more datasets/df/tabl to produce the broader/extended view of the data horizontally.\n",
    "  5 Categories of Joins:\n",
    "  inner, outer(left\n",
    "  right, full), self, cross, special optimized (semi, anti)\n",
    "  2. Lookup\n",
    "  3. Lookup & Enrichment\n",
    "  4. Schema Modeling  (Denormalization)\n",
    "  5. Windowing\n",
    "  6. Analytical\n",
    "  7. Set operations\n",
    "  8. grouping & aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "356bc795-a17a-44e7-855c-1fb8c12be5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Screenshot 2026-01-06 191933.png](./Screenshot 2026-01-06 191933.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567d7abf-9a79-4342-99aa-fd26f9514b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1. Joins\n",
    "Joins are Relation/connection of one or more tables to perform widened (horizontal) data analytics<br>\n",
    "A join connects two or more datasets using a common key to produce a wider dataset (more columns).<br>\n",
    "1. Frequently used simple joins (inner, left)\n",
    "2. InFrequent simple joins (self, right, full, cartesian)\n",
    "3. Advanced joins (Semi and Anti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767cac49-2ba3-4529-84d3-dddd87486956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#How to write join syntax in Spark and learn the semantics of join in spark\n",
    "rawdf1=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BreadButter/\",header=False,inferSchema=True).toDF(\"ID\",\"FName\",\"LName\",\"Age\",\"profession\")\n",
    "rawdf1=rawdf1.na.drop().where(\"id<>'ten' and id<>'trailer_data:end of file'\")\n",
    "leftdf=rawdf1.where(\"id in (4000100,4000101)\")\n",
    "rightdf=rawdf1.where(\"id in (4000100,4000102,4000103)\")\n",
    "leftdf.show(20,False)\n",
    "rightdf.show(20,False)\n",
    "print(\"Inner Join\")\n",
    "innerjoindf=leftdf.join(rightdf,how='inner',on='ID')\n",
    "innerjoindf.show(20)\n",
    "print(\"LEFT JOIN\")\n",
    "leftjoin=leftdf.join(rightdf,how='left',on='ID')\n",
    "leftjoin.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c9ae2f-672d-4910-bb54-408e574bf7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"RIGHT JOIN\")\n",
    "rightjoin=leftdf.join(rightdf,how='right',on='ID')\n",
    "rightjoin.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_End_End_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
