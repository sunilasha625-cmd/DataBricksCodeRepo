{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1973442c-8f18-4c2c-8ad7-e11941f98382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain ReadOps Assignment\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e51d0b5-9fb8-4827-9b12-d06cdfee1a08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da18272-81fc-4e17-9c5c-d0589039cc40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b56d3fb-2fe9-40ee-8442-faf06669c659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print(spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799b004f-9db8-475f-84b3-37198ff8941c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Write SQL statements to create:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa88cb3-1954-40e6-8659-0773f69e0d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###A catalog named telecom_catalog_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6509c4-015b-4241-8efa-7143052aebaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS telecom_catalog_assign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef7df5d-38fd-406b-8bd4-1ee4080b91c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create A schema landing_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa85704-b4bc-456e-b713-ebe25320ad03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS telecom_catalog_assign.landing_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "812300c6-4607-4028-b641-7d50ad685b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Create A volume landing_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c870a8-f46b-4d93-8999-f9c4d5d3f176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS telecom_catalog_assign.landing_zone.landing_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47cd78a-1b2f-4b4f-a12d-e580783b6fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using dbutils.fs.mkdirs, create folders\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f18112-7630-4a7a-aaf4-b8460e8bcd6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol\"\n",
    "\n",
    "customer_path = f\"{base_path}/customer/\"\n",
    "usage_path    = f\"{base_path}/usage/\"\n",
    "tower_r1_path = f\"{base_path}/tower/region1/\"\n",
    "tower_r2_path = f\"{base_path}/tower/region2/\"\n",
    "\n",
    "dbutils.fs.mkdirs(customer_path)\n",
    "dbutils.fs.mkdirs(usage_path)\n",
    "dbutils.fs.mkdirs(tower_r1_path)\n",
    "dbutils.fs.mkdirs(tower_r2_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7af17f-dd11-49d6-81e8-8d903908978a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "accb7443-87b5-423a-a5b7-d19dac2b1b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Volume vs DBFS/FileStore<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023e5bb3-18b1-4c95-9201-151c749dd980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DBFS/FileStore is a legacy, workspace-scoped file system mainly for demos, while Volumes are Unity Catalog–governed, secure, production-ready storage for files.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc226f6-2417-4234-b75d-92df1d187bd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Volumes:- Volumes are a Unity Catalog–managed storage layer that provides secure, governed access to files<br>\n",
    "Governance:-Governance is control + rules + tracking.<br>\n",
    "It answers:<br>\n",
    "Who can access the data?<br>\n",
    "Who changed the data?<br>\n",
    "When was it changed?<br>\n",
    "Are rules being followed?<br>\n",
    "\n",
    "**In Databricks**<br>\n",
    "**With Unity Catalog (Volumes)**:<br>\n",
    "Databricks knows who accessed the file<br>\n",
    "Databricks knows who modified it<br>\n",
    "Permissions are enforced<br>\n",
    "\n",
    "**With DBFS/FileStore**:<br>\n",
    "No tracking<br>\n",
    "No rules<br>\n",
    "Anyone with workspace access can see it<br>\n",
    "\n",
    "**2. Access Control**<br>\n",
    "Access control = permission system<br>\n",
    "Examples:<br>\n",
    "Read only<br>\n",
    "Write<br>\n",
    "Full control<br>\n",
    "\n",
    "a. In Databricks Volumes<br>\n",
    "*Ex:- Only data_team can read*<br>\n",
    "GRANT READ ON VOLUME main.default.raw_data TO data_team; <br>\n",
    "\n",
    "b. DBFS/FileStore<br>\n",
    "No such control<br>\n",
    "Everyone can access<br>\n",
    "\n",
    "**3. Security (PROTECTING data)**<br>\n",
    "Security = preventing unauthorized access or misuse<br>\n",
    "Includes:<br>\n",
    "Authentication (who you are)<br>\n",
    "Authorization (what you can do)<br>\n",
    "Auditing (what you did)<br>\n",
    "Real-life analogy<br>\n",
    "ATM:<br>\n",
    "Card + PIN<br>\n",
    "Logs every transaction<br>\n",
    "\n",
    "a. In Databricks<br>\n",
    "Volumes<br>\n",
    "Encrypted<br>\n",
    "Role-based access<br>\n",
    "Audited<br>\n",
    "\n",
    "b. DBFS<br>\n",
    "Minimal protection<br>\n",
    "No audit trail<br>\n",
    "\n",
    "**4. Production Ready (SAFE for real business)**<br>\n",
    "Simple meaning<br>\n",
    "Production ready = safe for company-critical data<br>\n",
    "\n",
    "a. In Databricks<br>\n",
    "Volumes<br>\n",
    "Designed for pipelines<br>\n",
    "Safe for Jobs<br>\n",
    "Used in production<br>\n",
    "\n",
    "b. DBFS<br>\n",
    "Only for testing<br>\n",
    "Not supported for regulated workloads<br>\n",
    "\n",
    "**5. SQL Support (Can SQL directly use it?)**<br>\n",
    "Simple meaning<br>\n",
    "Can I manage it using SQL?<br>\n",
    "Example<br>\n",
    "a. Volumes<br>\n",
    "CREATE VOLUME main.default.sales_data;\n",
    "SELECT * FROM csv.`/Volumes/main/default/sales_data/file.csv`;\n",
    "\n",
    "b. DBFS\n",
    "❌ SQL cannot CREATE or MANAGE DBFS locations\n",
    "\n",
    "| Term             | DBFS/FileStore | Volumes  |\n",
    "| ---------------- | -------------- | -------- |\n",
    "| Governance       | ❌ None         | ✅ Full   |\n",
    "| Access Control   | ❌ No           | ✅ Yes    |\n",
    "| Security         | ❌ Weak         | ✅ Strong |\n",
    "| Production Ready | ❌ No           | ✅ Yes    |\n",
    "| SQL Support      | ❌ No           | ✅ Yes    |\n",
    "\n",
    "Final Memory Trick\n",
    "\n",
    "DBFS = Development / Demo\n",
    "Volumes = Production / Governed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57304fb-fd76-4f64-91e1-4df7fd798e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Why production teams prefer Volumes for regulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a4890b2-085d-4070-bad5-b9e18444402f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What Is “Regulated Data”?<br>\n",
    "Regulated data is information that is governed by legal, compliance, or internal policy rules, such as:\n",
    "\n",
    "- Healthcare data (HIPAA)\n",
    "- Financial data\n",
    "- Personally Identifiable Information (PII)\n",
    "- Customer or employee records\n",
    "- Enterprise reporting data\n",
    "\n",
    "For such data, control and traceability are mandatory, not optional.\n",
    "\n",
    "1. Strong Governance (Mandatory for Regulation)<br>\n",
    "What regulation requires<br>\n",
    "\n",
    "- Know who accessed data\n",
    "- Know who modified data\n",
    "- Enforce company policies\n",
    "\n",
    "How Volumes help<br>\n",
    "Volumes are governed by Unity Catalog, which provides:<br>\n",
    "- Centralized metadata\n",
    "- Access tracking\n",
    "- Audit logs\n",
    "\n",
    "Why DBFS fails<br>\n",
    "- No centralized governance\n",
    "- No audit trail\n",
    "\n",
    "✔ Regulators demand proof — Volumes provide it\n",
    "\n",
    "2. Fine-Grained Access Control (Least Privilege)\n",
    "Requirement<br>\n",
    "- Users must access only what they need\n",
    "- No broad workspace access\n",
    "\n",
    "Volumes support\n",
    "GRANT READ ON VOLUME main.default.phi_data TO analytics_team;\n",
    "- Read-only access\n",
    "- No accidental writes\n",
    "- Controlled sharing\n",
    "\n",
    "DBFS limitation\n",
    "- Either full access or none\n",
    "- No role-based control\n",
    "\n",
    "✔ This is critical for compliance frameworks\n",
    "\n",
    "\n",
    "4. Auditing & Compliance Evidence<br>\n",
    "Auditors ask:<br>\n",
    "Who read this file?<br>\n",
    "When was it changed?<br>\n",
    "Was access authorized?<br>\n",
    "Volumes can answer these questions via:<br>\n",
    "Unity Catalog audit logs<br>\n",
    "Access history<br>\n",
    "\n",
    "DBFS cannot.<br>\n",
    "✔ Without audit logs, compliance fails<br>\n",
    "\n",
    "| Requirement      | Volumes        | DBFS            |\n",
    "| ---------------- | -------------- | --------------- |\n",
    "| Governance       | ✅ Yes          | ❌ No            |\n",
    "| Access control   | ✅ Fine-grained | ❌ None          |\n",
    "| Security         | ✅ Strong       | ❌ Weak          |\n",
    "| Auditing         | ✅ Available    | ❌ Not available |\n",
    "| Production ready | ✅ Yes          | ❌ No            |\n",
    "| Compliance ready | ✅ Yes          | ❌ No            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3114bf05-91b5-4756-89c0-01d41a88ee19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14e7751-c1f7-4499-9aa1-2591bab9150e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create DataFrames from the Given Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec89aa48-d304-41d8-a77d-b9a09c75b73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df=spark.createDataFrame([(101,\"Arun\",31,\"Chennai\",\"PREPAID\"),\n",
    "    (102,\"Meera\",45,\"Bangalore\",\"POSTPAID\"),\n",
    "    (103,\"Irfan\",29,\"Hyderabad\",\"PREPAID\"),\n",
    "    (104,\"Raj\",52,\"Mumbai\",\"POSTPAID\"),\n",
    "    (105,None,27,\"Delhi\",\"PREPAID\"),\n",
    "    (106,\"Sneha\",None,\"Pune\",\"PREPAID\")],[\"customer_id\",\"name\",\"age\",\"city\",\"plan_type\"])\n",
    "\n",
    "usage_df = spark.createDataFrame([\n",
    "    (101,320,1500,20),\n",
    "    (102,120,4000,5),\n",
    "    (103,540,600,52),\n",
    "    (104,45,200,2),\n",
    "    (105,0,0,0)\n",
    "], [\"customer_id\",\"voice_mins\",\"data_mb\",\"sms_count\"])\n",
    "\n",
    "tower_r1_df = spark.createDataFrame([\n",
    "    (5001,101,\"TWR01\",-80,\"2025-01-10 10:21:54\"),\n",
    "    (5004,104,\"TWR05\",-75,\"2025-01-10 11:01:12\")\n",
    "], [\"event_id\",\"customer_id\",\"tower_id\",\"signal_strength\",\"timestamp\"])\n",
    "\n",
    "#Tower Logs – Region 2 (empty)\n",
    "tower_r2_df = spark.createDataFrame([], tower_r1_df.schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c839bc-cc5c-4647-9b71-267b48881d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Copy (Write) the Data into the Volume Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91a4573-4243-45e9-975b-cd22a1823ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df.write.mode(\"overwrite\").csv(customer_path,header=True)\n",
    "usage_df.write.mode(\"overwrite\").csv(usage_path,header=True)\n",
    "tower_r1_df.write.mode(\"overwrite\").csv(tower_r1_path,header=True)\n",
    "tower_r2_df.write.mode(\"overwrite\").csv(tower_r2_path,header=True)\n",
    "display(customer_df)\n",
    "display(usage_df)\n",
    "display(tower_r1_df)\n",
    "display(tower_r2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67a066b1-0832-42ce-82b5-b2305f2be6d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Validate That Files Were Successfully Copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3661a01-469b-4fd1-a528-abd9e6931785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943b3535-8701-4a17-9465-5ae894b328c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check each dataset\n",
    "dbutils.fs.ls(customer_path)\n",
    "dbutils.fs.ls(usage_path)\n",
    "dbutils.fs.ls(tower_r1_path)\n",
    "dbutils.fs.ls(tower_r2_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb03a707-9532-4d63-a677-7786732dc9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Read all tower logs using: \n",
    "- Path glob filter (example: *.csv)\n",
    "- Multiple paths input\n",
    "- Recursive lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972a8ad7-b9a8-44c3-9247-4e5b9cb9337a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "##Directory Read Use Cases Using Path Glob Filter (*.csv)\n",
    "df1=spark.read.option(\"header\",\"True\").option(\"sep\",\",\").csv(f\"{base_path}/tower/*/*.csv\")\n",
    "df1.count()\n",
    "df1.display(3)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19e5b7a-5f4b-4a95-b162-bd2e0c7c8cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Multiple paths input\n",
    "df1=spark.read.options(header=\"True\",sep=\",\",inferSchema=True).csv(path=[\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\",\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"])\n",
    "df1.count()\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcfd518-7bbb-4102-a85f-90d2bfd66e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "##Recursive lookup\n",
    "df1 = spark.read.csv(\n",
    "    path = [f\"{base_path}/tower/*\"],\n",
    "    inferSchema = True,\n",
    "    header = True,\n",
    "    sep = \",\",\n",
    "    pathGlobFilter = \"*.csv\",\n",
    "    recursiveFileLookup = True\n",
    ")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b282fedd-870e-4129-9906-dc55e7d3bd51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## pathGlobFilter<br>\n",
    "Reads only files matching a pattern (e.g., *.csv) from subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672e91c9-120c-4ca2-a802-8670c643330e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.option(\"header\",\"True\").option(\"sep\",\",\").option(\"pathGlobalFilter\",\".csv\").option(\"inferSchema\",\"True\").csv(f\"{base_path}/tower/*\")\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efab0e8c-8c06-4561-bc89-8501e0c8b909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Using List of Paths in spark.read.csv([path1, path2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd49b9f2-cb4f-4c13-8f5c-780a6532411d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_of_path=f\"{base_path}/tower/*\"\n",
    "df1=spark.read.option(\"header\",\"True\").option(\"sep\",\",\").option(\"pathGlobalFilter\",\".csv\").csv(list_of_path)\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4b02af-5c5f-4be2-8eba-b8be12d06b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"]\n",
    "df3=spark.read.options(header='True', inferSchema='True',sep=',',pathGlobalFilter='.csv',recursiveFileLookup='True').csv(path)\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe3e021-1fcb-427f-abe2-7a407ecf7d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Using recursiveFileLookup<br>\n",
    "What it does<br>\n",
    "Recursively reads all files in all subdirectories.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3361e79-e6d0-4586-acdc-79592bdb9497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_recursive = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").option(\"pathGlobFilter\", \"*.csv\").option(\"recursiveFileLookup\", \"true\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "df_recursive.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7f669d-0f4f-4040-b573-5a281d449db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c835c1d-268d-498f-b6c5-e1a3d9053b0c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765650924506}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.format(\".csv\").options(header=\"False\",inferSchema=\"False\").csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\")\n",
    "print(df1.printSchema())\n",
    "df1.display(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cfffa5d-e513-4e85-ab2a-6f59e8daf2c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**From Above results we see**<br>\n",
    "- Column names are auto-generated (_c0, _c1, …)<br>\n",
    "- All columns are STRING<br>\n",
    "- Header row is treated as data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e561295-370c-4e48-9eee-2c772db1f1f5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765651544519}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.format(\".csv\").options(header=\"True\",inferSchema=\"True\").csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\")\n",
    "print(df1.printSchema())\n",
    "df1.display(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd6e0de-fe0c-4022-89e7-b2f1b876cb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**From above results we see**<br>\n",
    "- First row used as column names<br>\n",
    "- Spark tries to detect data types<br>\n",
    "- age becomes STRING (explained below)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1981533c-1712-4a61-bbe2-79e86d98c55e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##What Changed When Using header and inferSchema?<br>\n",
    "**Header**\n",
    "| Value | Effect                         |\n",
    "| ----- | ------------------------------ |\n",
    "| false | First row treated as data      |\n",
    "| true  | First row used as column names |<br>\n",
    "\n",
    "**inferSchema Option**\n",
    "| Value | Effect                               |\n",
    "| ----- | ------------------------------------ |\n",
    "| false | All columns read as STRING           |\n",
    "| true  | Spark samples data and assigns types |<br>\n",
    "\n",
    "What Spark Does\n",
    "- Spark samples multiple rows\n",
    "- Sees numeric values: 31, 45, 29\n",
    "- Sees non-numeric value: \"abc\"\n",
    "- Cannot safely cast entire column to INTEGER\n",
    "- Spark always chooses the safest common type\n",
    "- It will not partially fail or cast invalid rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02daf487-3c08-45c1-9bb5-64923d567abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How schema inference handled “abc” in age?<br>**\n",
    "- How Schema Inference Works (Step by Step)\n",
    "- Spark samples the data when inferSchema=true.\n",
    "- It tries to determine a single data type that can hold all values.\n",
    "- Spark will not partially fail or drop rows during inference.\n",
    "- Since \"abc\" cannot be cast to an integer, Spark cannot safely choose INT.\n",
    "- Spark falls back to STRING for the entire column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f00b03-91f7-47f0-aa94-a4e28ec09396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To provide schema (columname & datatype), what are the 2 basic options available that we learned so far ? inferSchema/toDF<br>\n",
    "We are going to learn additionally 2 more options to handle schema (colname & datatype)?<br>\n",
    "1. Using simple string format of define schema.<br>\n",
    "IMPORTANT: 2. Using structure type to define schema.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c91747-6274-46c5-b2a2-61d40ba97f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers.\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/cust_1.txt\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show()\n",
    "csv_df2=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/cust_1.txt\",inferSchema='True').toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df2.printSchema())\n",
    "\n",
    "#1. Using simple string format of define custom simple schema.\n",
    "str_struct=\"id integer,fname string,lname string,age integer,prof string\"\n",
    "csv_df3=spark.read.schema(str_struct).csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\")\n",
    "print(csv_df3.printSchema())\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "#2. Using StructType and StructField to define custom schema.\n",
    "schema=StructType([StructField(\"id\",IntegerType(),True),StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"prof\",StringType(),True)])\n",
    "csv_df4=spark.read.schema(schema).csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\")\n",
    "print(csv_df4.printSchema())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988e6be0-1631-49f3-a272-ab9a92383599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e117c8-5e8f-4bd6-9af6-445f35afd522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Apply column names using string using<br> toDF function for customer data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f33cf6-24b8-4d2f-b4ee-173cab6137e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.options(header='True',inferSchema='True',sep=',').csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\").toDF(\"ID\",\"FirstName\",\"LastName\",\"Age\",\"Prof\")\n",
    "print(df1.printSchema())\n",
    "df1.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55b66a6-1115-4ea5-94f8-d8588e9958c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Apply column names and datatype using the schema function for usage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008f5c56-6a25-44d9-bc26-2c147dce49b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_fun=\"ID integer,FirstName string,LastName string,Age integer,Prof string\"\n",
    "df1=spark.read.schema(schema_fun).options(header='True',inferSchema='True',sep=',').csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\")\n",
    "print(df1.printSchema())\n",
    "df1.show(20)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea1d135-e6bc-4817-bfd7-c7bafaec1aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb7ba16-9aff-4384-996e-6fcc16b8be6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema=StructType([StructField(\"ID\",IntegerType(),True),StructField(\"FirstName\",StringType(),True),StructField(\"LastName\",StringType(),True),StructField(\"Age\",IntegerType(),True),StructField(\"Prof\",StringType(),True)])\n",
    "df1=spark.read.schema(schema).csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\")\n",
    "print(df1.printSchema())\n",
    "df1.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb12247-1997-4088-b4e4-1281432b0ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will use format to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d8b0ed1-6550-4151-bab0-9e85bcbc5566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.format(\"csv\").options(header=\"True\",inferSchema=\"True\",sep=',').load(\"/Volumes/catalog2/database2/volume2/created_folder/patients.csv\")\n",
    "print(df1.printSchema())\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575f1e5d-da19-4242-ad78-b8c0a9062805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Write Operations using \n",
    "- csv, json, orc, parquet, delta, saveAsTable, insertInto, xml with different write mode, header and sep options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec6916e-3214-467a-982b-b12be10e3c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Write Operations (Data Conversion/Schema migration) – CSV Format Usecases\n",
    "1. Write customer data into CSV format using overwrite mode\n",
    "2. Write usage data into CSV format using append mode\n",
    "3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7565f03c-9025-443a-8240-c279eb44fe13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Write customer data into CSV format using overwrite mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d312530-a357-4769-8008-bed6a8b11f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_cust_csv=spark.read.format(\"csv\").options(header=\"True\",sep=',',inferSchema=\"True\").load(\"/Volumes/workspace/default/processed/CASE3/Customer/customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809bd06b-9aea-45f5-abbe-83435f6639e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_cust_csv.write.format(\"csv\").mode(\"overwrite\").options(header=\"True\",sep=\",\").save(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd22e9a3-70a6-46e8-a448-3b73dffc6880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write usage data into CSV format using append mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26761ee3-db87-43be-bd3b-66a7cc1bacca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_usage_csv=spark.read.csv(\"/Volumes/workspace/default/processed/CASE3/Usage/usage_day1.csv\",header=True,inferSchema=True,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e04fcb-36d7-47da-b89d-71de3af8efec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df1_usage_csv.write.format(\"csv\").mode(\"append\").options(header=\"True\",sep=\",\").save(\"/Volumes/workspace/default/processed/CASE3/Usage/usage_csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7209e038-af48-4509-8e6c-6b48396b4929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write tower data into CSV format with header enabled and custom separator (|)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e657d2e-5bc3-4b04-850b-c4598e068c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower_csv=spark.read.csv(\"/Volumes/workspace/default/processed/CASE3/Tower/\",header=True,inferSchema=True,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f563ad8-7e74-4f22-995f-276e8d2dfcb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower_csv.write.format(\"csv\").mode(\"overwrite\").options(header=\"True\",sep=\"|\").save(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0d9647-f3cd-42d8-a359-3f543a7a23fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. Read the tower data in a dataframe and show only 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665e3b2a-3804-4fbb-9630-a9994d96d57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_data_read=spark.read.format(\"csv\").options(header=\"True\",inferSchema=\"True\",sep=\"|\").load(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_csv\")\n",
    "tower_data_read.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de72acd1-1635-4e3f-b5a1-e0f695d9ee1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53c0600-1834-4f6b-a424-c3332266dd47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Write Operations (Data Conversion/Schema migration)– JSON Format Usecases\n",
    "1. Write customer data into JSON format using overwrite mode\n",
    "2. Write usage data into JSON format using append mode and snappy compression format\n",
    "3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73d31e6-2954-4542-b3cc-1b819db5b3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Write customer data into JSON format using overwrite mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a23dca0-6bb9-40b0-aad4-4f3d8de9a0fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_cust_csv.write.mode(\"overwrite\").format(\"json\").options(header=\"True\").save(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ac302e-4046-479b-8424-d989df71d21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Write usage data into JSON format using append mode and snappy compression format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883ec2e6-50c5-498d-999d-326db5d496cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_usage_csv.write.format(\"json\").mode(\"append\").options(header=\"True\",compression=\"snappy\").save(\"/Volumes/workspace/default/processed/CASE3/Usage/usage_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32a9ca7b-21d8-4f9c-a57f-4ff44c4538ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Write tower data into JSON format using ignore mode and observe the behavior of this mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd5fd62-d5fc-4f7b-b270-b9bb4f4efe91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower_csv.write.format(\"json\").mode(\"ignore\").options(header=\"True\").save(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14630679-1862-4954-90e3-7f29ca7a5763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Read the tower data in a dataframe and show only 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e70436-c215-456b-8722-2f031e175ab1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766315123575}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_json=spark.read.format(\"json\").load(\"/Volumes/workspace/default/raw/Data/jsontower/tower_json\")\n",
    "tower_json.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb59287-9a4f-4af9-8b39-52b863f0484c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55766309-25b1-418d-aec6-83896d573228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8. Write Operations (Data Conversion/Schema migration) – Parquet Format Usecases\n",
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "2. Write usage data into Parquet format using error mode\n",
    "3. Write tower data into Parquet format with gzip compression option\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b9d77f-3940-467b-8234-f1d20c87aa7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write customer data into Parquet format using overwrite mode and in a gzip format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217de8ab-ece7-4f03-89ae-e66f7bc92e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_cust_csv.write.format(\"parquet\").mode(\"overwrite\").options(header=\"True\",compression=\"gzip\").save(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7750a76e-ddcc-4b20-8817-5a5242fe71ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Write usage data into Parquet format using error mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f790fd4f-3b18-49ab-90f3-f84cd6fc600f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_usage_csv.write.format(\"parquet\").mode(\"error\").options(header=\"True\",compression=\"snappy\").save(\"/Volumes/workspace/default/processed/CASE3/Usage/usage_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe056f8-605e-48c0-a2b8-553e42a5f416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Write tower data into Parquet format with gzip compression option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f700d0-415a-4262-80f0-c059e7e47936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower_csv.write.format(\"parquet\").mode(\"append\").options(header=\"True\",compression=\"gzip\").save(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5176c532-60a4-4de3-a9c9-c3bb2a8bb67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Read the usage data in a dataframe and show only 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a83cbd8-3dea-438b-a7dc-380875bd73f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"parquet\").load(\"/Volumes/workspace/default/processed/CASE3/Usage/usage_parquet\")\n",
    "df.display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7f6c1d-5c68-487a-905a-3c7fc68b7556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b77d223-1fbe-496a-ba22-5b7e247d5d7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##9. Write Operations (Data Conversion/Schema migration) – Orc Format Usecases\n",
    "1. Write customer data into ORC format using overwrite mode\n",
    "2. Write usage data into ORC format using append mode\n",
    "3. Write tower data into ORC format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5742439f-0e31-4696-b2ab-22c66f3a1d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write customer data into ORC format using overwrite mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882bc7cb-fc1d-45e8-b515-f12611a3c809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_cust_csv.write.mode(\"overwrite\").format(\"orc\").save(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2d2f444-ba58-4cdf-92fe-06040469e775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write usage data into ORC format using append mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69a45ad-6d92-47b1-843b-5643f2227e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_usage_csv.write.format(\"orc\").mode(\"append\").save(\"/Volumes/workspace/default/processed/CASE3/Usage/usage_orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2056d3-2d05-4cc8-867f-02b5c82fb26a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write tower data into ORC format and see the output file structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e70a353-004b-4df4-80f6-1c3e5809f5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower_csv.write.format(\"orc\").save(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1fa5c29-a9c2-4898-866a-cb627748281f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Read the usage data in a dataframe and show only 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f8ed6a-cbed-4aac-a08f-d26ce3c73c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_orc=spark.read.format(\"orc\").load(\"/Volumes/workspace/default/processed/CASE3/Usage/usage_orc/\")\n",
    "read_orc.display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c5b4b1-e340-44e9-9902-8eb49e2927b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##10. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases\n",
    "1. Write customer data into Delta format using overwrite mode\n",
    "2. Write usage data into Delta format using append mode\n",
    "3. Write tower data into Delta format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32901024-dfb9-4c77-a46b-e7efc8de94c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write customer data into Delta format using overwrite mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30e875c-9e7b-4833-ada0-9c42b764cfe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_cust_csv.write.mode(\"overwrite\").format(\"delta\").save(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa2f72c-f3ed-477e-b048-26f83aba0822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write usage data into Delta format using append mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0618a0aa-656a-4aa6-bdb4-b251e267b00b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_usage_csv.write.mode(\"append\").format(\"delta\").save(\"/Volumes/workspace/default/processed/CASE3/Usage/usage_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a720561d-e759-46df-98a1-54ff8408f6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write tower data into Delta format and see the output file structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717aaf56-f887-416a-80b7-4564bb961284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tower_csv.write.format(\"delta\").save(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a019e6-d9c3-4163-b499-ad9519a7fd43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Read the usage data in a dataframe and show only 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cde8810-33e3-4dc7-b8e4-e1cc6de3f960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_delta_df=spark.read.format(\"delta\").load(\"/Volumes/workspace/default/processed/CASE3/Tower/tower_delta\")\n",
    "display(read_delta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2586d659-e684-41e7-bbb3-75a83f22cb03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec72a143-bf7b-46ca-84b8-ec01e332dd06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only.**\n",
    "\n",
    "-- Parquet Location vs Delta Location\n",
    "1. The core confusion (and the correct answer)<br>\n",
    "\n",
    "Yes, Delta tables store data as Parquet files.<br>\n",
    "But Parquet alone ≠ Delta.<br>\n",
    "\n",
    "The differentiating factor is the transaction log (_delta_log).<br>\n",
    "That _delta_log folder is the entire difference.<br>\n",
    "\n",
    "3. What Parquet-only gives you<br>\n",
    "Parquet is a file format, nothing more<br>\n",
    "It provides:<br>\n",
    "-- Columnar storage<br>\n",
    "-- Compression<br>\n",
    "-- Faster reads<br>\n",
    "\n",
    "It does NOT provide:<br>\n",
    "-- ACID transactions<br>\n",
    "-- Versioning<br>\n",
    "-- Schema enforcement<br>\n",
    "-- Concurrent write safety<br>\n",
    "-- Time travel<br>\n",
    "\n",
    "4. What Delta adds on top of Parquet<br>\n",
    "\n",
    "Delta = Parquet files + Transaction Log<br>\n",
    "\n",
    "The _delta_log provides<br>\n",
    "\n",
    "| Capability              | Parquet Only | Delta |\n",
    "| ----------------------- | ------------ | ----- |\n",
    "| Columnar storage        | Yes          | Yes   |\n",
    "| Compression             | Yes          | Yes   |\n",
    "| Schema enforcement      | No           | Yes   |\n",
    "| ACID transactions       | No           | Yes   |\n",
    "| Time travel             | No           | Yes   |\n",
    "| MERGE / UPDATE / DELETE | No           | Yes   |\n",
    "| Concurrent writes       | Unsafe       | Safe  |\n",
    "| Rollback                | No           | Yes   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377409f4-d79a-466f-816a-ca6bccecb252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##11. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using saveAsTable() as a managed table\n",
    "2. Write usage data using saveAsTable() with overwrite mode\n",
    "3. Drop the managed table and verify data removal\n",
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "5. Use spark.read.sql to write some simple queries on the above tables created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805ef654-2364-4da4-950f-c669f60a935e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write customer data using saveAsTable() as a managed table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c81183-370f-4286-9694-acec95ff1aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG if not EXISTS training;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a275123-d934-481d-ab6d-a8d501eb62d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS training.raw;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135d388e-cc87-4ff9-a8bb-0500aef5d8d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_cust_csv.write.saveAsTable(\"training.raw.customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579a1524-9d68-4b7b-ac4d-659d12fad111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Write usage data using saveAsTable() with overwrite mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22015137-17c9-4b35-a1d2-4ea6c60dad7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_usage_csv.write.mode(\"overwrite\").saveAsTable(\"training.raw.usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d4e298-fcc8-4890-94b5-95ff48890802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Drop the managed table and verify data removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f97bc0-f35f-4e86-9cd1-bf7b6ec33ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE training.raw.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5de302d-3d97-455b-8db5-e1b67626b6c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. Go and check the table overview and realize it is in delta format in the Catalog.**<br>\n",
    "->Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d25fe2b-f457-47c3-96ec-3f9ec7421516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Use spark.read.sql to write some simple queries on the above tables created.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ad897c-7639-4a82-9af7-60e5b03e9ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from customers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9258bba3-7adf-405f-be32-546803652136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select customer_name,city,plan_type from customers where customer_id='C001'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d96384-a771-4af0-8f57-8d4d2aeb1370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d5db7f-e62d-495a-93c7-a28d14b15914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##12. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using insertInto() in a new table and find the behavior\n",
    "2. Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "739b70fe-b1c0-471b-94c1-f8eae4b7903d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Write customer data using insertInto() in a new table and find the behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665eda3c-41a2-40fd-986f-35dee97484ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_cust_csv.write.insertInto(\"training.raw.customers_new\",overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "878d3073-c259-44ff-a0ad-4d782969bc12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write usage data using insertTable() with overwrite mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11013c09-f037-46e2-9fd1-5778c9aeb62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_usage_csv.write.insertInto(\"training.raw.usage\",overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44ceea98-6deb-40b9-8f23-abad82ad1ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##13. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data into XML format using rowTag as cust\n",
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd36ea0c-c1fd-4ecc-a850-2d677e086e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write customer data into XML format using rowTag as cust**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ed1a04-012f-40c2-91ba-f1b00211b4b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_cust_csv.write.mode(\"overwrite\").xml(\"/Volumes/workspace/default/processed/CASE3/Customer/customer_xml\",rowTag=\"cust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2023b221-d184-4f5d-98b9-8ca2485c1381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write usage data into XML format using overwrite mode with the rowTag as usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876f2a0f-b1fd-4bc0-9931-f133a2dca7c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1_usage_csv.write.mode(\"overwrite\").xml(\"/Volumes/workspace/default/processed/CASE3/Usage/usage_xml\",rowTag=\"usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e40b9ff-7bf8-402f-adc0-8429db8d5ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Download the xml data and open the file in notepad++ and see how the xml file looks like.**\n",
    "- Yes, Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "680bcffd-7b7b-430a-bcee-179b2a6dfd87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##14. Compare all the downloaded files (csv, json, orc, parquet, delta and xml) <br>\n",
    "1. Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big.<br>\n",
    "\n",
    "*Orginal **customer.csv** file size is 286.00 B*\n",
    "- cust_csv----->280.00 B--->No compression<br>\n",
    "- cust_delta--->1.69KB---->Snappy compresion<br>\n",
    "- cust_json---->602.00 B--->no compression<br>\n",
    "- cust_orc------>1 KB--->Snappy compression<br>\n",
    "- cust_parquet--->2.04KB---->gzip compression<br>\n",
    "- cust_xml------>1.19KB--->no compression<br>\n",
    "\n",
    "Compression reduces data size only when the data volume is large enough to amortize format overhead.<br>\n",
    "Which means<br>\n",
    "Format overhead is extra information stored in a file that is not your actual data, such as:<br>\n",
    "Schema definitions<br>\n",
    "Column metadata-extra information about each column<br>\n",
    "Like Minimum value,Maximum value,Number of nulls for each column<br>\n",
    "\n",
    "3. Format-by-format explanation<br>\n",
    "🔹 CSV (286 B → 280 B)<br>\n",
    "- Row-based<br>\n",
    "- No schema storage<br>\n",
    "- No metadata<br>\n",
    "- Almost zero overhead<br>\n",
    "- This is why it stays smallest.<br>\n",
    "\n",
    "**JSON (602 B, no compression)**<br>\n",
    "- Why larger than CSV<br>\n",
    "- Repeats column names on every row<br>\n",
    "- Uses structural characters { } : , \"<br>\n",
    "- UTF-8 text<br>\n",
    "- JSON is self-describing, which costs space<br>\n",
    "\n",
    "**🔹 XML (1.19 KB, no compression)**<br>\n",
    "- Why even larger<br>\n",
    "- Opening and closing tags<br>\n",
    "- Deep verbosity<br>\n",
    "- Repeated tag names<br>\n",
    "\n",
    "**🔹 Parquet (2.04 KB, gzip)**<br>\n",
    "Why bigger despite gzip<br>\n",
    "- Columnar format<br>\n",
    "- Schema<br>\n",
    "- Column metadata<br>\n",
    "\n",
    "**ORC (1 KB, Snappy)**<br>\n",
    "Same story as Parquet<br>\n",
    "\n",
    "**🔹 Delta (1.69 KB, Snappy)**<br>\n",
    "Delta is not just a file format.<br>\n",
    "\n",
    "It includes:<br>\n",
    "- Parquet files<br>\n",
    "- _delta_log JSON transaction log<br>\n",
    "- Versioning metadata<br>\n",
    "- ACID guarantees<br>\n",
    "- Even for one row, Delta must create logs.<br>\n",
    "- This is why Delta is never optimal for tiny datasets.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad490558-3c27-444a-9f04-b98ac0bb1306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "###15. Try to do permutation and combination of performing Schema Migration & Data Conversion operations like...\n",
    "1. Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format\n",
    "2. Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format\n",
    "3. Read any one of the above delta data in a dataframe and write it to dbfs in a xml format\n",
    "4. Read any one of the above delta table in a dataframe and write it to dbfs in a json format\n",
    "5. Read any one of the above delta table in a dataframe and write it to another table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b532610-ebb5-4759-b0a6-f95e4a43b632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65aa78a8-229e-4af9-8fec-ffa416540ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read any one of the above orc data in a dataframe\n",
    "orc_df=spark.read.format(\"orc\").load(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1a4af0-0832-4e9d-b1bd-b52b80ba409b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write it to dbfs in a parquet format\n",
    "orc_df.write.parquet(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_parquet_write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "406a49c7-6a91-444d-9b8d-d3dc0c0a990f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48b2754-005c-4448-80ab-17ef08b9f58f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read any one of the above parquet data in a dataframe\n",
    "parquet_df=spark.read.format(\"parquet\").load(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6522c44-c6dd-4896-b553-8b457287f593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write it to dbfs in a delta format\n",
    "parquet_df.write.format(\"delta\").save(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_delta_write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "834a6f5e-26d6-47ec-a9bc-bbab804dc808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Read any one of the above delta data in a dataframe and write it to dbfs in a xml format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a7c9be-3f92-4524-8a41-8a79c3075ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read any one of the above delta data in a dataframe\n",
    "delta_df=spark.read.format('delta').load(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_delta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2443c8-2220-4913-8f77-8f9eb726e08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write it to dbfs in a xml format\n",
    "delta_df.write.xml(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_xml_write\",rowTag=\"cust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ded843-f7fc-4a97-8409-d8ad789e0310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. Read any one of the above delta table in a dataframe and write it to dbfs in a json format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9198f9cc-8aca-45f2-ae9f-bfa74a156877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read any one of the above delta table in a dataframe\n",
    "delta_df=spark.read.format('delta').load(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_delta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee26605-fac5-4620-9ddb-4cba254f3099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write it to dbfs in a json format\n",
    "delta_df.write.json(\"/Volumes/workspace/default/processed/CASE3/Customer/cust_json_write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bd3fdf7-63da-49ca-8ba6-8f952f5c6668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Read any one of the above delta table in a dataframe and write it to another table**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308753f8-f865-43dc-b185-41a59c92e1a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read any one of the above delta table in a dataframe\n",
    "df_delta_src=spark.table(\"workspace.default.customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44ed000-e106-432b-a1b8-d34efad804fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write it to another table\n",
    "df_delta_src.write.format('delta').saveAsTable(\"workspace.default.customers_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de6f8de4-7158-4763-bf61-a8cc0306c7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 16. Final Exercise: When to Use Each File Format (Simple One-Liners)\n",
    "\n",
    "### 1. CSV (Comma-Separated Values)\n",
    "**When to use / Benefits:**\n",
    "Data is organized in rows and column<br>  \n",
    "When to use<br>\n",
    "- Small files<br>\n",
    "- Learning, testing, or debugging<br>\n",
    "\n",
    "Benefits<br>\n",
    "- Very easy to read<br>\n",
    "- Opens in Excel<br>\n",
    "- Almost no overhead<br>\n",
    "\n",
    "---\n",
    "\n",
    "### 2. JSON (JavaScript Object Notation)\n",
    "**When to use / Benefits:**  \n",
    "Use JSON when data is semi-structured or hierarchical (nested). It is flexible, self-describing, and commonly used for APIs and data exchange.<br>\n",
    "- Data coming from APIs<br>\n",
    "- Web applications<br>\n",
    "- Data has nested information (object inside object)<br>\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ORC (Optimized Row Columnar)\n",
    "**When to use / Benefits:**  \n",
    "- High compression<br>\n",
    "- Fast reads<br>\n",
    "- Stores column statistics<br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Parquet\n",
    "**When to use / Benefits:**  \n",
    "- Column-based (reads only required columns)<br>\n",
    "- Good compression<br>\n",
    "- Very fast for analytics<br>\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Delta (Delta Lake format)\n",
    "**When to use / Benefits:**  \n",
    "Use Delta when you need reliability on a data lake. It adds ACID transactions, schema enforcement, time travel, and data versioning on top of Parquet.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. XML\n",
    "**When to use / Benefits:**  \n",
    "Use XML when working with legacy systems or strict document-based integrations. It is highly structured and self-describing but very verbose and inefficient for large data processing.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Delta Tables\n",
    "**When to use / Benefits:**  \n",
    "- ACID transactions<br>\n",
    "- Schema enforcement<br>\n",
    "- Handles large-scale data safely<br>\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6723119100374737,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Read_Write_UseCase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
