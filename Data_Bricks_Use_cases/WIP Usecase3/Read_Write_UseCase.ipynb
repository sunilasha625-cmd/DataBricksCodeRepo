{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1973442c-8f18-4c2c-8ad7-e11941f98382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain ReadOps Assignment\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e51d0b5-9fb8-4827-9b12-d06cdfee1a08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6da18272-81fc-4e17-9c5c-d0589039cc40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b56d3fb-2fe9-40ee-8442-faf06669c659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print(spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "799b004f-9db8-475f-84b3-37198ff8941c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Write SQL statements to create:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afa88cb3-1954-40e6-8659-0773f69e0d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###A catalog named telecom_catalog_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6509c4-015b-4241-8efa-7143052aebaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS telecom_catalog_assign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ef7df5d-38fd-406b-8bd4-1ee4080b91c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create A schema landing_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa85704-b4bc-456e-b713-ebe25320ad03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS telecom_catalog_assign.landing_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "812300c6-4607-4028-b641-7d50ad685b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Create A volume landing_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c870a8-f46b-4d93-8999-f9c4d5d3f176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS telecom_catalog_assign.landing_zone.landing_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e47cd78a-1b2f-4b4f-a12d-e580783b6fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using dbutils.fs.mkdirs, create folders\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f18112-7630-4a7a-aaf4-b8460e8bcd6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol\"\n",
    "\n",
    "customer_path = f\"{base_path}/customer/\"\n",
    "usage_path    = f\"{base_path}/usage/\"\n",
    "tower_r1_path = f\"{base_path}/tower/region1/\"\n",
    "tower_r2_path = f\"{base_path}/tower/region2/\"\n",
    "\n",
    "dbutils.fs.mkdirs(customer_path)\n",
    "dbutils.fs.mkdirs(usage_path)\n",
    "dbutils.fs.mkdirs(tower_r1_path)\n",
    "dbutils.fs.mkdirs(tower_r2_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f7af17f-dd11-49d6-81e8-8d903908978a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "accb7443-87b5-423a-a5b7-d19dac2b1b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Volume vs DBFS/FileStore<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023e5bb3-18b1-4c95-9201-151c749dd980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DBFS/FileStore is a legacy, workspace-scoped file system mainly for demos, while Volumes are Unity Catalog–governed, secure, production-ready storage for files.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc226f6-2417-4234-b75d-92df1d187bd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Volumes:- Volumes are a Unity Catalog–managed storage layer that provides secure, governed access to files<br>\n",
    "Governance:-Governance is control + rules + tracking.<br>\n",
    "It answers:<br>\n",
    "Who can access the data?<br>\n",
    "Who changed the data?<br>\n",
    "When was it changed?<br>\n",
    "Are rules being followed?<br>\n",
    "\n",
    "**In Databricks**<br>\n",
    "**With Unity Catalog (Volumes)**:<br>\n",
    "Databricks knows who accessed the file<br>\n",
    "Databricks knows who modified it<br>\n",
    "Permissions are enforced<br>\n",
    "\n",
    "**With DBFS/FileStore**:<br>\n",
    "No tracking<br>\n",
    "No rules<br>\n",
    "Anyone with workspace access can see it<br>\n",
    "\n",
    "**2. Access Control**<br>\n",
    "Access control = permission system<br>\n",
    "Examples:<br>\n",
    "Read only<br>\n",
    "Write<br>\n",
    "Full control<br>\n",
    "\n",
    "a. In Databricks Volumes<br>\n",
    "*Ex:- Only data_team can read*<br>\n",
    "GRANT READ ON VOLUME main.default.raw_data TO data_team; <br>\n",
    "\n",
    "b. DBFS/FileStore<br>\n",
    "No such control<br>\n",
    "Everyone can access<br>\n",
    "\n",
    "**3. Security (PROTECTING data)**<br>\n",
    "Security = preventing unauthorized access or misuse<br>\n",
    "Includes:<br>\n",
    "Authentication (who you are)<br>\n",
    "Authorization (what you can do)<br>\n",
    "Auditing (what you did)<br>\n",
    "Real-life analogy<br>\n",
    "ATM:<br>\n",
    "Card + PIN<br>\n",
    "Logs every transaction<br>\n",
    "\n",
    "a. In Databricks<br>\n",
    "Volumes<br>\n",
    "Encrypted<br>\n",
    "Role-based access<br>\n",
    "Audited<br>\n",
    "\n",
    "b. DBFS<br>\n",
    "Minimal protection<br>\n",
    "No audit trail<br>\n",
    "\n",
    "**4. Production Ready (SAFE for real business)**<br>\n",
    "Simple meaning<br>\n",
    "Production ready = safe for company-critical data<br>\n",
    "\n",
    "a. In Databricks<br>\n",
    "Volumes<br>\n",
    "Designed for pipelines<br>\n",
    "Safe for Jobs<br>\n",
    "Used in production<br>\n",
    "\n",
    "b. DBFS<br>\n",
    "Only for testing<br>\n",
    "Not supported for regulated workloads<br>\n",
    "\n",
    "**5. SQL Support (Can SQL directly use it?)**<br>\n",
    "Simple meaning<br>\n",
    "Can I manage it using SQL?<br>\n",
    "Example<br>\n",
    "a. Volumes<br>\n",
    "CREATE VOLUME main.default.sales_data;\n",
    "SELECT * FROM csv.`/Volumes/main/default/sales_data/file.csv`;\n",
    "\n",
    "b. DBFS\n",
    "❌ SQL cannot CREATE or MANAGE DBFS locations\n",
    "\n",
    "| Term             | DBFS/FileStore | Volumes  |\n",
    "| ---------------- | -------------- | -------- |\n",
    "| Governance       | ❌ None         | ✅ Full   |\n",
    "| Access Control   | ❌ No           | ✅ Yes    |\n",
    "| Security         | ❌ Weak         | ✅ Strong |\n",
    "| Production Ready | ❌ No           | ✅ Yes    |\n",
    "| SQL Support      | ❌ No           | ✅ Yes    |\n",
    "\n",
    "Final Memory Trick\n",
    "\n",
    "DBFS = Development / Demo\n",
    "Volumes = Production / Governed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f57304fb-fd76-4f64-91e1-4df7fd798e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Why production teams prefer Volumes for regulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a4890b2-085d-4070-bad5-b9e18444402f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What Is “Regulated Data”?<br>\n",
    "Regulated data is information that is governed by legal, compliance, or internal policy rules, such as:\n",
    "\n",
    "- Healthcare data (HIPAA)\n",
    "- Financial data\n",
    "- Personally Identifiable Information (PII)\n",
    "- Customer or employee records\n",
    "- Enterprise reporting data\n",
    "\n",
    "For such data, control and traceability are mandatory, not optional.\n",
    "\n",
    "1. Strong Governance (Mandatory for Regulation)<br>\n",
    "What regulation requires<br>\n",
    "\n",
    "- Know who accessed data\n",
    "- Know who modified data\n",
    "- Enforce company policies\n",
    "\n",
    "How Volumes help<br>\n",
    "Volumes are governed by Unity Catalog, which provides:<br>\n",
    "- Centralized metadata\n",
    "- Access tracking\n",
    "- Audit logs\n",
    "\n",
    "Why DBFS fails<br>\n",
    "- No centralized governance\n",
    "- No audit trail\n",
    "\n",
    "✔ Regulators demand proof — Volumes provide it\n",
    "\n",
    "2. Fine-Grained Access Control (Least Privilege)\n",
    "Requirement<br>\n",
    "- Users must access only what they need\n",
    "- No broad workspace access\n",
    "\n",
    "Volumes support\n",
    "GRANT READ ON VOLUME main.default.phi_data TO analytics_team;\n",
    "- Read-only access\n",
    "- No accidental writes\n",
    "- Controlled sharing\n",
    "\n",
    "DBFS limitation\n",
    "- Either full access or none\n",
    "- No role-based control\n",
    "\n",
    "✔ This is critical for compliance frameworks\n",
    "\n",
    "\n",
    "4. Auditing & Compliance Evidence<br>\n",
    "Auditors ask:<br>\n",
    "Who read this file?<br>\n",
    "When was it changed?<br>\n",
    "Was access authorized?<br>\n",
    "Volumes can answer these questions via:<br>\n",
    "Unity Catalog audit logs<br>\n",
    "Access history<br>\n",
    "\n",
    "DBFS cannot.<br>\n",
    "✔ Without audit logs, compliance fails<br>\n",
    "\n",
    "| Requirement      | Volumes        | DBFS            |\n",
    "| ---------------- | -------------- | --------------- |\n",
    "| Governance       | ✅ Yes          | ❌ No            |\n",
    "| Access control   | ✅ Fine-grained | ❌ None          |\n",
    "| Security         | ✅ Strong       | ❌ Weak          |\n",
    "| Auditing         | ✅ Available    | ❌ Not available |\n",
    "| Production ready | ✅ Yes          | ❌ No            |\n",
    "| Compliance ready | ✅ Yes          | ❌ No            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3114bf05-91b5-4756-89c0-01d41a88ee19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d14e7751-c1f7-4499-9aa1-2591bab9150e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create DataFrames from the Given Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec89aa48-d304-41d8-a77d-b9a09c75b73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df=spark.createDataFrame([(101,\"Arun\",31,\"Chennai\",\"PREPAID\"),\n",
    "    (102,\"Meera\",45,\"Bangalore\",\"POSTPAID\"),\n",
    "    (103,\"Irfan\",29,\"Hyderabad\",\"PREPAID\"),\n",
    "    (104,\"Raj\",52,\"Mumbai\",\"POSTPAID\"),\n",
    "    (105,None,27,\"Delhi\",\"PREPAID\"),\n",
    "    (106,\"Sneha\",None,\"Pune\",\"PREPAID\")],[\"customer_id\",\"name\",\"age\",\"city\",\"plan_type\"])\n",
    "\n",
    "usage_df = spark.createDataFrame([\n",
    "    (101,320,1500,20),\n",
    "    (102,120,4000,5),\n",
    "    (103,540,600,52),\n",
    "    (104,45,200,2),\n",
    "    (105,0,0,0)\n",
    "], [\"customer_id\",\"voice_mins\",\"data_mb\",\"sms_count\"])\n",
    "\n",
    "tower_r1_df = spark.createDataFrame([\n",
    "    (5001,101,\"TWR01\",-80,\"2025-01-10 10:21:54\"),\n",
    "    (5004,104,\"TWR05\",-75,\"2025-01-10 11:01:12\")\n",
    "], [\"event_id\",\"customer_id\",\"tower_id\",\"signal_strength\",\"timestamp\"])\n",
    "\n",
    "#Tower Logs – Region 2 (empty)\n",
    "tower_r2_df = spark.createDataFrame([], tower_r1_df.schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33c839bc-cc5c-4647-9b71-267b48881d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Copy (Write) the Data into the Volume Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91a4573-4243-45e9-975b-cd22a1823ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df.write.mode(\"overwrite\").csv(customer_path,header=True)\n",
    "usage_df.write.mode(\"overwrite\").csv(usage_path,header=True)\n",
    "tower_r1_df.write.mode(\"overwrite\").csv(tower_r1_path,header=True)\n",
    "tower_r2_df.write.mode(\"overwrite\").csv(tower_r2_path,header=True)\n",
    "display(customer_df)\n",
    "display(usage_df)\n",
    "display(tower_r1_df)\n",
    "display(tower_r2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a066b1-0832-42ce-82b5-b2305f2be6d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Validate That Files Were Successfully Copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3661a01-469b-4fd1-a528-abd9e6931785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943b3535-8701-4a17-9465-5ae894b328c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check each dataset\n",
    "dbutils.fs.ls(customer_path)\n",
    "dbutils.fs.ls(usage_path)\n",
    "dbutils.fs.ls(tower_r1_path)\n",
    "dbutils.fs.ls(tower_r2_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb03a707-9532-4d63-a677-7786732dc9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Read all tower logs using: \n",
    "- Path glob filter (example: *.csv)\n",
    "- Multiple paths input\n",
    "- Recursive lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972a8ad7-b9a8-44c3-9247-4e5b9cb9337a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "##Directory Read Use Cases Using Path Glob Filter (*.csv)\n",
    "df1=spark.read.option(\"header\",\"True\").option(\"sep\",\",\").csv(f\"{base_path}/tower/*/*.csv\")\n",
    "df1.count()\n",
    "df1.display(3)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19e5b7a-5f4b-4a95-b162-bd2e0c7c8cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Multiple paths input\n",
    "df1=spark.read.options(header=\"True\",sep=\",\",inferSchema=True).csv(path=[\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\",\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"])\n",
    "df1.count()\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcfd518-7bbb-4102-a85f-90d2bfd66e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "##Recursive lookup\n",
    "df1 = spark.read.csv(\n",
    "    path = [f\"{base_path}/tower/*\"],\n",
    "    inferSchema = True,\n",
    "    header = True,\n",
    "    sep = \",\",\n",
    "    pathGlobFilter = \"*.csv\",\n",
    "    recursiveFileLookup = True\n",
    ")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b282fedd-870e-4129-9906-dc55e7d3bd51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## pathGlobFilter<br>\n",
    "Reads only files matching a pattern (e.g., *.csv) from subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672e91c9-120c-4ca2-a802-8670c643330e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.option(\"header\",\"True\").option(\"sep\",\",\").option(\"pathGlobalFilter\",\".csv\").option(\"inferSchema\",\"True\").csv(f\"{base_path}/tower/*\")\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efab0e8c-8c06-4561-bc89-8501e0c8b909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Using List of Paths in spark.read.csv([path1, path2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd49b9f2-cb4f-4c13-8f5c-780a6532411d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_of_path=f\"{base_path}/tower/*\"\n",
    "df1=spark.read.option(\"header\",\"True\").option(\"sep\",\",\").option(\"pathGlobalFilter\",\".csv\").csv(list_of_path)\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4b02af-5c5f-4be2-8eba-b8be12d06b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"]\n",
    "df3=spark.read.options(header='True', inferSchema='True',sep=',',pathGlobalFilter='.csv',recursiveFileLookup='True').csv(path)\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fe3e021-1fcb-427f-abe2-7a407ecf7d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Using recursiveFileLookup<br>\n",
    "What it does<br>\n",
    "Recursively reads all files in all subdirectories.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3361e79-e6d0-4586-acdc-79592bdb9497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_recursive = spark.read.option(\"header\", \"true\").option(\"sep\", \",\").option(\"pathGlobFilter\", \"*.csv\").option(\"recursiveFileLookup\", \"true\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "df_recursive.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d7f669d-0f4f-4040-b573-5a281d449db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c835c1d-268d-498f-b6c5-e1a3d9053b0c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765650924506}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.format(\".csv\").options(header=\"False\",inferSchema=\"False\").csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\")\n",
    "print(df1.printSchema())\n",
    "df1.display(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cfffa5d-e513-4e85-ab2a-6f59e8daf2c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**From Above results we see**<br>\n",
    "- Column names are auto-generated (_c0, _c1, …)<br>\n",
    "- All columns are STRING<br>\n",
    "- Header row is treated as data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e561295-370c-4e48-9eee-2c772db1f1f5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765651544519}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.format(\".csv\").options(header=\"True\",inferSchema=\"True\").csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\")\n",
    "print(df1.printSchema())\n",
    "df1.display(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd6e0de-fe0c-4022-89e7-b2f1b876cb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**From above results we see**<br>\n",
    "- First row used as column names<br>\n",
    "- Spark tries to detect data types<br>\n",
    "- age becomes STRING (explained below)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1981533c-1712-4a61-bbe2-79e86d98c55e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##What Changed When Using header and inferSchema?<br>\n",
    "**Header**\n",
    "| Value | Effect                         |\n",
    "| ----- | ------------------------------ |\n",
    "| false | First row treated as data      |\n",
    "| true  | First row used as column names |<br>\n",
    "\n",
    "**inferSchema Option**\n",
    "| Value | Effect                               |\n",
    "| ----- | ------------------------------------ |\n",
    "| false | All columns read as STRING           |\n",
    "| true  | Spark samples data and assigns types |<br>\n",
    "\n",
    "What Spark Does\n",
    "- Spark samples multiple rows\n",
    "- Sees numeric values: 31, 45, 29\n",
    "- Sees non-numeric value: \"abc\"\n",
    "- Cannot safely cast entire column to INTEGER\n",
    "- Spark always chooses the safest common type\n",
    "- It will not partially fail or cast invalid rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02daf487-3c08-45c1-9bb5-64923d567abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How schema inference handled “abc” in age?<br>**\n",
    "- How Schema Inference Works (Step by Step)\n",
    "- Spark samples the data when inferSchema=true.\n",
    "- It tries to determine a single data type that can hold all values.\n",
    "- Spark will not partially fail or drop rows during inference.\n",
    "- Since \"abc\" cannot be cast to an integer, Spark cannot safely choose INT.\n",
    "- Spark falls back to STRING for the entire column."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8520640417861855,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Read_Write_UseCase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
