{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00631ab-71b1-4ad5-a654-751b3746fffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa843b9-d18d-4b6a-b92c-ab6eb2afe667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](./logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1e3853-7e94-4e81-a743-bb302ff55ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10677f57-f424-40d9-90a3-bcf94ec9c1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)\n",
    "\n",
    "**Source 1: Logistics Shipment Data (JSON Format)**\n",
    "- Data is received from the source system in JSON[Semi strcutured format]\n",
    "- key–value pairs\n",
    "\n",
    "**Source 2: Logistics Data (CSV Format – 4 Columns)**\n",
    "- Data is received in CSV format with 4 columns\n",
    "- Header present, no footer\n",
    "- Null columns and null records are there\n",
    "- Data format inconsistencies observed like age contain string value\n",
    "- Includes additional column(s)\n",
    "\n",
    "**Source 3: Logistics Data (CSV Format – 7 Columns)**\n",
    "- Data is received in CSV format with 7 columns\n",
    "- Header present, no footer\n",
    "- Contains duplicate records\n",
    "- Null columns and null records are there\n",
    "- Data format inconsistencies observed like age contain string value\n",
    "- Includes additional column(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381c25cd-503d-4662-8935-611381f53535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440d7677-d3dc-4053-99ee-ae0de28ebc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ad2f93-eff6-4706-b3fd-0552962f5f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source1_df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source1\",header=True,inferSchema=True).toDF(\"Shipment_id\",\"First_Name\",\"Last_Name\",\"Age\",\"Role\")\n",
    "\n",
    "print(source1_df.printSchema())\n",
    "\n",
    "display(source1_df.show(10,False))\n",
    "display(source1_df.columns)\n",
    "display(source1_df.dtypes) #Age is in string format and shippment ID is in string type\n",
    "dedup_df = source1_df.distinct()\n",
    "removed_rows = source1_df.exceptAll(dedup_df)\n",
    "removed_rows.show(truncate=False)\n",
    "\n",
    "print(\"Original count:\", source1_df.count())\n",
    "print(\"After distinct:\", dedup_df.count())\n",
    "print(\"Duplicates removed:\", source1_df.count() - dedup_df.count())\n",
    "print(\"de-duplicated given id column count:\",source1_df.dropDuplicates(['Shipment_id']).count())\n",
    "\n",
    "\n",
    "source2_df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source2\",header=True)\n",
    "display(source2_df.show(10,False))\n",
    "display(source2_df.columns)\n",
    "display(source2_df.dtypes) #Age is in string format and shippment ID is in string type\n",
    "dedup_df1 = source2_df.distinct()\n",
    "removed_rows1 = source2_df.exceptAll(dedup_df1)\n",
    "removed_rows1.show(truncate=False)\n",
    "\n",
    "print(\"Original count:\", source2_df.count())\n",
    "print(\"After distinct:\", dedup_df1.count())\n",
    "print(\"Duplicates removed:\", source2_df.count() - dedup_df1.count())\n",
    "print(\"de-duplicated given id column count:\",source2_df.dropDuplicates(['Shipment_id']).count())\n",
    "\n",
    "display(source1_df.summary())\n",
    "display(source2_df.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e108dfe1-7eb5-4e65-af8b-55202f227770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having:<br>\n",
    "3. fewer columns than expected<br>\n",
    "4. more columns than expected<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb0a0e8-2072-4fce-86ee-2fde74755a74",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768070510245}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768112750026}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "find_df1 = source1_df.where(\"Shipment_id rlike '[A-Za-z]'\")\n",
    "print(\"shipment_id is non-numeric from source1_df:\")\n",
    "display(find_df1)\n",
    "source1_df.schema\n",
    "\n",
    "find_df1 = source2_df.where(\"Shipment_id rlike '[A-Za-z]'\")\n",
    "print(\"shipment_id is non-numeric from source2_df:\")\n",
    "display(find_df1)\n",
    "source2_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c34378-8d5e-45a1-ac94-31e77d7d8945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Count rows having:<br>\n",
    "3. fewer columns than expected<br>\n",
    "4. more columns than expected<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e4b3a5-3a13-4bf4-a493-b32840400b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size,col,split,when\n",
    "expected_cols = 5   # change as per your file\n",
    "delimiter = \",\"\n",
    "print(\"Source df1 Count rows:\")\n",
    "raw_df1=spark.read.text(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source1\")\n",
    "df_with_col_count=raw_df1.withColumn('actual_col_count',size(split(col('value'),delimiter)))\n",
    "\n",
    "df_flagged=df_with_col_count.withColumn(\"column_status\",when(col(\"actual_col_count\")<expected_cols,\"FEWER_COLUMNS\").when(col(\"actual_col_count\")>expected_cols,\"MORE_COLUMNS\").otherwise(\"EXPECTED_COLUMNS\"))\n",
    "display(df_flagged)\n",
    "\n",
    "df_bad_record = df_with_col_count.where(col(\"actual_col_count\") != expected_cols).groupBy(\"actual_col_count\").count()\n",
    "display(df_bad_record)\n",
    "\n",
    "print(\"Source df2 Count rows:\")\n",
    "raw_df2=spark.read.text(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source2\")\n",
    "df_with_col_count1=raw_df2.withColumn(\"actual_column_count\",size(split(col(\"value\"),delimiter)))\n",
    "df_flagged1=df_with_col_count1.withColumn(\"column_status\",when(col(\"actual_column_count\")<7,\"FEWER_COLUMNS\").when(col(\"actual_column_count\")>7,\"MORE_COLUMNS\").otherwise(\"EXPECTED_COLUMNS\"))\n",
    "display(df_flagged1)\n",
    "df_bad_record1=df_flagged1.where(col(\"actual_column_count\")!=7).groupBy(\"actual_column_count\").count()\n",
    "display(df_bad_record1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a1284eb-b0ea-40d9-b377-dd4c85347d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Spark Session Object\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Logistic_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68f94fa-4095-4108-9d56-496b68660a48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343f47ce-8521-4ab6-8cb2-1b1a0f6339d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema<br>\n",
    "2. Align them into a single canonical schema: \n",
    "- shipment_id,<br>\n",
    "- first_name,<br>\n",
    "- last_name,<br>\n",
    "- age,<br>\n",
    "- role,<br>\n",
    "- hub_location,<br>\n",
    "- vehicle_type,<br>\n",
    "- data_source<br>\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f03b86c-7f9c-441d-8b5f-991c35a5758f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Source 1 (System A): id, fname, lname, age<br>\n",
    "Source 2 (System B): shipment_id, full_name, years<br>\n",
    "Canonical schema (decided by you):- shipment_id, first_name, last_name, age<br>\n",
    "\n",
    "All source data is reshaped into this structure before further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da91b102-4109-450f-8b5a-6dcd7d2ba9c2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768116708883}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "source1_raw = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"mode\", \"permissive\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source1\")\n",
    "\n",
    "source2_raw = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"mode\", \"permissive\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source2\")\n",
    "\n",
    "source1_canonical = source1_raw.select(\n",
    "    col(\"shipment_id\").cast(\"string\").alias(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\").cast(\"string\"),\n",
    "    col(\"role\"),\n",
    "    lit(None).cast(\"string\").alias(\"hub_location\"),\n",
    "    lit(None).cast(\"string\").alias(\"vehicle_type\"),\n",
    "    lit(\"system1\").alias(\"data_source\")\n",
    ")\n",
    "source2_canonical = source2_raw.select(\n",
    "    col(\"shipment_id\").cast(\"string\").alias(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\").cast(\"string\"),\n",
    "    col(\"role\"),\n",
    "    col(\"hub_location\"),\n",
    "    col(\"vehicle_type\"),\n",
    "    lit(\"system2\").alias(\"data_source\")\n",
    ")\n",
    "canonical_df = source1_canonical.unionByName(source2_canonical)\n",
    "display(canonical_df)\n",
    "print(canonical_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5030ef-d3b7-4323-8017-0124de47553c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing:\n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8befc2f-c1eb-4aad-9ba4-14012617a99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "print(\"Before dropping duplicates:\", canonical_df.count())\n",
    "canonical_df1 = canonical_df.na.drop(how='any',subset=[\"shipment_id\",\"role\"])\n",
    "print(\"After dropping duplicates:\", canonical_df1.count())\n",
    "\n",
    "\n",
    "#Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "print(\"Before dropping duplicates:\", canonical_df1.count())\n",
    "canonical_df2 = canonical_df1.na.drop(how='all',subset=[\"first_name\",\"last_name\"])\n",
    "print(\"After dropping duplicates:\", canonical_df2.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7daf5756-d515-4ee8-b0c9-4ca3f9445fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Join Readiness Rule\n",
    "A record must have a valid join key (shipment_id) to participate in downstream joins.\n",
    "If the join key is NULL, the record is not usable and must be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b00af5-054e-4067-b94d-924d8675d600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Join Readiness Rule - Drop records where the join key is null: shipment_id\n",
    "from pyspark.sql.functions import col\n",
    "print(\"Before Join Readiness check:\", canonical_df2.count())\n",
    "canonical_df_join_ready = canonical_df2.filter(col(\"shipment_id\").isNotNull())\n",
    "print(\"After Join Readiness check:\", canonical_df_join_ready.count())\n",
    "\n",
    "#OR\n",
    "\n",
    "#canonical_df.na.drop(subset=[\"shipment_id\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bfca716-8686-46a2-91fb-6f3e713a4eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####3.Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1<br>\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV<br>\n",
    "bike to TwoWheeler<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a878cf-4fa4-496e-a85c-631689756c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "cleaned_df=canonical_df_join_ready.na.fill(\"-1\",subset=[\"age\"])\n",
    "\n",
    "#Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "cleaned_df1=cleaned_df.na.fill(\"UNKNOWN\",subset=[\"vehicle_type\"])\n",
    "\n",
    "#Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 and \"\" to -1\n",
    "replacedata={'ten':'-1','':'-1'}\n",
    "cleaned_df2=cleaned_df1.na.replace(replacedata,subset=[\"age\"])\n",
    "cleaned_df1.printSchema()\n",
    "\n",
    "#Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV and bike to TwoWheeler\n",
    "replacedata1={'Truck':'LMV','Bike':'TwoWheeler'}\n",
    "cleaned_df3=cleaned_df2.na.replace(replacedata1,subset=[\"vehicle_type\"])\n",
    "display(cleaned_df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560177e5-635c-4c5d-8866-aaef7dab2964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342e4962-c24f-4f31-916b-9bf07e49d895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating shipments Details data Dataframe creation <br>\n",
    "1. Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23760b2-6e37-4881-a5ae-1414853c1a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_clean_df=spark.read.option(\"multiLine\", True).json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_shipment_detail_3000.json\")\n",
    "display(json_clean_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f56140-82e6-468e-9528-cca271681367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "hub_location - Convert values to initcap case<br>\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2) <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4dbb5f-8a73-4514-9ec8-525195dd9534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower,upper,initcap,current_timestamp\n",
    "#1. Add a column<br> \n",
    "#Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp #'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "json_clean_df1=json_clean_df.withColumn(\"domain\",lit(\"Logistic\")).withColumn(\"ingestion_timestamp\",current_timestamp()).withColumn(\"is_expedited\",lit('False'))\n",
    "display(json_clean_df1.limit(5))\n",
    "\n",
    "\n",
    "#Column Uniformity: role - Convert to lowercase\n",
    "cleaned_df4=cleaned_df3.withColumn(\"role\",lower(col(\"role\")))\n",
    "\n",
    "\n",
    "#vehicle_type - Convert values to UPPERCASE\n",
    "cleaned_df5=cleaned_df4.withColumn(\"vehicle_type\",upper(col(\"vehicle_type\")))\n",
    "\n",
    "\n",
    "#Source Files: logistics_shipment_detail_3000.json (and the merged master files) hub_location - Convert values to initcap case\n",
    "cleaned_df6=cleaned_df5.withColumn(\"hub_location\",initcap(col(\"hub_location\")))\n",
    "display(cleaned_df5.limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c897f0a-adfe-4bb1-9190-5cb9d8f6d3af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0294c7d3-21df-4440-8512-31cabb637c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,round\n",
    "\n",
    "#Convert shipment_date to yyyy-MM-dd\n",
    "json_clean_df2=json_clean_df1.withColumn(\"shipment_date\",to_date(col(\"shipment_date\"),'yy-MM-dd'))\n",
    "\n",
    "\n",
    "#Ensure delivery_cost has 2 decimal precision\n",
    "json_clean_df3=json_clean_df2.withColumn(\"shipment_cost\",round(col(\"shipment_cost\"),2))\n",
    "display(json_clean_df3.limit(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f9969e-01cd-400e-b3bc-e68146ecad2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2) <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e9c3ba-9042-4cd6-99ba-5a2cbc743fd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Source File: logistics_source1 & logistics_source2\n",
    "#age: Cast String to Integer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cleaned_df6.printSchema()\n",
    "cleaned_df7 = cleaned_df6.withColumn(\"age\",col(\"age\").cast(\"int\"))\n",
    "cleaned_df7.printSchema()\n",
    "\n",
    "#shipment_weight_kg: Cast to Double\n",
    "json_clean_df3.printSchema()\n",
    "\n",
    "json_clean_df4=json_clean_df3.withColumn(\"is_expedited\",col(\"is_expedited\").cast(\"Boolean\"))\n",
    "json_clean_df4.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7690ac58-2cae-4727-91d1-6601aca9a86a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "336565ea-1395-4cd7-b7b0-6e84931153ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Naming Standardization\n",
    "#Source File: logistics_source1 & logistics_source2\n",
    "#Rename: first_name to staff_first_name\n",
    "#Rename: last_name to staff_last_name\n",
    "cleaned_df8=cleaned_df7.withColumnsRenamed({\"first_name\":\"staff_first_name\",\"last_name\":\"staff_last_name\"})\n",
    "\n",
    "#Rename: hub_location to origin_hub_city\n",
    "cleaned_df9=cleaned_df8.withColumnRenamed(\"hub_location\",\"origin_hub_city\")\n",
    "cleaned_df9.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0306338f-0085-4d28-9b32-a8ca1092ae4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f2be00-4f0c-404a-b1cc-21bd9c66342c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768477696688}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df10 = cleaned_df9.where(\"NOT Shipment_id rlike '[A-Za-z]'\")\n",
    "complte_df=cleaned_df10.unionByName(json_clean_df4,allowMissingColumns=True)\n",
    "\n",
    "cleaned_df11= complte_df.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\",\"shipment_cost\",\"ingestion_timestamp\")\n",
    "display(cleaned_df11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d848ae47-c118-4749-8961-09683000f770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404c6a94-3a4e-4ce5-bf84-bcac08828c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df12=cleaned_df11.distinct()\n",
    "cleaned_df13=cleaned_df12.dropDuplicates(subset=['\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\",\"shipment_cost\",\"ingestion_timestamp\"'])\n",
    "display(cleaned_df12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8c7afa9-0fbe-4bcd-ba71-b13c33f340a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967909b6-4525-401d-901b-72cc46672986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add Audit Timestamp (load_dt) Source File: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "Scenario: We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "Action: Add a column load_dt using the function current_timestamp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc4476c4-0fd2-472f-983a-8c1eb0eaaf88",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768479492977}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df11=cleaned_df10.withColumn(\"load_dt\",current_timestamp())\n",
    "display(cleaned_df11.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1b1b8b-98d7-431d-a72b-43c4336447f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Create Full Name (full_name) Source File: DF of logistics_source1 and logistics_source2\n",
    "\n",
    "Scenario: The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "Action: Create full_name by concatenating first_name and last_name with a space separator.\n",
    "Result: \"Rajesh\" + \" \" + \"Kumar\" -> \"Rajesh Kumar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584c843d-44ee-44a9-af07-7ec4fdaa8f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col\n",
    "\n",
    "cleaned_df12=cleaned_df11.withColumn(\"Full_Name\", concat(col(\"staff_first_name\"), lit(' '), col(\"staff_last_name\")))\n",
    "display(cleaned_df12.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6adadad8-685f-416c-9b18-5317a42f4e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Define Route Segment (route_segment) Source File: DF of logistics_shipment_detail_3000.json\n",
    "\n",
    "Scenario: The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "Action: Combine source_city and destination_city with a hyphen.\n",
    "Result: \"Chennai\" + \"-\" + \"Pune\" -> \"Chennai-Pune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3561370-be6e-4271-9591-dae29aef6c64",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768482349042}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_clean_df5=json_clean_df4.withColumn(\"route_segment\",concat(col(\"source_city\"),lit('-'),col(\"destination_city\")))\n",
    "display(json_clean_df5.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b071182f-a1f3-46b9-8f89-613ada85f5a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Generate Vehicle Identifier (vehicle_identifier) Source File: DF of logistics_shipment_detail_3000.json\n",
    "\n",
    "Scenario: We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "Action: Combine vehicle_type and shipment_id to create a composite key.\n",
    "Result: \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee71bcf-3282-498c-ba0d-56878c83b545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_clean_df6=json_clean_df5.withColumn(\"vehicle_identifier\",concat(col(\"vehicle_type\"),lit('_'),col(\"shipment_id\")))\n",
    "display(json_clean_df6.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5fb3629-05a7-462f-b1fd-b10455ef8b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Deriving of Columns (Time Intelligence)\n",
    "*Extracting temporal features from dates to enable period-based analysis and reporting.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0eebc38-fae5-484b-83b3-fe3d3fd7aa2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Derive Shipment Year (`shipment_year`)**\n",
    "* **Scenario:** Management needs an annual performance report to compare growth year-over-year.\n",
    "* **Action:** Extract the year component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a74e9c9-a18c-4ceb-a409-3277bed9eb61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "json_clean_df7=json_clean_df6.withColumn(\"shipment_year\",year(col(\"shipment_date\")))\n",
    "display(json_clean_df7.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3f36c6-2d74-4482-824e-ae3e28c4d5e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Derive Shipment Month (`shipment_month`)**\n",
    "* **Scenario:** Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "* **Action:** Extract the month component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **4** (April)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c05642-b5e5-4d10-a916-e668010b8be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "json_clean_df8=json_clean_df7.withColumn(\"shipment_month\",month(col(\"shipment_date\")))\n",
    "display(json_clean_df8.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea809587-fe4e-456c-a2a2-69aee595cdc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Flag Weekend Operations (`is_weekend`)**\n",
    "* **Scenario:** The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "* **Action:** Flag as **'True'** if the `shipment_date` falls on a Saturday or Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b928fb-17aa-434a-8974-3a1673075a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "json_clean_df9=json_clean_df8.withColumn(\"is_weekend\",when(dayofweek(col(\"shipment_date\")).isin(1,7),True).otherwise(False))\n",
    "display(json_clean_df9.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0981865e-1eab-45c9-ba89-c8cf2b3b577d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. Flag shipment status (`is_expedited`)**\n",
    "* **Scenario:** The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "* **Action:** Flag as **'True'** if the `shipment_status` IN_TRANSIT or DELIVERED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b0a8ae2-04ad-4ad8-a807-7531536bee1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "json_clean_df10=json_clean_df9.withColumn(\"is_expedited\",when(col(\"shipment_status\")==\"IN_TRANSIT\",True).when(col(\"shipment_status\")==\"DELIVERED\",True).otherwise(False))\n",
    "display(json_clean_df10.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42a71e3-576c-4ddb-ab47-ac7f6098278b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Enrichment/Business Logics (Calculated Fields)\n",
    "*Deriving new metrics and financial indicators using mathematical and date-based operations.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448603f1-60ee-42a2-b0ef-ce8cef4ab582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Calculate Unit Cost (`cost_per_kg`)**\n",
    "* **Scenario:** The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "* **Action:** Divide `shipment_cost` by `shipment_weight_kg`.\n",
    "* **Logic:** `shipment_cost / shipment_weight_kg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c23a8a3-a407-4a1f-8cf2-ad3eb82a7bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import try_divide,round\n",
    "json_clean_df11=json_clean_df10.withColumn(\"cost_per_kg\",round(try_divide(col(\"shipment_cost\"),col(\"shipment_weight_kg\")),2))\n",
    "display(json_clean_df11.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e56d78e5-024a-406a-85e4-29977bfd06ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Track Shipment Age (`days_since_shipment`)**\n",
    "* **Scenario:** The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "* **Action:** Calculate the difference in days between the `current_date` and the `shipment_date`.\n",
    "* **Logic:** `datediff(current_date(), shipment_date)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4fce00a-b919-46ba-b52e-f6d0fb5e91d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_diff,current_date\n",
    "json_clean_df12=json_clean_df11.withColumn(\"days_since_shipment\",date_diff(current_date(), \"shipment_date\"))\n",
    "display(json_clean_df12.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381bfe15-639d-48fd-b3a6-51b1bad27080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Compute Tax Liability (`tax_amount`)**\n",
    "* **Scenario:** For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "* **Action:** Calculate 18% GST on the total `shipment_cost`.\n",
    "* **Logic:** `shipment_cost * 0.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd0970c-e4b1-4b14-9342-bcc8ecbd047e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "json_clean_df13=json_clean_df12.withColumn(\"tax_amount\",round(col(\"shipment_cost\")*0.18,2))\n",
    "display(json_clean_df13.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5582315a-bb35-4360-ac92-7a4488f0616b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Remove/Eliminate (drop, select, selectExpr)\n",
    "*Excluding unnecessary or redundant columns to optimize storage and privacy.*<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "\n",
    "**1. Remove Redundant Name Columns**\n",
    "* **Scenario:** Since we have already created the `full_name` column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "* **Action:** Drop the `first_name` and `last_name` columns.\n",
    "* **Logic:** `df.drop(\"first_name\", \"last_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9470f76-9b0f-4db6-afd3-c25ddce55e03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned_df12.limit(5))\n",
    "cleaned_df13=cleaned_df12.drop(\"staff_first_name\", \"staff_last_name\")\n",
    "display(cleaned_df13.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701195da-bd71-4194-bb07-3ba81e479298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Splitting & Merging/Melting of Columns\n",
    "Reshaping columns to extract hidden values or combine fields for better analysis.\n",
    "Source File: DF of logistics_shipment_detail_3000.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174dd01b-4288-4c5a-9e80-982c931a67db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Splitting (Extraction)**\n",
    "*Breaking one column into multiple to isolate key information.*\n",
    "* **Split Order Code:**\n",
    "  * **Action:** Split `order_id` (\"ORD100000\") into two new columns:\n",
    "    * `order_prefix` (\"ORD\")\n",
    "    * `order_sequence` (\"100000\")\n",
    "* **Split Date:**\n",
    "  * **Action:** Split `shipment_date` into three separate columns for partitioning:\n",
    "    * `ship_year` (2024)\n",
    "    * `ship_month` (4)\n",
    "    * `ship_day` (23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a47124a-acb1-48c3-bd87-4691ea54ecac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring,day\n",
    "json_clean_df14=json_clean_df13.withColumn(\"order_prefix\",substring(col(\"order_id\"),1,3)).withColumn(\"order_sequence\",substring(col(\"order_id\"),4,6))\n",
    "display(json_clean_df14.limit(5))\n",
    "\n",
    "json_clean_df15=json_clean_df14.withColumn(\"ship_year\",year(col(\"shipment_date\"))).withColumn(\"ship_month\",month(col(\"shipment_date\"))).withColumn(\"ship_day\",day(col(\"shipment_date\")))\n",
    "display(json_clean_df15.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a1e475-02f8-4364-9a70-291c0627a2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f9348a0-6fce-4fae-abe7-70345dc6b275",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768486684378}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_clean_df16=json_clean_df15.withColumn(\"route_lane\",concat(col(\"source_city\"),lit('->'),col(\"destination_city\")))\n",
    "display(json_clean_df16.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f03697d-5d17-49b6-93dc-539c6d8aa301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1f85a6-b549-4cf6-a1cd-1f1a2f14d704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6b0a70-3fb1-49cb-934d-b29d78243bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bonus(role, age):\n",
    "    if role == 'driver' and age > 50:\n",
    "        bonus = \"15% of Salary (Reward for Seniority)\"\n",
    "    elif role == 'driver' and age < 30:\n",
    "        bonus = \"5% of Salary (Encouragement for Juniors)\"\n",
    "    else:\n",
    "        bonus = 0\n",
    "\n",
    "    return bonus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7036190-46a6-48d8-bd75-bc4ad268d215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99139e2a-a132-4872-8199-be9f8ce66f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "cleaned_df14=cleaned_df13.withColumn(\"projected_bonus\",udf(calculate_bonus)(col(\"role\"),col(\"age\")))\n",
    "display(cleaned_df14.limit(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ddb145-8d64-4766-a349-36e0d28c2980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd51767a-956d-4df6-874a-4d8c36dd8954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mask_identity(name):\n",
    "    if name is None:\n",
    "        return None\n",
    "\n",
    "    name = name.strip()\n",
    "\n",
    "    # Handle very short names safely\n",
    "    if len(name) <= 3:\n",
    "        return name[0] + \"****\" + name[-1]\n",
    "\n",
    "    return name[:2] + \"****\" + name[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4306c57-087b-43ae-b95c-57de613b0c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df15=cleaned_df12.withColumn(\"mask_identity\",udf(mask_identity)(\"staff_first_name\"))\n",
    "display(cleaned_df15.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3837cb54-ee82-444d-9b69-bd977792efe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbedcc77-b706-4cdb-963f-83c013d8fa61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim,length\n",
    "cleaned_df15=cleaned_df12.withColumn(\"mask_identity\",concat(substring(trim(col(\"staff_first_name\")),1,2)\n",
    "                                                            ,lit('****'),substring(col(\"staff_first_name\"),length(trim(col(\"staff_first_name\"))),1)))\n",
    "display(cleaned_df15.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de9ff6c8-3210-43b5-beab-e87feb0e9d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbbf8fed-afe8-4e4f-8333-ec09eb1ba3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Select (Projection)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdad4b9-260b-4880-8e62-21f122ff95a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df16=cleaned_df15.select(\"staff_first_name\",\"role\",\"origin_hub_city\")\n",
    "#display(cleaned_df16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b68e51-3921-4822-94c6-6d1e61c038c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. Filter (Selection)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463a8fbe-a6ac-49eb-92b8-a6d4776f308e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768489958697}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_clean_df17=json_clean_df16.filter(\"shipment_status IN ('DELAYED', 'RETURNED')\")\n",
    "display(json_clean_df17.limit(5))\n",
    "\n",
    "cleaned_df18=cleaned_df15.where(\"age>50\")\n",
    "display(cleaned_df18.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2c8e91-8721-4136-a72f-7e7c4130b4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 40,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1faefbd-1429-4691-adfd-250d360adb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_clean_df17=json_clean_df16.withColumn(\"is_high_value\",when(col(\"shipment_cost\")>40000,True).otherwise(False))\n",
    "display(json_clean_df17)\n",
    "\n",
    "#Scenario: Flag weekend operations for overtime calculation.\n",
    "#Action: Create flag is_weekend = True if day is Saturday or Sunday.\n",
    "\n",
    "#This flag is already added in this DF previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ba9ce8-e749-4c83-bd9c-188beed07774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. Format (Standardization)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"₹30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\" → **\"CHENNAI\"**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23cfb936-e81c-4e03-b5ae-6c5da858b3c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, concat, lit, format_number,upper\n",
    "\n",
    "json_clean_df18 = json_clean_df17.withColumn(\"shipment_cost\",concat(lit(\"₹\"),format_number(col(\"shipment_cost\"),2)))\n",
    "\n",
    "\n",
    "json_clean_df19=json_clean_df18.withColumn(\"source_city\",upper(\"source_city\"))\n",
    "display(json_clean_df19.limit(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efed566-8ffd-4f70-8cbb-1daf4cbe1e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8401a67a-a6ee-433a-8642-e9b38e58a0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df16=cleaned_df15.groupBy(\"origin_hub_city\").count()\n",
    "display(cleaned_df16)\n",
    "\n",
    "#Scenario: Fleet capacity analysis.\n",
    "#Action: Group by vehicle_type and Sum the shipment_weight_kg.\n",
    "#Need clarification here because source files are wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c82691-42f3-4e8c-8391-95eef52468e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6. Sorting (Ordering)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending) then `priority_flag` (Descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e36f53-b937-43ee-a97b-13df60c63af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "json_clean_df20=json_clean_df19.orderBy(desc(\"shipment_cost\"))\n",
    "json_clean_df20=json_clean_df19.orderBy(\"shipment_cost\",ascending=True) #priority_flag (Descending) no such flag we added in json so wrong action\n",
    "display(json_clean_df20.limit(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b1b2a7-2710-46df-9230-badc74dcd3a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**7. Limit (Top-N Analysis)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4533249c-a535-4421-9511-9477b6cb4235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_clean_df21=json_clean_df20.filter(col(\"shipment_status\")=='DELAYED').orderBy(\"shipment_cost\").limit(10)\n",
    "display(json_clean_df21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef9ba8c-4951-445c-97ea-f6cd357b9108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59214d4-375f-4b50-904a-a775e916c202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> DF of logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> DF of logistics_shipment_detail_3000.json<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c881fc-f10e-4003-94d0-49aeec157797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068af84c-d7dd-4dea-b3e2-acd0ef461434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "innerdf1=cleaned_df15.join(json_clean_df20,how='inner',on=\"shipment_id\")\n",
    "display(innerdf1.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "663c285f-0fc7-4554-85bd-38eaa180e6da",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768497721484}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leftdf1=cleaned_df15.alias(\"staff_df\").join(json_clean_df20.alias(\"shipments_df\"),how='left',on=\"shipment_id\")\n",
    "resultdf=leftdf1.select(cleaned_df15[\"*\"]).where(\"shipments_df.shipment_id is NULL\")\n",
    "display(resultdf.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff936eb2-956d-4c2f-b937-6b6335f0fd32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fixed join: staffdf and shipment_df"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af484cb0-30ae-45a0-83c4-0bdbb00fde2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac75cdc3-5a5b-4deb-9129-98ba57be19e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1efa7411-2d5a-49d4-a78e-7c6013c6a37e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selfdf1=cleaned_df15.alias(\"staff_df1\").join(cleaned_df15.alias(\"staff_df2\"),how='inner',on=col(\"staff_df1.origin_hub_city\")==col(\"staff_df2.origin_hub_city\"))\n",
    "resultdf = (\n",
    "    selfdf1\n",
    "        .select(\n",
    "            col(\"staff_df1.origin_hub_city\").alias(\"hub_location\"),\n",
    "\n",
    "            col(\"staff_df1.shipment_id\").alias(\"emp1_shipment_id\"),\n",
    "            col(\"staff_df1.staff_first_name\").alias(\"emp1_first_name\"),\n",
    "            col(\"staff_df1.staff_last_name\").alias(\"emp1_last_name\"),\n",
    "            col(\"staff_df1.age\").alias(\"emp1_age\"),\n",
    "            col(\"staff_df1.role\").alias(\"emp1_role\"),\n",
    "            col(\"staff_df1.vehicle_type\").alias(\"emp1_vehicle_type\"),\n",
    "\n",
    "            col(\"staff_df2.shipment_id\").alias(\"emp2_shipment_id\"),\n",
    "            col(\"staff_df2.staff_first_name\").alias(\"emp2_first_name\"),\n",
    "            col(\"staff_df2.staff_last_name\").alias(\"emp2_last_name\"),\n",
    "            col(\"staff_df2.age\").alias(\"emp2_age\"),\n",
    "            col(\"staff_df2.role\").alias(\"emp2_role\"),\n",
    "            col(\"staff_df2.vehicle_type\").alias(\"emp2_vehicle_type\")\n",
    "        )\n",
    "        .where(col(\"staff_df1.shipment_id\")!= col(\"staff_df2.shipment_id\"))\n",
    ")\n",
    "\n",
    "display(resultdf.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370d36ca-ebd6-4ab7-a2c9-ead75e32ff04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2463a021-9a99-4b10-b18a-1820d516a470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RightJoinDf=cleaned_df15.alias(\"staff_df\").join(json_clean_df20.alias(\"shipments_df\"),how='right',on=col(\"staff_df.shipment_id\")==col(\"shipments_df.shipment_id\"))\n",
    "resultdf=RightJoinDf.where(col(\"staff_df.shipment_id\").isNull())\n",
    "display(resultdf.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e21d702-f74a-4d05-9d9c-737c6fc8932a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d216bf-b50b-44d2-a1f7-d875a222d857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fullOuterJoin=cleaned_df15.alias(\"staff_df\").join(json_clean_df20.alias(\"shipments_df\"),how='full',on=col(\"staff_df.shipment_id\")==col(\"shipments_df.shipment_id\"))\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "reconciliation_df = fullOuterJoin.withColumn(\n",
    "    \"audit_status\",\n",
    "    when(\n",
    "        col(\"staff_df.shipment_id\").isNull(),\n",
    "        \"UNASSIGNED_SHIPMENT\"\n",
    "    ).when(\n",
    "        col(\"shipments_df.shipment_id\").isNull(),\n",
    "        \"IDLE_DRIVER\"\n",
    "    ).otherwise(\n",
    "        \"VALID_ASSIGNMENT\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(reconciliation_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb979071-44b0-491c-85f6-1c00f0c37f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "976b55c8-d669-4154-91ce-9d74673c7654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_clean_df21=json_clean_df20.filter(col(\"shipment_status\").isin(\"CREATED\",\"DELAYED\"))\n",
    "crossJoinDf=cleaned_df15.alias(\"staff_df\").join(json_clean_df21.alias(\"shipments_df\"),how=\"cross\")\n",
    "display(crossJoinDf.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93d79ab-4af6-4234-90dc-cc40e79917ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1.3 Advanced Joins (Semi and Anti)**\n",
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a35772-bd9b-40d8-aaf9-e81dd836deb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d1479a5-b1d4-44dc-ae6f-657b0ca7e958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SemiJoinDf=cleaned_df15.alias(\"staff_df\").join(json_clean_df20.alias(\"shipments_df\"),how='left_semi',on=col(\"staff_df.shipment_id\")==col(\"shipments_df.shipment_id\"))\n",
    "display(SemiJoinDf.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5969b635-b750-43f8-bb0c-435f6fadf24b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d654b9ca-94a3-4e10-92cc-3fc48c8e9f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leftantijoin=cleaned_df15.alias(\"staff_df\").join(json_clean_df20.alias(\"shipments_df\"),how='left_anti',on=col(\"staff_df.shipment_id\")==col(\"shipments_df.shipment_id\"))\n",
    "display(leftantijoin.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc1853c2-693d-4498-9eba-6e18f72fc0cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv dataframe<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup `hub_location` (eg. \"Pune\") in a Master Latitude/Longitude Master_City_List.csv dataframe and enrich our logistics_source (merged dataframe) by adding `lat` and `long` columns for map plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb8fad81-d1a6-411e-aaa4-5b87e365d9da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "staffdf=cleaned_df15\n",
    "master_city_df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/Master_City_List.csv\",header=True,inferSchema=True)\n",
    "\n",
    "geo_tagged_df = cleaned_df15.alias(\"staff_df\").join(master_city_df.alias(\"city_df\"),col(\"staff_df.origin_hub_city\")== col(\"city_df.city_name\"),\"left\")\n",
    "#display(geo_tagged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0730e3cf-6843-4512-a8ae-f6b668d57bdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: DF of All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "Staff (dimension)\n",
    "Shipments (fact)\n",
    "Vehicle_Master (dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e530dc-3167-4673-8ab1-bfed9ec99112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "staff_df = cleaned_df15\n",
    "shipment_df = json_clean_df20\n",
    "city_df = master_city_df\n",
    "\n",
    "#Join Shipments with Staff\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "shipment_staff_df = shipment_df.alias(\"s\") \\\n",
    "    .join(\n",
    "        staff_df.alias(\"st\"),\n",
    "        col(\"s.shipment_id\") == col(\"st.shipment_id\"),\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"s.shipment_id\").alias(\"shipment_id\"),\n",
    "        col(\"s.shipment_cost\"),\n",
    "        col(\"s.shipment_date\"),\n",
    "        col(\"st.origin_hub_city\").alias(\"origin_hub_city\"),  # ✅ FIX\n",
    "        col(\"s.vehicle_type\"),\n",
    "        col(\"st.staff_first_name\"),\n",
    "        col(\"st.staff_last_name\"),\n",
    "        col(\"st.role\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "#Join Vehicle Master\n",
    "wide_shipment_history = shipment_staff_df.alias(\"ssf\") \\\n",
    "    .join(\n",
    "        city_df.alias(\"c\"),\n",
    "        col(\"ssf.origin_hub_city\") == col(\"c.city_name\"),\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"ssf.shipment_id\"),\n",
    "        col(\"ssf.shipment_cost\"),\n",
    "        col(\"ssf.shipment_date\"),\n",
    "        col(\"ssf.origin_hub_city\"),\n",
    "        col(\"ssf.vehicle_type\"),\n",
    "        col(\"ssf.staff_first_name\"),\n",
    "        col(\"ssf.staff_last_name\"),\n",
    "        col(\"ssf.role\"),\n",
    "        col(\"c.city_name\").alias(\"hub_city_name\"),\n",
    "        col(\"c.country\"),\n",
    "        col(\"c.latitude\"),\n",
    "        col(\"c.longitude\")\n",
    "    )\n",
    "\n",
    "\n",
    "wide_shipment_history.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Gold\",header=True,mode=\"overwrite\")\n",
    "\n",
    "display(wide_shipment_history.limit(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61d1a84-9c13-406f-9ada-c21af9434cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04c21ba-a7a3-41f7-97cf-e3a205c1379d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace,dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "clean_df = wide_shipment_history.withColumn(\n",
    "    \"shipment_cost\",\n",
    "    regexp_replace(col(\"shipment_cost\"), \"[₹,]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "driver_cost_df = clean_df.groupBy(\n",
    "    \"origin_hub_city\",\n",
    "    \"staff_first_name\",\n",
    "    \"staff_last_name\"\n",
    ").agg(\n",
    "    sum(\"shipment_cost\").alias(\"total_shipment_cost\")\n",
    ")\n",
    "\n",
    "\n",
    "top3df=driver_cost_df.withColumn(\"Row_numberSeqNum\",dense_rank().over(Window.partitionBy(\"origin_hub_city\").orderBy(desc(\"total_shipment_cost\")))).where(\"Row_numberSeqNum<=3\")\n",
    "display(top3df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6376f43-d074-4c47-b100-3826ecf94d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4846583-f19b-4a61-a489-82d8d9990985",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768654823128}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, datediff\n",
    "\n",
    "df_with_prev=json_clean_df15.withColumn(\"prev_shipment_date\",lag(\"shipment_date\").over(Window.partitionBy(\"vehicle_identifier\").orderBy(\"shipment_date\")))\n",
    "\n",
    "\n",
    "idle_time_df = df_with_prev.withColumn(\"idle_days\",datediff(col(\"shipment_date\"), col(\"prev_shipment_date\")))\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "idle_time_df = idle_time_df.withColumn(\n",
    "    \"idle_days\",\n",
    "    when(col(\"prev_shipment_date\").isNull(), 0)\n",
    "    .otherwise(col(\"idle_days\"))\n",
    ")\n",
    "\n",
    "idle_time_df.select(\n",
    "    \"vehicle_identifier\",\n",
    "    \"shipment_id\",\n",
    "    \"shipment_date\",\n",
    "    \"prev_shipment_date\",\n",
    "    \"idle_days\"\n",
    ").orderBy(\"vehicle_identifier\", \"shipment_date\").display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "445f50ea-5d1c-47f2-8d2b-22d6b9a782be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **7. Set Operations**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006d9ec2-fed4-45eb-a11f-2be3a2b95e19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uniondf=source1_canonical.union(source2_canonical)\n",
    "print(\"Union Count of the data:\",uniondf.count())\n",
    "intersectdf=source1_canonical.intersect(source2_canonical)\n",
    "print(\"intersect Count of the data:\",intersectdf.count())\n",
    "#EXCEPT (Distinct Difference)\n",
    "exceptdf=source1_canonical.exceptAll(source2_canonical)\n",
    "print(\"exceptAll Count of the data:\",exceptdf.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415e9785-3277-4255-a944-a66b024f6b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "DF of logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7cd7a0c-3403-4441-9c27-543251e280db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44896f5c-c6e7-4b70-8527-2d75ccb3841a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d89b420-ba0e-4b1b-a2c9-6fa9d75be997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "innerdf1.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "logistics_data_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
