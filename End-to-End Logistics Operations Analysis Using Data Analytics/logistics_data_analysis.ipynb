{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00631ab-71b1-4ad5-a654-751b3746fffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa843b9-d18d-4b6a-b92c-ab6eb2afe667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](./logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1e3853-7e94-4e81-a743-bb302ff55ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10677f57-f424-40d9-90a3-bcf94ec9c1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)\n",
    "\n",
    "**Source 1: Logistics Shipment Data (JSON Format)**\n",
    "- Data is received from the source system in JSON[Semi strcutured format]\n",
    "- key–value pairs\n",
    "\n",
    "**Source 2: Logistics Data (CSV Format – 4 Columns)**\n",
    "- Data is received in CSV format with 4 columns\n",
    "- Header present, no footer\n",
    "- Null columns and null records are there\n",
    "- Data format inconsistencies observed like age contain string value\n",
    "- Includes additional column(s)\n",
    "\n",
    "**Source 3: Logistics Data (CSV Format – 7 Columns)**\n",
    "- Data is received in CSV format with 7 columns\n",
    "- Header present, no footer\n",
    "- Contains duplicate records\n",
    "- Null columns and null records are there\n",
    "- Data format inconsistencies observed like age contain string value\n",
    "- Includes additional column(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381c25cd-503d-4662-8935-611381f53535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440d7677-d3dc-4053-99ee-ae0de28ebc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ad2f93-eff6-4706-b3fd-0552962f5f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source1_df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source1\",header=True,inferSchema=True).toDF(\"Shipment_id\",\"First_Name\",\"Last_Name\",\"Age\",\"Role\")\n",
    "\n",
    "print(source1_df.printSchema())\n",
    "\n",
    "display(source1_df.show(10,False))\n",
    "display(source1_df.columns)\n",
    "display(source1_df.dtypes) #Age is in string format and shippment ID is in string type\n",
    "print(\"actual count of the data in Source1:\",source1_df.count())\n",
    "print(\"De-duplicated record count (all columns using distinct):\",source1_df.distinct().count())\n",
    "print(\"de-duplicated given id column count:\",source1_df.dropDuplicates(['Shipment_id']).count())\n",
    "\n",
    "\n",
    "source2_df=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source2\",header=True)\n",
    "display(source2_df.show(10,False))\n",
    "display(source2_df.columns)\n",
    "display(source2_df.dtypes) #Age is in string format and shippment ID is in string type\n",
    "print(\"actual count of the data in Source2:\",source2_df.count())\n",
    "print(\"De-duplicated record count (all columns using dropDuplicates)\",source2_df.dropDuplicates().count())\n",
    "print(\"de-duplicated given id column count:\",source2_df.dropDuplicates(['Shipment_id']).count())\n",
    "\n",
    "display(source1_df.summary())\n",
    "display(source2_df.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e108dfe1-7eb5-4e65-af8b-55202f227770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having:<br>\n",
    "3. fewer columns than expected<br>\n",
    "4. more columns than expected<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb0a0e8-2072-4fce-86ee-2fde74755a74",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768070510245}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768112750026}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "find_df1 = source1_df.where(\"Shipment_id rlike '[A-Za-z]'\")\n",
    "print(\"shipment_id is non-numeric from source1_df:\")\n",
    "display(find_df1)\n",
    "source1_df.schema\n",
    "\n",
    "find_df1 = source2_df.where(\"Shipment_id rlike '[A-Za-z]'\")\n",
    "print(\"shipment_id is non-numeric from source2_df:\")\n",
    "display(find_df1)\n",
    "source2_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c34378-8d5e-45a1-ac94-31e77d7d8945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Count rows having:<br>\n",
    "3. fewer columns than expected<br>\n",
    "4. more columns than expected<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e4b3a5-3a13-4bf4-a493-b32840400b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size,col,split,when\n",
    "expected_cols = 5   # change as per your file\n",
    "delimiter = \",\"\n",
    "print(\"Source df1 Count rows:\")\n",
    "raw_df1=spark.read.text(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source1\")\n",
    "df_with_col_count=raw_df1.withColumn('actual_col_count',size(split(col('value'),delimiter)))\n",
    "\n",
    "df_flagged=df_with_col_count.withColumn(\"column_status\",when(col(\"actual_col_count\")<expected_cols,\"FEWER_COLUMNS\").when(col(\"actual_col_count\")>expected_cols,\"MORE_COLUMNS\").otherwise(\"EXPECTED_COLUMNS\"))\n",
    "display(df_flagged)\n",
    "\n",
    "df_bad_record = df_with_col_count.where(col(\"actual_col_count\") != expected_cols).groupBy(\"actual_col_count\").count()\n",
    "display(df_bad_record)\n",
    "\n",
    "print(\"Source df2 Count rows:\")\n",
    "raw_df2=spark.read.text(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source2\")\n",
    "df_with_col_count1=raw_df2.withColumn(\"actual_column_count\",size(split(col(\"value\"),delimiter)))\n",
    "df_flagged1=df_with_col_count1.withColumn(\"column_status\",when(col(\"actual_column_count\")<7,\"FEWER_COLUMNS\").when(col(\"actual_column_count\")>7,\"MORE_COLUMNS\").otherwise(\"EXPECTED_COLUMNS\"))\n",
    "display(df_flagged1)\n",
    "df_bad_record1=df_flagged1.where(col(\"actual_column_count\")!=7).groupBy(\"actual_column_count\").count()\n",
    "display(df_bad_record1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a1284eb-b0ea-40d9-b377-dd4c85347d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Spark Session Object\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"Logistic_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68f94fa-4095-4108-9d56-496b68660a48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343f47ce-8521-4ab6-8cb2-1b1a0f6339d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema<br>\n",
    "2. Align them into a single canonical schema: \n",
    "- shipment_id,<br>\n",
    "- first_name,<br>\n",
    "- last_name,<br>\n",
    "- age,<br>\n",
    "- role,<br>\n",
    "- hub_location,<br>\n",
    "- vehicle_type,<br>\n",
    "- data_source<br>\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f03b86c-7f9c-441d-8b5f-991c35a5758f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Source 1 (System A): id, fname, lname, age<br>\n",
    "Source 2 (System B): shipment_id, full_name, years<br>\n",
    "Canonical schema (decided by you):- shipment_id, first_name, last_name, age<br>\n",
    "\n",
    "All source data is reshaped into this structure before further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da91b102-4109-450f-8b5a-6dcd7d2ba9c2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768116708883}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "source1_raw = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"mode\", \"permissive\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source1\")\n",
    "\n",
    "source2_raw = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"mode\", \"permissive\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_source2\")\n",
    "\n",
    "source1_canonical = source1_raw.select(\n",
    "    col(\"shipment_id\").cast(\"string\").alias(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\").cast(\"string\"),\n",
    "    col(\"role\"),\n",
    "    lit(None).cast(\"string\").alias(\"hub_location\"),\n",
    "    lit(None).cast(\"string\").alias(\"vehicle_type\"),\n",
    "    lit(\"system1\").alias(\"data_source\")\n",
    ")\n",
    "source2_canonical = source2_raw.select(\n",
    "    col(\"shipment_id\").cast(\"string\").alias(\"shipment_id\"),\n",
    "    col(\"first_name\"),\n",
    "    col(\"last_name\"),\n",
    "    col(\"age\").cast(\"string\"),\n",
    "    col(\"role\"),\n",
    "    col(\"hub_location\"),\n",
    "    col(\"vehicle_type\"),\n",
    "    lit(\"system2\").alias(\"data_source\")\n",
    ")\n",
    "canonical_df = source1_canonical.unionByName(source2_canonical)\n",
    "display(canonical_df)\n",
    "print(canonical_df.printSchema())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5030ef-d3b7-4323-8017-0124de47553c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing:\n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8befc2f-c1eb-4aad-9ba4-14012617a99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "print(\"Before dropping duplicates:\", canonical_df.count())\n",
    "canonical_df1 = canonical_df.na.drop(how='any',subset=[\"shipment_id\",\"role\"])\n",
    "print(\"After dropping duplicates:\", canonical_df1.count())\n",
    "\n",
    "\n",
    "#Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "print(\"Before dropping duplicates:\", canonical_df1.count())\n",
    "canonical_df2 = canonical_df1.na.drop(how='all',subset=[\"first_name\",\"last_name\"])\n",
    "print(\"After dropping duplicates:\", canonical_df2.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7daf5756-d515-4ee8-b0c9-4ca3f9445fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Join Readiness Rule\n",
    "A record must have a valid join key (shipment_id) to participate in downstream joins.\n",
    "If the join key is NULL, the record is not usable and must be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b00af5-054e-4067-b94d-924d8675d600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Join Readiness Rule - Drop records where the join key is null: shipment_id\n",
    "from pyspark.sql.functions import col\n",
    "print(\"Before Join Readiness check:\", canonical_df2.count())\n",
    "canonical_df_join_ready = canonical_df2.filter(col(\"shipment_id\").isNotNull())\n",
    "print(\"After Join Readiness check:\", canonical_df_join_ready.count())\n",
    "\n",
    "#OR\n",
    "\n",
    "#canonical_df.na.drop(subset=[\"shipment_id\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bfca716-8686-46a2-91fb-6f3e713a4eba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####3.Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1<br>\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV<br>\n",
    "bike to TwoWheeler<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a878cf-4fa4-496e-a85c-631689756c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "cleaned_df=canonical_df_join_ready.na.fill(\"-1\",subset=[\"age\"])\n",
    "\n",
    "#Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "cleaned_df1=cleaned_df.na.fill(\"UNKNOWN\",subset=[\"vehicle_type\"])\n",
    "\n",
    "#Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 and \"\" to -1\n",
    "replacedata={'ten':'-1','':'-1'}\n",
    "cleaned_df2=cleaned_df1.na.replace(replacedata,subset=[\"age\"])\n",
    "\n",
    "#Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV and bike to TwoWheeler\n",
    "replacedata1={'Truck':'LMV','Bike':'TwoWheeler'}\n",
    "cleaned_df3=cleaned_df2.na.replace(replacedata1,subset=[\"vehicle_type\"])\n",
    "display(cleaned_df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560177e5-635c-4c5d-8866-aaef7dab2964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342e4962-c24f-4f31-916b-9bf07e49d895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Detail Dataframe creation <br>\n",
    "1. Read Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23760b2-6e37-4881-a5ae-1414853c1a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_clean_df=spark.read.option(\"multiLine\", True).json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/logistics_data_analysis/Silver_data/logistics_shipment_detail_3000.json\")\n",
    "display(json_clean_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f56140-82e6-468e-9528-cca271681367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: logistics_shipment_detail_3000.json<br>: domain as 'Logistics'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: logistics_source1 & logistics_source2<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: logistics_shipment_detail_3000.json (and the merged master files)\n",
    "hub_location - Convert values to initcap case<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: logistics_shipment_detail_3000.json\n",
    "Convert shipment_ref to string<br>\n",
    "Pad to 10 characters with leading zeros<br>\n",
    "Convert dispatch_date to yyyy-MM-dd<br>\n",
    "Ensure delivery_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: logistics_source1 & logistics_source2 <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: logistics_source1 & logistics_source2<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: All 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4dbb5f-8a73-4514-9ec8-525195dd9534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower,upper,initcap\n",
    "#Add a column\n",
    "#Source File: logistics_shipment_detail_3000.json\n",
    "#domain as 'Logistics'\n",
    "json_clean_df1=json_clean_df.withColumn(\"domain\",lit(\"Logistic\"))\n",
    "display(json_clean_df1.limit(5))\n",
    "\n",
    "#Column Uniformity: role - Convert to lowercase\n",
    "cleaned_df4=cleaned_df3.withColumn(\"role\",lower(col(\"role\")))\n",
    "\n",
    "\n",
    "#vehicle_type - Convert values to UPPERCASE\n",
    "cleaned_df5=cleaned_df4.withColumn(\"vehicle_type\",upper(col(\"vehicle_type\")))\n",
    "\n",
    "\n",
    "#Source Files: logistics_shipment_detail_3000.json (and the merged master files) hub_location - Convert values to initcap case\n",
    "cleaned_df6=cleaned_df5.withColumn(\"hub_location\",initcap(col(\"hub_location\")))\n",
    "display(cleaned_df5.limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c897f0a-adfe-4bb1-9190-5cb9d8f6d3af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Format Standardization:<br>\n",
    "Source Files: logistics_shipment_detail_3000.json Convert shipment_ref to string<br>\n",
    "Pad to 10 characters with leading zeros<br>\n",
    "Convert dispatch_date to yyyy-MM-dd<br>\n",
    "Ensure delivery_cost has 2 decimal precision<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0294c7d3-21df-4440-8512-31cabb637c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date,round\n",
    "#Format Standardization:\n",
    "#Source Files: logistics_shipment_detail_3000.json Convert shipment_ref to string\n",
    "json_clean_df1.printSchema()\n",
    "#->shipment_ref column not found in the data\n",
    "\n",
    "#Pad to 10 characters with leading zeros\n",
    "#->Not sure which column\n",
    "\n",
    "#Convert dispatch_date to yyyy-MM-dd\n",
    "json_clean_df2=json_clean_df1.withColumn(\"shipment_date\",to_date(col(\"shipment_date\"),'yyyy-MM-dd'))\n",
    "\n",
    "\n",
    "#Ensure delivery_cost has 2 decimal precision\n",
    "json_clean_df3=json_clean_df2.withColumn(\"shipment_cost\",round(col(\"shipment_cost\"),2))\n",
    "display(json_clean_df3.limit(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f9969e-01cd-400e-b3bc-e68146ecad2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: logistics_source1 & logistics_source2<br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e9c3ba-9042-4cd6-99ba-5a2cbc743fd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Source File: logistics_source1 & logistics_source2\n",
    "#age: Cast String to Integer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cleaned_df6.printSchema()\n",
    "cleaned_df7 = cleaned_df6.withColumn(\"age\",col(\"age\").cast(\"int\"))\n",
    "cleaned_df7.printSchema()\n",
    "\n",
    "#shipment_weight_kg: Cast to Double\n",
    "json_clean_df3.printSchema()\n",
    "#->Shippement_weight is already in double format\n",
    "\n",
    "#is_expedited: Cast to Boolean\n",
    "#-->Not Sure what to do, current dataset has no columns with is_expedited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "336565ea-1395-4cd7-b7b0-6e84931153ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Naming Standardization\n",
    "#Source File: logistics_source1 & logistics_source2\n",
    "#Rename: first_name to staff_first_name\n",
    "#Rename: last_name to staff_last_name\n",
    "cleaned_df8=cleaned_df7.withColumnsRenamed({\"first_name\":\"staff_first_name\",\"last_name\":\"staff_last_name\"})\n",
    "\n",
    "#Rename: hub_location to origin_hub_city\n",
    "cleaned_df9=cleaned_df8.withColumnRenamed(\"hub_location\",\"origin_hub_city\")\n",
    "cleaned_df9.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0306338f-0085-4d28-9b32-a8ca1092ae4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: All 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89f2be00-4f0c-404a-b1cc-21bd9c66342c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#question is not clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d848ae47-c118-4749-8961-09683000f770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "404c6a94-3a4e-4ce5-bf84-bcac08828c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "logistics_data_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
