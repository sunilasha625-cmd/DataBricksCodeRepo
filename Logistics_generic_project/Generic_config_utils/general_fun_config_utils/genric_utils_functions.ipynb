{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7716af-a0df-438b-9d5b-669c75cbfeee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This Notebook contains the following functions:<br>\n",
    "1. Python UDF Function\n",
    "2. Generic Framework - Business specific \n",
    "3. Generic Framework - Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135fbf09-93eb-484f-bb81-d1f9596fa7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running Utility Notebook to initialize all functions to use further\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1dcafe-66e2-4d28-9ff1-316374e9716b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating UDF to convert string to number, hence we don't have to filter string values or manipulate string values manually using dictionary word_to_num={'one':'1','two':'2'}<br>\n",
    "Eg. If we pass \"twenty thousand two hundred and one\" -> 20201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c9fd7b-5dcd-4e37-a790-43572595c708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install word2number\n",
    "#print(w2n.word_to_num(\"two hundred and thirty five\")) ---->235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695610b8-651c-4d96-b27e-6d9c65d3d46e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from word2number import w2n\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def word_to_num(value):\n",
    "    try:\n",
    "        #if already numeric\n",
    "        return int(value)\n",
    "    except:\n",
    "        try:\n",
    "            return w2n.word_to_num(value.lower())\n",
    "        except:\n",
    "            return None\n",
    "#Register this Python function and tell Spark what type it will return\n",
    "#udf(function, returnType)\n",
    "word_to_num_udf=udf(word_to_num,IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5923216c-90e6-45fb-bad6-7bb64ee8bc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Inline/Business Specific Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e0a09a-4c24-48e8-af4b-7d50299734b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def standardize_staff(df):\n",
    "    return (\n",
    "        df.withColumn(\"shipment_id\",word_to_num_udf(col(\"shipment_id\")).cast(\"long\"))\n",
    "        .withColumn(\"age\",word_to_num_udf(col(\"age\")).cast(\"int\"))\n",
    "        .withColumn(\"role\",f.lower(\"role\"))\n",
    "        .withColumn(\"origin_hub_city\",f.initcap(col(\"origin_hub_city\")))\n",
    "        .withColumn(\"load_dt\",f.current_timestamp())\n",
    "        .withColumn(\"full_name\",f.concat_ws(\" \",\"first_name\",\"last_name\"))\n",
    "        .withColumn(\"hub_location\",f.initcap(\"hub_location\"))\n",
    "        .drop(\"first_name\",\"last_name\")\n",
    "        .withColumnRenamed(\"full_name\",\"staff_full_name\")\n",
    "    )\n",
    "\n",
    "def scrub_geotag(df):\n",
    "    return(\n",
    "        df.withColumn(\"city_name\",f.initcap(\"city_name\"))\n",
    "        .withColumn(\"masked_hub_location\",f.initcap(\"masked_hub_location\"))\n",
    "    )\n",
    "\n",
    "def standardize_shipments(df):\n",
    "    return(\n",
    "        df\n",
    "        .withColumn(\"domain\", F.lit(\"Logistics\"))\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"is_expedited\", F.lit(False).cast(\"boolean\"))\n",
    "        .withColumn(\"shipment_date\", F.to_date(\"shipment_date\", \"yy-MM-dd\"))\n",
    "        .withColumn(\"shipment_cost\", F.round(\"shipment_cost\", 2))\n",
    "        .withColumn(\"shipment_weight_kg\", F.col(\"shipment_weight_kg\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "def enrich_shipments(df):\n",
    "    return(\n",
    "        df.withCoulmn(\"route_segment\",f.concat_ws(\"-\",\"source_city\",\"destination_city\"))\n",
    "        .withColumn(\"vehicle_identifier\",f.concat_ws(\"_\",\"vehicle_type\", \"shipment_id\"))\n",
    "        .withColumn(\"shipment_year\",f.year(\"shipment_year\"))\n",
    "        .withColumn(\"shipment_year\",f.month(\"shipment_year\"))\n",
    "        .withColumn(\"is_weekend\",f.dayofweek(\"shipment_date\").isin([1,7]))\n",
    "        .withColumn(\"is_expedited\",f.col(\"shipment_status\").isin(\"IN_TRANSIT\",\"DELIVERED\"))\n",
    "        .withColumn(\"cost_per_kg\",\n",
    "        f.round(F.col(\"shipment_cost\") / F.col(\"shipment_weight_kg\"), 2))\n",
    "        .withColumn(\"tax_amount\",\n",
    "        f.round(F.col(\"shipment_cost\") * 0.18, 2))\n",
    "        .withColumn(\"days_since_shipment\",\n",
    "        f.datediff(F.current_date(), \"shipment_date\"))\n",
    "        .withColumn(\"is_high_value\",\n",
    "        f.col(\"shipment_cost\") > 50000)\n",
    "    )\n",
    "\n",
    "def split_columns(df):\n",
    "    return (\n",
    "        df.withColumn(\"order_prefix\",f.substring(\"order_id\",1,3))\n",
    "        .withColumn(\"order_sequence\",f.substring(\"order_id\",4,10))\n",
    "        .withColumn(\"ship_year\",f.year(\"shipment_date\"))\n",
    "        .withColumn(\"ship_month\",f.month(\"shipment_date\"))\n",
    "        .withColumn(\"ship_day\",f.dayofweek(\"shipment_date\"))\n",
    "        .withColumn(\"route_lane\",f.concat(\"->\",\"source_city\",\"destination_city\"))\n",
    "    )\n",
    "\n",
    "def mask_name(col):\n",
    "    return f.concat(\n",
    "        f.substring(col, 1, 2),\n",
    "        f.lit(\"****\"),\n",
    "        f.substring(col, -1, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac6dc05-9c13-48a5-b01f-9beff3c3723c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9249f5b7-13d5-4d4a-94cc-00b3d9f94184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "def get_spark_session(app_name=\"Some Anonymous Data Engineering Project\"):\n",
    "    try:\n",
    "        spark=SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            return spark\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (SparkSession.builder.config(\"spark.sql.shuffle.partitions\", \"1\").appName(app_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47099f5f-38a0-4663-a278-727244bf6a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Think of shuffle as:\n",
    "\n",
    "Spark collects data from all workers → redistributes it → creates N buckets (partitions).\n",
    "\n",
    "spark.sql.shuffle.partitions = N\n",
    "means: “After shuffle, split the result into N pieces.”\n",
    "\n",
    "Each partition = one task = one core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0418ec-399b-440e-b268-71f87a550307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#All generic functions for reading data from files & tables\n",
    "def read_csv_df(spark,path,header=True,infer_schema=True,sep=\",\"):\n",
    "    return_df=spark.read.option(\"header\",header).option(\"inferSchema\",infer_schema).option(\"sep\",sep)\\\n",
    "        .csv(path)\n",
    "    return return_df\n",
    "\n",
    "def read_json_df(spark,path,mline=True):\n",
    "    return_df=spark.read.json(path,multiLine=mline,mode=\"PERMISSIVE\")\n",
    "    return return_df\n",
    "\n",
    "\n",
    "def read_delta_df(spark,path):\n",
    "    return spark.read.format(\"delta\").load(path)\n",
    "\n",
    "def read_file(spark,filetype,path,header=True,infer_schema=True,mline=True):\n",
    "    if filetype==\"csv\":\n",
    "        return spark.read.csv(path,header=header,inferSchema=infer_schema)#read_csv_df(spark,path)\n",
    "    elif filetype==\"json\":\n",
    "        return read_json_df(spark,path)\n",
    "    elif filetype==\"delta\":\n",
    "        return read_delta_df(spark,path)\n",
    "    elif filetype=='orc':\n",
    "        return spark.read.orc(path)\n",
    "    else:\n",
    "        raise Exception(\"File type not supported\")\n",
    "\n",
    "def read_table(spark,table_name):\n",
    "    return spark.table(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3bc17f9-d9fc-4998-a615-264f3781241e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Return Joined DF\n",
    "def join_df(df1,df2,how=\"inner\",on=\"shipment_id\"):#To avoid cartesian/cross join, i am adding some column in the on\n",
    "    return df1.join(df2, on=on, how=how)\n",
    "\n",
    "def unionDf(df1,df2):\n",
    "    return df1.union(df2)\n",
    "def unionDfSql(spark,view1,view2):    \n",
    "    returndf=spark.sql(f\"select * from view1 union select * from view2\")\n",
    "    return returndf\n",
    "\n",
    "def mergeDf(df1,df2,allowmissingcol=True):\n",
    "    return df1.unionByName(df2, allowMissingColumns=allowmissingcol)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "genric_utils_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
