{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e944e6-e68c-4d67-99ea-0cb41312c7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Auto Loader** is Databricks’ cloud-native file ingestion engine for ingesting new files incrementally from object storage.<br>\n",
    "\n",
    "Auto Loader incrementally processes only new files arriving in cloud storage, without reprocessing old files, and scales to millions of files efficiently.\n",
    "\n",
    "Supported Sources:\n",
    "- AWS S3\n",
    "- Azure ADLS Gen2\n",
    "- Google Cloud Storage (GCS)\n",
    "\n",
    "Auto Loader uses two main mechanisms or Modes:\n",
    "It detects new files only using either:\n",
    "- **Directory listing** - Directory listing scans storage paths to detect new files using the checkpoint feature\n",
    "- File notification - Processes files as soon as they arrive at scale ((recommended))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68e8e4cf-dcae-4f72-baae-b7fd50d8f92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A checkpoint is like a bookmark that remembers how much work is already done, so Spark doesn’t start again from the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7520717a-10b5-4fb9-a028-ef8077bb0eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Directory listing Mode**<br>\n",
    "1. Auto Loader periodically scans the storage directory, lists all files, compares them with its checkpoint, and processes only the newly discovered files.<br>\n",
    "1. Auto Loader scans the input directory\n",
    "2. Lists all files present\n",
    "3. Compares with checkpoint metadata\n",
    "4. Identifies new files\n",
    "5. Reads only new files\n",
    "6. Updates checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124ad64f-2dd4-4114-acb6-e4cc60244b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**File notification Mode**<br>\n",
    "Auto Loader does not scan directories.\n",
    "Instead, it listens to cloud storage events that notify when a new file arrives.\n",
    "\n",
    "1. File arrives in cloud storage\n",
    "2. Cloud emits an event\n",
    "3. Event is sent to a message queue\n",
    "4. Auto Loader reads event metadata\n",
    "5. Auto Loader reads only that file\n",
    "6. Updates checkpoint\n",
    "\n",
    "No directory scanning at all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960dafdf-6a7b-4f89-af3c-f89d224f3b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Directory listing\n",
    "1. Spark lists directory\n",
    "2. Detects new CSV or any files\n",
    "3. Infers schema / evolves if needed\n",
    "4. store the file into bronze layer\n",
    "5. Updates checkpoint (file1 is processed...)\n",
    "6. Waits for next job run to list directory and collect any new files into bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8ed585-3f17-4225-8558-9049e0f339e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Part                   | Meaning                    |\n",
    "| ---------------------- | -------------------------- |\n",
    "| `spark`                | Spark session              |\n",
    "| `readStream`           | Read data **continuously** |\n",
    "| `format(\"cloudFiles\")` | Use **Auto Loader**        |\n",
    "<br>\n",
    "\n",
    "**option(\"checkpointLocation\")**<br>\n",
    "CheckpointLocation is a memory folder that remembers how much data is already processed.<br>\n",
    "\n",
    "**If you remove checkpointLocation**\n",
    "\n",
    "- Stream starts from scratch\n",
    "- Files get reprocessed\n",
    "- Duplicate data risk\n",
    "\n",
    "-------\n",
    "\n",
    "**option(\"cloudFiles.schemaLocation)**<br>\n",
    "SchemaLocation is a folder where Auto Loader remembers the data structure (schema).<br>\n",
    "\n",
    "**What it stores**\n",
    "\n",
    "- Column names\n",
    "- Data types\n",
    "- Schema versions (history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "282e640d-a3d1-4402-8e5a-2385b6b85378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.readStream.format(\"cloudFiles\")\\\n",
    "    .option(\"cloudFiles.format\",\"csv\")\\\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\",\"addNewColumns\")\\\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\",5)\\\n",
    "    .option(\"cloudFiles.inferColumnTypes\",True)\\\n",
    "    .option(\"checkpointLocation\",\"/Volumes/sales_project/sales/pipeline_1/_checkpoint/\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\",\"/Volumes/sales_project/sales/pipeline_1/_schema/\")\\\n",
    "    .option(\"header\",True)\\\n",
    "    .load(\"/Volumes/sales_project/sales/pipeline_1/AWS_Raw_Data/\")#this can be s3/adls/gcs\n",
    "    #.option(\"cloudFiles.useNotifications\", \"true\") (Remove this option to enable directory listing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8288fcd3-6c13-469f-9e50-4162b2d55e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "availableNow=True tells Spark to process all data that is currently available and then stop the stream automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109d1b1a-532b-452e-b7d9-d89183a4d78a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.writeStream.trigger(availableNow=True)\\\n",
    "    .option(\"checkpointLocation\",\"/Volumes/sales_project/sales/pipeline_1/_checkpoint/\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\",\"/Volumes/sales_project/sales/pipeline_1/_schema/\")\\\n",
    "    .start(\"/Volumes/sales_project/sales/pipeline_1/streamwrite1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b427b7f4-06e4-430f-8f68-da11eed51aef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 10"
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"delta\").load(\"/Volumes/sales_project/sales/pipeline_1/streamwrite1/\")\n",
    "display(df2)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5724402994500045,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lakeflow_ingestion_cloudfile_autoloader_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
