{"version":"NotebookV1","origId":4136916387898490,"name":"gold_core_curated_5a_sparksql","language":"python","commands":[{"version":"CommandV1","origId":6358089889637131,"guid":"069a23bc-6ff3-4db1-b325-62c85ed2613c","subtype":"command","commandType":"auto","position":0.9166666666666667,"command":"%run /Workspace/Users/infoblisstech@gmail.com/databricks-code-repo/4_logistics_usecase/generic_project/general_conf_utils_1_2/util_functions2","commandVersion":0,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":"Notebook not found: util_functions2. Notebooks can be specified via a relative path (./Notebook or ../folder/Notebook) or via an absolute path (/Abs/Path/to/Notebook). Make sure you are specifying the path correctly.\n\nStacktrace:\n  /Users/infoblisstech@gmail.com/databricks-code-repo/4_logistics_usecase/generic_project/bronze_ingest3: python","errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846576303,"submitTime":1768846569780,"finishTime":1768846579103,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"df953069-864d-4123-802e-7229ecf4f7e2"},{"version":"CommandV1","origId":6933974008983480,"guid":"0465bb75-4f19-462d-a7bd-d42a30c8db58","subtype":"command","commandType":"auto","position":0.9583333333333334,"command":"shipments=f\"{SILVER}/shipments\"\nstaff=f\"{SILVER}/staff\"","commandVersion":8,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846579120,"submitTime":1768846569958,"finishTime":1768846579203,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"6dc4ee1e-0c5b-464d-b56a-402e996268a2"},{"version":"CommandV1","origId":4921811840474177,"guid":"26a25bac-903c-4ece-a73e-3f8bbbef35ef","subtype":"command","commandType":"auto","position":0.9791666666666667,"command":"spark.sql(f\"\"\"\n          CREATE OR REPLACE TEMP VIEW silver_staff_geo_tv\nAS\nSELECT s.*  \nFROM {SILVERDB}.silver_staff s\nSEMI JOIN {SILVERDB}.silver_geotag s_geo\nON s.hub_location = s_geo.city_name\n          \"\"\")\n\nspark.sql(f\"\"\"\n          CREATE OR REPLACE TEMP VIEW silver_staff_geo_latlong_tv\nAS\nSELECT s.*,s_geo.latitude,s_geo.longitude \nFROM silver_staff_geo_tv s\nINNER JOIN {SILVERDB}.silver_geotag s_geo\nON s.hub_location = s_geo.city_name\n          \"\"\")","commandVersion":42,"state":"finished","results":{"type":"listResults","data":[{"type":"mimeBundle","data":{"text/plain":"DataFrame[]"},"executionCount":49,"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"kernelSessionId":"a2cba0ae-ec74fef4aa8fce1c95e8064c"}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":{"type":"baseError","stackFrames":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)","File \u001B[0;32m<command-4921811840474177>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124m          CREATE OR REPLACE TEMP VIEW silver_staff_geo_tv\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124mAS\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124mSELECT s.*  \u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124mFROM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSILVERDB\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.silver_staff s\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124mSEMI JOIN \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSILVERDB\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.silver_geotag s_geo\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124mON s.hub_location = s_geo.city\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m          \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m          CREATE OR REPLACE TEMP VIEW silver_staff_geo_latlong_tv\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124mAS\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;124mON s.hub_location = s_geo.city\u001B[39m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124m          \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/session.py:875\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m    872\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n\u001B[1;32m    874\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001B[0;32m--> 875\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    877\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n","File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `s_geo`.`city` cannot be resolved. Did you mean one of the following? [`s_geo`.`country`, `s_geo`.`city_name`, `s`.`age`, `s_geo`.`latitude`, `s`.`load_dt`]. SQLSTATE: 42703; line 7 pos 20;\n'CreateViewCommand `silver_staff_geo_tv`, SELECT s.*  \nFROM prodcatalog.logistics.silver_staff s\nSEMI JOIN prodcatalog.logistics.silver_geotag s_geo\nON s.hub_location = s_geo.city, false, true, LocalTempView, UNSUPPORTED, false\n+- 'Project [s.*]\n   +- 'Join LeftSemi, (hub_location#17213 = 's_geo.city)\n      :- SubqueryAlias s\n      :  +- SubqueryAlias prodcatalog.logistics.silver_staff\n      :     +- Relation prodcatalog.logistics.silver_staff[shipment_id#17210L,age#17211,role#17212,hub_location#17213,vehicle_type#17214,origin_hub_city#17215,load_dt#17216,staff_full_name#17217] parquet\n      +- SubqueryAlias s_geo\n         +- SubqueryAlias prodcatalog.logistics.silver_geotag\n            +- Relation prodcatalog.logistics.silver_geotag[city_name#17218,country#17219,latitude#17220,longitude#17221,masked_hub_location#17222] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:645)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:192)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:484)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1078)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1077)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1077)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1077)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1080)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:484)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:484)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:484)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:863)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:826)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3811)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3635)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3458)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{},"jupyterProps":{"ename":"AnalysisException","evalue":"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `s_geo`.`city` cannot be resolved. Did you mean one of the following? [`s_geo`.`country`, `s_geo`.`city_name`, `s`.`age`, `s_geo`.`latitude`, `s`.`load_dt`]. SQLSTATE: 42703; line 7 pos 20;\n'CreateViewCommand `silver_staff_geo_tv`, SELECT s.*  \nFROM prodcatalog.logistics.silver_staff s\nSEMI JOIN prodcatalog.logistics.silver_geotag s_geo\nON s.hub_location = s_geo.city, false, true, LocalTempView, UNSUPPORTED, false\n+- 'Project [s.*]\n   +- 'Join LeftSemi, (hub_location#17213 = 's_geo.city)\n      :- SubqueryAlias s\n      :  +- SubqueryAlias prodcatalog.logistics.silver_staff\n      :     +- Relation prodcatalog.logistics.silver_staff[shipment_id#17210L,age#17211,role#17212,hub_location#17213,vehicle_type#17214,origin_hub_city#17215,load_dt#17216,staff_full_name#17217] parquet\n      +- SubqueryAlias s_geo\n         +- SubqueryAlias prodcatalog.logistics.silver_geotag\n            +- Relation prodcatalog.logistics.silver_geotag[city_name#17218,country#17219,latitude#17220,longitude#17221,masked_hub_location#17222] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:645)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:192)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:484)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1078)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1077)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1077)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1077)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1080)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:484)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:484)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:484)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:863)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:826)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3811)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3635)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3458)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"},"sqlProps":{"sqlState":"42703","errorClass":"UNRESOLVED_COLUMN.WITH_SUGGESTION","stackTrace":"org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:645)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:192)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:484)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1078)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1(CheckAnalysis.scala:1077)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$foreachUpSkippingSecureView$1$adapted(CheckAnalysis.scala:1077)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.traverse$1(CheckAnalysis.scala:1077)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.foreachUpSkippingSecureView(CheckAnalysis.scala:1080)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:484)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:484)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:484)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:439)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:145)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:863)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:826)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3811)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3635)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3458)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)","startIndex":365,"stopIndex":374,"pysparkCallSite":"","pysparkFragment":"","pysparkSummary":"","breakingChangeInfo":null}},"workflows":[],"startTime":1768846579220,"submitTime":1768846569960,"finishTime":1768846589914,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["mimeBundle",null]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"ac8fa46a-7eeb-4cc5-a0fa-4166bddf362b"},{"version":"CommandV1","origId":8576460854135469,"guid":"6747966e-5c55-4b11-81c9-fadbee1f5515","subtype":"command","commandType":"auto","position":1.0,"command":"spark.sql(f\"\"\"\n          CREATE OR REPLACE TABLE {GOLDDB}.gold_core_curated_tbl\nUSING DELTA\nAS\nSELECT\n    s.shipment_id,\n    CONCAT(\n        SUBSTRING(s.staff_full_name, 1, 2),\n        '****',\n        SUBSTRING(s.staff_full_name, -1, 1)\n    ) AS masked_staff_name,\n    s.role,\n    s.origin_hub_city,\n    s.latitude,\n    s.longitude,\n    sh.shipment_cost,\n    sh.shipment_year,\n    sh.shipment_month,\n    sh.route_segment,\n    sh.cost_per_kg,\n    sh.tax_amount,\n    sh.ingestion_timestamp,\n    sh.is_expedited,\n    sh.is_weekend,\n    sh.is_high_value,\n    sh.order_prefix,\n    sh.order_sequence,\n    sh.ship_day,\n    sh.route_lane\nFROM silver_staff_geo_latlong_tv s\nINNER JOIN {SILVERDB}.silver_shipments sh\n    USING (shipment_id);\n          \"\"\")","commandVersion":52,"state":"finished","results":{"type":"listResults","data":[{"type":"mimeBundle","data":{"text/plain":"DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"},"executionCount":50,"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"kernelSessionId":"a2cba0ae-ec74fef4aa8fce1c95e8064c"}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":{"type":"baseError","stackFrames":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{},"jupyterProps":null,"sqlProps":null},"workflows":[],"startTime":1768846589934,"submitTime":1768846569962,"finishTime":1768846596122,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["mimeBundle",null]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"c90f49e2-3d3c-4784-b9b3-a0569bec3df4"},{"version":"CommandV1","origId":6172445262621888,"guid":"f87dbc27-d509-4925-bb4f-c864300429a2","subtype":"command","commandType":"auto","position":0.8611111111111112,"command":"dbutils.widgets.text(\"catalog\",\"\")\nCATALOG=dbutils.widgets.get(\"catalog\").strip()\ndbutils.widgets.text(\"schema\",\"\")\nSCHEMA=dbutils.widgets.get(\"schema\").strip()","commandVersion":0,"state":"input","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"d5cc2288-a025-4336-96a4-c6fb330dafa6"},{"version":"CommandV1","origId":6172445262621889,"guid":"ef1e99b0-24cb-4ee1-9024-bbf674fe1a07","subtype":"command","commandType":"auto","position":0.888888888888889,"command":"%python\nimport json\n\nconfig_nb_output = dbutils.notebook.run(\n    \"/Workspace/Users/infoblisstech@gmail.com/databricks-code-repo/4_logistics_usecase/generic_project/general_conf_utils_1_2/configs_path1\",\n    120,{\"catalog\": CATALOG,\"schema\": SCHEMA})\n\nconfig_dict = json.loads(config_nb_output)\n\nCATALOG = config_dict[\"CATALOG\"]\nSCHEMA = config_dict[\"SCHEMA\"]\nSRC=config_dict[\"SRC\"]\nBRONZE = config_dict[\"BRONZE\"]\nSILVER = config_dict[\"SILVER\"]\nGOLD = config_dict[\"GOLD\"]\nSILVERDB = config_dict[\"SILVERDB\"]\nGOLDDB = config_dict[\"GOLDDB\"]","commandVersion":1,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":"One of the commands could not be parsed","errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768746808320,"submitTime":1768746803774,"finishTime":1768746845803,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"d9feda18-41f9-4d04-82d4-e5ddfdb8627a"},{"version":"CommandV1","origId":4921811840474250,"guid":"e58979b2-36f3-49bd-acdf-a239afac1a06","subtype":"script","commandType":"auto","position":1.027777777777778,"command":"","commandVersion":2,"state":"finished","results":{"type":"listResults","data":[{"type":"ansi","data":"Requirement already satisfied: word2number in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5bd8ea84-d4ef-4fde-b034-4f026e09b374/lib/python3.12/site-packages (1.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n","name":"stdout","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{}}],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846576303,"submitTime":0,"finishTime":1768846578004,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["069a23bc-6ff3-4db1-b325-62c85ed2613c"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[["ansi",293]],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"091c523a-36d9-4d3e-afb9-fb90e63708f3"},{"version":"CommandV1","origId":4921811840474251,"guid":"90c8be4c-b1f7-4efc-90ec-3af37f5c265f","subtype":"script","commandType":"auto","position":1.1388888888888888,"command":"","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846578031,"submitTime":0,"finishTime":1768846578303,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["069a23bc-6ff3-4db1-b325-62c85ed2613c"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"906a486e-d5db-4ded-85b9-fe7a51e3d0b2"},{"version":"CommandV1","origId":4921811840474252,"guid":"daa6d216-277b-44cd-9504-b06a09b87cd9","subtype":"script","commandType":"auto","position":1.25,"command":"","commandVersion":104,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846578320,"submitTime":0,"finishTime":1768846578501,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["069a23bc-6ff3-4db1-b325-62c85ed2613c"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"ca134f5d-35c9-419f-8d8b-7fa1e19c3509"},{"version":"CommandV1","origId":4921811840474253,"guid":"aac46c8b-9622-4c7c-8c0b-7a742bc2fb7d","subtype":"script","commandType":"auto","position":1.3611111111111112,"command":"","commandVersion":1,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846578515,"submitTime":0,"finishTime":1768846578600,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["069a23bc-6ff3-4db1-b325-62c85ed2613c"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"4c42bf4a-cd7e-4685-aca7-a5d7b171f822"},{"version":"CommandV1","origId":4921811840474254,"guid":"50ebef9f-39de-4a3d-937a-1dfa29c1025d","subtype":"script","commandType":"auto","position":1.4722222222222223,"command":"","commandVersion":15,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846578615,"submitTime":0,"finishTime":1768846578801,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["069a23bc-6ff3-4db1-b325-62c85ed2613c"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"342ddd87-0e33-4fb8-af94-49a5efaaddc7"},{"version":"CommandV1","origId":4921811840474255,"guid":"d5251cf2-e3da-42d5-a221-8c5890c2542d","subtype":"script","commandType":"auto","position":1.5833333333333335,"command":"","commandVersion":2,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846578812,"submitTime":0,"finishTime":1768846578902,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["069a23bc-6ff3-4db1-b325-62c85ed2613c"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"6c15ae68-11f5-4ff6-a474-ea9358fa5dd2"},{"version":"CommandV1","origId":4921811840474256,"guid":"322d76a2-1dd6-4cb1-a96b-a4226f5b5a4e","subtype":"script","commandType":"auto","position":1.6944444444444446,"command":"","commandVersion":0,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846578913,"submitTime":0,"finishTime":1768846579003,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["069a23bc-6ff3-4db1-b325-62c85ed2613c"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"36ec6b70-4d7e-4ffc-a105-3a044815366f"},{"version":"CommandV1","origId":4921811840474257,"guid":"e1dfc303-bd12-49c0-891b-f08954bff030","subtype":"script","commandType":"auto","position":1.8055555555555556,"command":"","commandVersion":21,"state":"finished","results":{"type":"listResults","data":[],"arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[],"metadata":{"isDbfsCommandResult":false}},"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"baseErrorDetails":null,"workflows":[],"startTime":1768846579020,"submitTime":0,"finishTime":1768846579103,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":["069a23bc-6ff3-4db1-b325-62c85ed2613c"],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"latestAssumeRoleInfo":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{"rowLimit":10000,"byteLimit":2048000},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"tableResultSettingsMap":{},"nuid":"a8571e16-3f59-4f75-80d9-7c1076b4a14c"}],"dashboards":[],"guid":"1f743b53-8ada-4904-a898-182ea8f6da98","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":6358089889637130,"dataframes":["_sqldf"]}},"reposExportFormat":"JUPYTER","environmentMetadata":{"client":"4","base_environment":""},"computePreferences":null,"inputWidgetPreferences":null,"environmentSerializationPreferences":null}