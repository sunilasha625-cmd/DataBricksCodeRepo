{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b634dadd-24b7-4a5e-b503-e57f33879b2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#By Knowing this notebook, we can become an eligible \"Data Egress Developer/Engineer\"\n",
    "###We are writing data in Structured(csv), Semi Structured(JSON/XML), Serialized files (orc/parquet/delta) (Datalake), Table (delta/hive) (Lakehouse) format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b04876-823e-4e4c-b011-4931b2cb4471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Let's get some data we have already..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a81fac-7eae-4f23-9219-edeefab336eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Important Takeaways: CSV vs JSON\n",
    "\n",
    "## CSV (Structured Data)\n",
    "- **Format**: Plain text, tabular (rows & columns), 2D table  \n",
    "- **Schema**: Header row defines column names; order matters  \n",
    "- **Limitations**:  \n",
    "  - No native schema migration  \n",
    "  - Position-based, no metadata or versioning  \n",
    "  - Cannot handle column renames, reordering, or type changes automatically  \n",
    "- **Use Case**: Structured data with fixed schema  \n",
    "\n",
    "## JSON (Semi-Structured / Dynamic Data)\n",
    "- **Format**: JavaScript Object Notation (dictionary of key-value pairs)  \n",
    "- **Standard**: `{\"k1\":\"string\",\"k2\":123,\"k3\":true}`  \n",
    "  - Keys must be unique and in double quotes  \n",
    "  - Values can be string, number, boolean, object, array, or null  \n",
    "- **Advantages**:  \n",
    "  - Handles semi-structured/dynamic data  \n",
    "  - Flexible schema (column names, types, order can differ)  \n",
    "  - Common for API responses or dynamic sources  \n",
    "  - Efficient for data exchange and parsing  \n",
    "  - Supports nested/complex/hierarchical data  \n",
    "- **Use Case**: Dynamic data, API responses, real-time operations (e.g., clickstream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9432ebf-1255-481d-a80a-296bd2e0597b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extract\n",
    "ingest_df1=spark.read.csv(\"/Volumes/catalog2/database2/volume2/created_folder/custs_header\", header=True, inferSchema=True,samplingRatio=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f387999-f8ad-47c4-b624-d6b9f3459fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Writing the data in Builtin - different file formats & different targets (all targets in this world we can write the data also...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca2bafd-2798-4ff3-81aa-c3dfa098180b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Writing in csv (structured data (2D data Table/Frames with rows and columns)) format with few basic options listed below (Schema (structure) Migration)\n",
    "custid,fname,lname,age,profession -> custid~fname~lname~prof~age\n",
    "- header\n",
    "- sep\n",
    "- mode\n",
    "# Writing in CSV (Structured Data)\n",
    "\n",
    "**CSV is:**  \n",
    "- Structured, tabular data  \n",
    "- Rows and columns (2D table)  \n",
    "- Column order matters  \n",
    "- Column names come from the header row  \n",
    "\n",
    "**CSV supports schema migration only through explicit transformations, not natively.**\n",
    "\n",
    "**Why CSV does NOT support schema migration natively:**  \n",
    "- Plain text format  \n",
    "- Position-based  \n",
    "- No embedded schema  \n",
    "- No metadata or versioning  \n",
    "\n",
    "**Because of this, CSV cannot automatically handle:**  \n",
    "- Column renames  \n",
    "- Column reordering  \n",
    "- Data type changes  \n",
    "- Backward compatibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c27f37-0074-4d7b-8e8d-8211361118fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Correct way to do schema migration with CSV (Spark example)<br>\n",
    "Original CSV (v1)<br>\n",
    "custid,fname,lname,age,profession<br>\n",
    "\n",
    "Target CSV (v2)\n",
    "custid,fname,lname,prof,age<br>\n",
    "below is the code shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89773471-4d5f-40a2-84bd-dfea00c1df71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#it is just an example\n",
    "df_v2 = df_v1 \\\n",
    "  .withColumnRenamed(\"profession\", \"prof\") \\\n",
    "  .select(\"custid\", \"fname\", \"lname\", \"prof\", \"age\")\n",
    "\n",
    "df_v2.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \"~\") \\\n",
    "  .csv(\"/path/customers_v2\")\n",
    "\n",
    "  #This is schema migration done outside the CSV format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42708a5b-449d-4bc4-8568-03a6261568b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We are performing schema migration from comma to tilde delimiter\n",
    "ingest_df1.write.mode(\"overwrite\").csv(\"/Volumes/catalog2/database2/volume2/created_folder/writing_data/\",header=True,sep=\"/\")\n",
    "#4 modes of writing - append,overwrite,ignore,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0eec46e-5950-4ec0-84b8-590eb2ade4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We are performing schema migration by applying some transformations (this is our bread and butter that we learn exclusively further\n",
    "transformed_df=ingest_df1.select(\"custid\",\"fname\",\"lname\",\"age\",\"profession\").withColumnRenamed(\"fname\",\"firstname\").withColumnRenamed(\"lname\",\"lastname\")\n",
    "#Load\n",
    "transformed_df.write.mode(\"overwrite\").csv(\"/Volumes/catalog2/database2/volume2/created_folder/writing_data/\",header=True,sep=\"~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b78c33fe-58a5-4426-bc40-dedd7281eb15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "####2. Writing in json format with few basic options listed below\n",
    "path<br>\n",
    "mode\n",
    "- We did a schema migration and data conversion from csv to json format (ie structued to semi structured format)\n",
    "- json - we learn a lot subsequently (nested/hierarchical/complex/multiline...), \n",
    "- what is json - fundamentally it is a dictionary of dictionaries\n",
    "- json - java script object notation\n",
    "- Standard json format (can't be changed) - {\"k1\":\"string value\",\"k2\":numbervalue,\"k3\":v2} where key has to be unique & enclosed in double quotes and value can be anything\n",
    "- **when to go with json or benifits** - \n",
    "- a. If we have data in a semistructure format (with variable data format with dynamic schema)\n",
    "- eg. {\"custid\":4000001,\"profession\":\"Pilot\",\"age\":55,\"city\":\"NY\"}\n",
    "-     {\"custid\":4000001,\"fname\":\"Kristina\",\"lname\":\"Chung\",\"prof\":\"Pilot\",\"age\":\"55\"}\n",
    "- b. columns/column names or the types or the order can be different\n",
    "- c. json will be provided by the sources if the data is dynamic in nature (not sure about number or order of columns) or if the data is api response in nature.\n",
    "- d. json is a efficient data format (serialized/encoded) for performing data exchange between applications via network & good for parsing also & good for object by object operations (row by row operation in realtime fashion eg. amazon click stream operations)\n",
    "- e. json can be used to group or create hierarchy of data in a complex or in a nested format eg. https://randomuser.me/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74aac5b-02d2-45d5-8c0c-5c3e78990084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Writing Data in JSON Format\n",
    "\n",
    "- **JSON**: JavaScript Object Notation, fundamentally a dictionary of key-value pairs.\n",
    "- **Standard Format**: `{\"k1\":\"string\",\"k2\":123,\"k3\":true}`  \n",
    "  - Keys must be unique and in double quotes.  \n",
    "  - Values can be string, number, boolean, object, array, or null.\n",
    "- **Use Cases / Benefits**:\n",
    "  1. Ideal for **semi-structured or dynamic data**.  \n",
    "     Example:  \n",
    "     `{\"custid\":4000001,\"profession\":\"Pilot\",\"age\":55}`  \n",
    "     `{\"custid\":4000002,\"fname\":\"Kristina\",\"lname\":\"Chung\",\"prof\":\"Pilot\",\"age\":\"55\"}`\n",
    "  2. **Flexible schema**: column names, types, or order can differ.\n",
    "  3. Common for **API responses** or dynamic sources.\n",
    "  4. **Efficient for data exchange**: serialized format, easy parsing, supports row-by-row operations (e.g., clickstream data).\n",
    "  5. Supports **nested/complex/hierarchical data**, e.g., grouping objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fbcdc11-84ec-4195-b480-10e16d82ad22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ingest_df1 i am writing into path\n",
    "ingest_df1.write.json(path=\"/Volumes/workspace/wd36schema/ingestion_volume/target/jsonout\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c8d80b0-d697-4f41-a2d4-65a900488264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.json(\"/Volumes/catalog2/database2/volume2/created_folder/Key_based/copy/copy.json\")\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de546ab-db42-40d3-ae57-97b698d0140a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df1 data i am writing into json_copy direct\n",
    "df1.write.mode(\"overwrite\").json(\"/Volumes/catalog2/database2/volume2/created_folder/Key_based/copy/json_copy/\")\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01ad911-2064-4e84-a69d-f26afd5233e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2=spark.read.json(\"/Volumes/catalog2/database2/volume2/created_folder/Key_based/key_based.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f7df51-ce90-4691-b783-0fee21d4ebb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def2 data I am writing it into json_copy\n",
    "df2.write.mode(\"append\").json(\"/Volumes/catalog2/database2/volume2/created_folder/Key_based/copy/json_copy/\")\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fd9a99-b9fc-4f97-b5df-c13ef5903166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#in df1 the order is \n",
    "#{\"custid\":1,\"fname\":\"John\",\"lname\":\"Doe\",\"age\":30,\"profession\":\"Pilot\"}\n",
    "#in df2 the order is \n",
    "#{\"profession\":\"Doctor\",\"age\":40,\"lname\":\"Smith\",\"fname\":\"Jane\",\"custid\":2}\n",
    "#schema id different means order of the column is different abut still it woorked\n",
    "spark.read.json(\"/Volumes/catalog2/database2/volume2/created_folder/Key_based/copy/json_copy/\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77389e0b-b43e-457a-80ef-399d3a30aa4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3.Serialization (encoding in a more optimized fashion) & Deserialization File formats (Binary/Brainy File formats)\n",
    "Data Mechanics: \n",
    "1. encoding/decoding(machine format) - converting the data from human readable format to machine understandable format for performant data transfer (eg. Network transfer of data will be encoded)\n",
    "2. *compression/uncompression(encoding+space+time) - shrinking the data in some format using some libraries (tradeoff between time and size) (eg. Compress before store or transfer) - snappy is a good compression tech used in bigdata platform\n",
    "3. encryption (encoding+security) - Addition to encoding, encryption add security hence data is (performant+secured) (using some algos - SHA/MD5/AES/DES/RSA/DSA..)\n",
    "4. *Serialization (applicable more for bigdata) - Serialization is encoding + performant by saving space + processing intelligent bigdata format - Fast, Compact, Interoperable, Extensible (additional configs), Scalable (cluster compute operations), Secured (binary format)..\n",
    "5. *masking - Encoding of data (in some other format not supposed to be machine format) which should not be allowed to decode (used for security purpose)\n",
    "\n",
    "What are the (builtin) serialized file formats we are going to learn?\n",
    "orc\n",
    "parquet\n",
    "delta(databricks properatory)\n",
    "\n",
    "- We did a schema migration and data conversion from csv/json to serialized data format (ie structued to sturctured(internall binary unstructured) format)\n",
    "- We learn/use a lot/heavily subsequently\n",
    "- what is serialized - fundamentally they are intelligent/encoded/serialized/binary data formats applied with lot of optimization & space reduction strategies.. (encoded/compressed/intelligent)\n",
    "- orc - optimized row column format (Columnar formats)\n",
    "- parquet - tiled data format (Columnar formats)\n",
    "- delta(databricks properatory) enriched parquet format - Delta (modified/changes) operations can be performed (ACID property (DML))\n",
    "- format - serialized/encoded , we can't see with mere eyes, only some library is used deserialized/decoded data can be accessed as structured data\n",
    "- **when to go with serialized or benifits** - \n",
    "- a. For storage benifits for eg. orc will save 65+% of space for eg. if i store 1gb data it occupy 350mb space, with compression (snappy) it can improved more...\n",
    "- b. For processing optimization. Orc/parquet/delta will provide the required data alone if you query using Pushdown optimization .\n",
    "- c. Interoperability feature - this data format can be understandable in multiple environments for eg. bigquery can parse this data.\n",
    "- d. Secured\n",
    "- **In the projects/environments when to use what fileformats - we learn in detail later...\n",
    "| Format  | Schema Type              | Storage Efficiency | Analytics Performance | Updates Supported |\n",
    "|--------|--------------------------|--------------------|-----------------------|------------------|\n",
    "| CSV    | Structured               | Low                | Slow                  | No               |\n",
    "| JSON   | Semi-structured           | Low                | Slow                  | No               |\n",
    "| ORC    | Structured / Striped      | High               | Fast                  | Limited          |\n",
    "| Parquet| Structured / Nested       | High               | Very Fast             | Limited          |\n",
    "| Delta  | Structured / Evolving     | High               | Very Fast             | Highly           |\n",
    "| XML    | Semi-structured           | Low                | Slow                  | No               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8320c4f1-ca3a-46fe-9de1-f6ee824dcfbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. Serialization & Deserialization (Binary / Optimized File Formats)\n",
    "\n",
    "## Core Concepts\n",
    "- **Encoding / Decoding**:  \n",
    "  Converting human-readable data into machine-readable format for faster data transfer (e.g., network transmission).\n",
    "- **Compression / Uncompression**:  \n",
    "  Reduces data size using libraries (trade-off between time and space).  \n",
    "  Common in big data: **Snappy**.\n",
    "- **Encryption**:  \n",
    "  Adds security on top of encoding (e.g., SHA, MD5, AES, RSA).\n",
    "- **Serialization (Big Data Focus)**:  \n",
    "  - Encoding + compression + intelligent storage  \n",
    "  - Optimized for **speed, space, and scalability**  \n",
    "  - Binary, compact, interoperable, and secure\n",
    "- **Masking**:  \n",
    "  Data obfuscation for security; not meant to be decoded back.\n",
    "\n",
    "## Built-in Serialized File Formats\n",
    "- **ORC**\n",
    "- **Parquet**\n",
    "- **Delta (Databricks proprietary)**\n",
    "\n",
    "## What is Serialized Data?\n",
    "- Intelligent, binary, encoded formats with heavy optimization\n",
    "- Not human-readable\n",
    "- Accessed only through libraries (deserialization)\n",
    "- Used after converting CSV / JSON into optimized formats\n",
    "\n",
    "## Format Characteristics\n",
    "- **ORC**: Optimized columnar format (striped storage)\n",
    "- **Parquet**: Columnar, tiled data format\n",
    "- **Delta**: Enhanced Parquet with ACID support (DML operations)\n",
    "\n",
    "## When to Use Serialized Formats (Benefits)\n",
    "- **Storage Efficiency**:  \n",
    "  Saves significant space (e.g., ORC can reduce storage by ~65%+)\n",
    "- **Processing Performance**:  \n",
    "  Uses predicate pushdown to read only required data\n",
    "- **Interoperability**:  \n",
    "  Supported across platforms (e.g., Spark, BigQuery)\n",
    "- **Security**:  \n",
    "  Binary and encrypted-friendly formats\n",
    "\n",
    "## File Format Comparison\n",
    "\n",
    "| Format   | Schema Type              | Storage Efficiency | Analytics Performance | Updates Supported |\n",
    "|---------|--------------------------|--------------------|-----------------------|------------------|\n",
    "| CSV     | Structured               | Low                | Slow                  | No               |\n",
    "| JSON    | Semi-structured           | Low                | Slow                  | No               |\n",
    "| ORC     | Structured / Striped      | High               | Fast                  | Limited          |\n",
    "| Parquet | Structured / Nested       | High               | Very Fast             | Limited          |\n",
    "| Delta   | Structured / Evolving     | High               | Very Fast             | High (ACID)      |\n",
    "| XML     | Semi-structured           | Low                | Slow                  | No               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "230bfc18-9fa4-442b-bae0-65f7f31b9166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Serialization & Deserialization\n",
    "\n",
    "## Binary / Optimized (\"Brainy\") File Formats\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is Serialization?\n",
    "\n",
    "**Serialization** is the process of converting data from a **human-readable or in-memory representation** into a **compact, machine-friendly (binary) format** so it can be:\n",
    "\n",
    "* Stored on disk\n",
    "* Transferred over a network\n",
    "* Processed efficiently by distributed systems\n",
    "\n",
    "### Simple Definition\n",
    "\n",
    "> **Serialization = Object / Data → Byte Stream (Binary Format)**\n",
    "\n",
    "### Example\n",
    "\n",
    "**In-memory / human-readable data (JSON-like):**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": 101,\n",
    "  \"name\": \"Sunil\",\n",
    "  \"salary\": 50000\n",
    "}\n",
    "```\n",
    "\n",
    "**After serialization (binary form – conceptual):**\n",
    "\n",
    "```\n",
    "01100101 00000000 01010011 01110101 01101110 01101001 01101100 ...\n",
    "```\n",
    "\n",
    "This binary representation is **not human-readable**, but machines can read and process it very fast.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What is Deserialization?\n",
    "\n",
    "**Deserialization** is the reverse process of serialization.\n",
    "\n",
    "### Simple Definition\n",
    "\n",
    "> **Deserialization = Byte Stream (Binary Format) → Object / Data**\n",
    "\n",
    "### Example\n",
    "\n",
    "* A Spark job reads a Parquet file\n",
    "* Spark **deserializes** the binary data\n",
    "* Converts it back into:\n",
    "\n",
    "  * Rows\n",
    "  * Columns\n",
    "  * DataFrame objects\n",
    "\n",
    "This allows filtering, aggregation, and transformations.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why Serialization is Needed\n",
    "\n",
    "Serialization is essential in big data and distributed systems due to the following reasons:\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "1. **Performance**\n",
    "\n",
    "   * Binary formats are faster than text formats (CSV, JSON)\n",
    "\n",
    "2. **Smaller Storage Size**\n",
    "\n",
    "   * Binary data consumes less disk space\n",
    "\n",
    "3. **Efficient Network Transfer**\n",
    "\n",
    "   * Less data is transferred between nodes\n",
    "\n",
    "4. **Schema Awareness**\n",
    "\n",
    "   * Some formats store column names and data types internally\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Binary / \"Brainy\" File Formats\n",
    "\n",
    "> \"Brainy\" is an informal term meaning **intelligent and optimized binary formats**.\n",
    "\n",
    "These formats are:\n",
    "\n",
    "* Binary (not human-readable)\n",
    "* Schema-aware\n",
    "* Compressed\n",
    "* Optimized for analytics and performance\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Common Binary File Formats\n",
    "\n",
    "| Format               | Type             | Usage                        |\n",
    "| -------------------- | ---------------- | ---------------------------- |\n",
    "| **Parquet**          | Columnar binary  | Analytics, Spark, Databricks |\n",
    "| **ORC**              | Columnar binary  | Hive, high compression       |\n",
    "| **Avro**             | Row-based binary | Streaming, Kafka             |\n",
    "| **Protocol Buffers** | Binary           | Fast network communication   |\n",
    "| **Thrift**           | Binary           | Cross-language serialization |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Text vs Binary File Formats\n",
    "\n",
    "| Feature            | CSV / JSON | Parquet / ORC / Avro |\n",
    "| ------------------ | ---------- | -------------------- |\n",
    "| Human-readable     | Yes        | No                   |\n",
    "| File size          | Large      | Small                |\n",
    "| Read/Write speed   | Slow       | Fast                 |\n",
    "| Compression        | External   | Built-in             |\n",
    "| Schema support     | Weak       | Strong               |\n",
    "| Analytics friendly | No         | Yes                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Spark / Databricks Example\n",
    "\n",
    "### Writing Data (Serialization)\n",
    "\n",
    "```python\n",
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .parquet(\"/mnt/curated/customer\")\n",
    "```\n",
    "\n",
    "What happens internally:\n",
    "\n",
    "* Spark serializes the DataFrame\n",
    "* Converts rows into Parquet binary format\n",
    "* Applies compression\n",
    "* Writes optimized files to storage\n",
    "\n",
    "---\n",
    "\n",
    "### Reading Data (Deserialization)\n",
    "\n",
    "```python\n",
    "df = spark.read.parquet(\"/mnt/curated/customer\")\n",
    "```\n",
    "\n",
    "What happens internally:\n",
    "\n",
    "* Spark reads Parquet binary files\n",
    "* Deserializes data into objects\n",
    "* Creates a DataFrame for processing\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Real-Time Project Context\n",
    "\n",
    "* **Bronze layer**: Raw data (CSV / JSON)\n",
    "* **Silver layer**: Cleaned & structured (Parquet / ORC)\n",
    "* **Gold layer**: Aggregated & analytics-ready (Parquet)\n",
    "\n",
    "Serialization happens while moving from **Bronze → Silver → Gold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21dddb45-ab26-46c7-a5da-9b047ea3a0b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_df1.write.orc(\"/Volumes/catalog2/database2/volume2/Write_bascics/orcoutput\",mode='overwrite',compression='zlib')#by default orc/parquet uses snappy compression\n",
    "spark.read.orc(\"/Volumes/catalog2/database2/volume2/Write_bascics/orcoutput\").show(2)#uncompression + deserialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2b79c8-cc4a-4179-a0a1-6e0885186665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.orc(\"/Volumes/catalog2/database2/volume2/Write_bascics/orcoutput\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3fb16a7-62db-4f75-90e3-dc8329da82de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Orc/Parquet follows WORM feature (Write Once Read Many)\n",
    "ingest_df1.write.mode(\"overwrite\").options(compression=\"snappy\").parquet(\"/Volumes/catalog2/database2/volume2/Write_bascics/parquetoutput\")#by default orc/parquet uses snappy compression\n",
    "spark.read.parquet(\"/Volumes/catalog2/database2/volume2/Write_bascics/parquetoutput\").show(2)#uncompression + deserialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0d31e9-e26e-4cf7-b8fb-6609ebbe13ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Delta follows WMRM feature (Write Many Read Many)\n",
    "ingest_df1.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/catalog2/database2/volume2/Write_bascics/deltaoutput\")\n",
    "spark.read.format(\"delta\").load(\"/Volumes/catalog2/database2/volume2/Write_bascics/deltaoutput\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4216da-3743-4c15-ae4c-c7650730442e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_df1.write.mode(\"overwrite\").xml(\"/Volumes/catalog2/database2/volume2/Write_bascics/xmloutput\",rowTag=\"cust\")\n",
    "spark.read.xml(\"/Volumes/catalog2/database2/volume2/Write_bascics/xmloutput\",rowTag=\"cust\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4228d636-de76-42d2-9cf2-73e577e45c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Basic_Write_ops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
