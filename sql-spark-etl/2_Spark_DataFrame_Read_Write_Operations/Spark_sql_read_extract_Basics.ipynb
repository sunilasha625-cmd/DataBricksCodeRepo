{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8547cf42-dc5e-4bd4-9e17-08b9f2ffebab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### First Let's understand the basic Catalog + Volume Feature of Databricks \n",
    "Filesystem Hierarchy of Volume in Databricks (DBFS)?<br>\n",
    "Catalog -> /OurWorkspace/catalog/schema(database)/volume/folder/data files<br>\n",
    "Tables Hierarchy of Databricks?<br>\n",
    "Catalog -> /OurWorkspace/catalog/schema(database)/tables/data(dbfs filesystem/some other filesystems)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23f2d460-bd50-484f-aada-a90dbe192091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####If we need to create schema/volume/folder programatically, follow the below steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea44626-8ba7-417a-b8da-83fb69eddeca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS Catalog1;\n",
    "USE CATALOG Catalog1;\n",
    "CREATE SCHEMA IF NOT EXISTS Schema1;\n",
    "USE SCHEMA Schema1;\n",
    "CREATE VOLUME IF NOT EXISTS Volume1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d9df3c3-edf8-4bae-858a-19f07944d558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DROP CATALOG → Works only if catalog is fully empty<br>\n",
    "DROP CATALOG CASCADE → Works even if catalog contains objects, because it force-deletes them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "302a71bd-5adc-46b8-9fa8-16fd565ec52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP CATALOG catalog1 CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8175343c-ac99-4c8d-ba50-fe8130d856f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS Catalog2;\n",
    "CREATE DATABASE IF NOT EXISTS Catalog2.DataBase2;\n",
    "CREATE VOLUME IF NOT EXISTS Catalog2.DataBase2.Volume2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91ab3df-ead6-43f9-8ab9-4ac74655dec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:///Volumes/workspace/default/healthcare/created_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a52331d-0990-4228-b2ce-1527ff7b935e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Spark SQL<br>\n",
    "###1.E(Extract) \n",
    "L(Load)<br>\n",
    "Inbuilt libraries sources/targets & Inbuilt data Formats<br>\n",
    "2. Bread & Butter (T(Transformation) A(Analytical))<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3eaaff0-1a4b-4141-907c-373609fd1164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Learn How to Create Dataframes from filesystem using different options\n",
    "Download the data from the below drive url <br>\n",
    "https://drive.google.com/drive/folders/1Tw7V9eBtUxy0xQMW38z3-bzWI_ewzLm6?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ec4677-09b6-4073-bebd-87c9949a91c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###How Do We Write a Typical Spark Application (Core(Obsolute),SQL(Important),Streaming(Mid level important))\n",
    "####Before we Create Dataframe/RDD, what is the prerequisite? We need to create spark session object by instantiating sparksession class (by default databricks did that if you create a notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fab536a-9289-4f0f-9df0-71b7d787f9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)#already instantiated by databricks\n",
    "spark1=SparkSession.builder.appName(\"Spark\").getOrCreate()\n",
    "print(spark1)#we instantiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86451839-662c-439a-ba97-f1c92cd0da28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Create a DBFS volume namely commondata and upload the above data in that volume.<br>\n",
    "- What are other FS uri's available? file:///, hdfs:///, dbfs:///, gs:///, s3:///, adls:///, blob:///"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d35885-0a16-4611-9117-0c3bca3184b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###How to Read/Extract the data from the filesytem and load it into the distributed memory for further processing/load - using diffent methodologies/options from different sources(fs & db) and different builtin formats (csv/json/orc/parquet/delta/tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f2511d-65f0-4c8b-adcc-414c583ee1d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Screenshot 2025-12-11 193250.png](./Screenshot 2025-12-11 193250.png \"Screenshot 2025-12-11 193250.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c840128f-6531-4d2c-ab7b-b66248dfd52e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "#If I do not specify any options in the CSV function, what defaults does Spark use?\n",
    "#1. Delimiter->By default, Spark uses a comma , as the field separator.\n",
    "#Example default: sep=\",\"\n",
    "\n",
    "#2. Column Names (Header)->If the CSV file does not have a header, Spark automatically assigns column names as:_c0, _c1, _c2, ..., _cn\n",
    "#This is because the default option is:\n",
    "#eader = false\n",
    "\n",
    "#3. Data Types->By default, Spark reads all columns as string type because:\n",
    "#inferSchema = false\n",
    "#(Spark does not try to detect column types unless you enable it.\n",
    "\n",
    "#we have more options to see further for above all\n",
    "\n",
    "df1=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/custs\")\n",
    "df1.show(3)#display with produce output in a dataframe format\n",
    "print(df1.printSchema())\n",
    "display(df1)#display with produce output in a beautified table format, specific to databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "939b4899-6172-46e8-b437-39a5c625b43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "csv_df1=spark.read.csv\n",
    "(path: PathOrPaths, schema: Optional[Union[StructType, str]]=None, sep: Optional[str]=None, encoding: Optional[str]=None, quote: Optional[str]=None, escape: Optional[str]=None, comment: Optional[str]=None, header: Optional[Union[bool, str]]=None, inferSchema: Optional[Union[bool, str]]=None, ignoreLeadingWhiteSpace: Optional[Union[bool, str]]=None, ignoreTrailingWhiteSpace: Optional[Union[bool, str]]=None, nullValue: Optional[str]=None, nanValue: Optional[str]=None, positiveInf: Optional[str]=None, negativeInf: Optional[str]=None, dateFormat: Optional[str]=None, timestampFormat: Optional[str]=None, maxColumns: Optional[Union[int, str]]=None, maxCharsPerColumn: Optional[Union[int, str]]=None, maxMalformedLogPerPartition: Optional[Union[int, str]]=None, mode: Optional[str]=None, columnNameOfCorruptRecord: Optional[str]=None, multiLine: Optional[Union[bool, str]]=None, charToEscapeQuoteEscaping: Optional[str]=None, samplingRatio: Optional[Union[float, str]]=None, enforceSchema: Optional[Union[bool, str]]=None, emptyValue: Optional[str]=None, locale: Optional[str]=None, lineSep: Optional[str]=None, pathGlobFilter: Optional[Union[bool, str]]=None, recursiveFileLookup: Optional[Union[bool, str]]=None, modifiedBefore: Optional[Union[bool, str]]=None, modifiedAfter: Optional[Union[bool, str]]=None, unescapedQuoteHandling: Optional[str]=None) -> \"DataFrame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e42e98c-db3e-400c-9e1a-701d2c3e4f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sample data of custs_1\n",
    "4000001,Kristina,Chung,55,Pilot<br>\n",
    "4000002,Paige,Chen,77,Teacher\n",
    "\n",
    "Sample data of custs_header<br>\n",
    "custid,fname,lname,age,profession<br>\n",
    "4000001,Kristina,Chung,55,Pilot<br>\n",
    "4000002,Paige,Chen,77,Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfbb4395-63a3-4a9f-878e-1db112b5e27d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "#1. Header Concepts (Either we have define the column names or we have to use the column names from the data)\n",
    "#Important option is...\n",
    "#By default, Spark assigns column names as _c0, _c1, _c2, ..., _cn because it does not treat the first row as a header (header = false).\n",
    "#header=True means we are asking spark to take the first row as header and not as a data\n",
    "df1=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/custs_header\",header=True)\n",
    "df1.show(3)\n",
    "#df1.write.csv(\"/Volumes/workspace/default/healthcare/created_folder/copy.csv\",header=True,mode=\"overwrite\")\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers.\n",
    "df2=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/cust_1.txt\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "df2.show(2)#display with produce output in a dataframe format\n",
    "print(df2.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799efb03-edb4-4157-83c1-d4b5c117db1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Printing Schema (equivalent to describe table)\n",
    "df1.printSchema()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd3ffa2-933a-4c91-a9ab-04722a73d2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Inferring Schema \n",
    "# (Performance Consideration: Use this function causiously because it scans the entire data by immediately evaluating and executing\n",
    "# hence, not good for large data or not good to use on the predefined schema dataset)\n",
    "#sample data\n",
    "#4004979,Tara,Drake,32,\n",
    "#4004980,Earl,Hahn,34,Human resources assistant\n",
    "#4004981,Don,Jones,THIRTY SIX,Lawyer\n",
    "df2=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/cust_1.txt\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "df2.where(\"id in (4000009,4000010)\").show(2)\n",
    "print(df2.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dae761a-e7f7-4195-a164-f3eafa0d3bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/cust_1.txt\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(df2.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47843e10-21c4-44a9-ace6-871f9d9a7316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Using delimiter or seperator option\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/delimiter_sample.txt\",header=True,sep=\"~\")\n",
    "csv_df1.show(2)\n",
    "csv_df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf12336-7f96-429f-9179-5ea7b6def141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5. Using different options to create dataframe with csv and other module... (2 methodologies with 3 ways of creating dataframes)\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/delimiter_sample.txt\",inferSchema=True, header=True,sep=\"~\")\n",
    "csv_df1.show(2)\n",
    "#or another way of creating dataframe (from any sources whether builtin or external)...\n",
    "#option can be used for 1 or 2 option...\n",
    "spark.read.option(\"header\",\"True\").option(\"sep\",\"~\").option(\"inferSchema\",\"True\").format(\"csv\").csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/delimiter_sample.txt\").show(2)\n",
    "#options can be used for multiple options in one function as a parameter...\n",
    "spark.read.options(header=\"True\",sep=\"~\",inferSchema=\"True\").format(\"csv\").csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/delimiter_sample.txt\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfba0bb4-d8eb-495b-b2b4-1f062658ea78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generic way of read and load data into dataframe using fundamental options from built in sources (csv/orc/parquet/xml/json/table) (inferschema, header, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e336f630-cd2c-4e6a-bc93-3eeb1cba3130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/delimiter_sample.txt\",inferSchema=True,header=True,sep='~')\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfa5e051-340b-4bb0-ab08-7ed52a22addf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generic way of read and load data into dataframe using extended options from external sources (bigquery/redshift/athena/synapse) (tmpfolder, access controls)\n",
    "ex, I can use format(\"bigquery\") but in using fundamental option it is not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b0fdfe-e4ac-44a5-85bf-d98f627a6f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df3=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/delimiter_sample.txt\")\n",
    "csv_df3.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4230d0d6-4830-4d33-ad5b-7969d1720315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading data from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6e3f5f-a638-411c-b2a5-0d93fbc93ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/cust*\",inferSchema=True,header=True,sep='~')\n",
    "print(csv_df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d18e760-ed9e-4cd3-926a-c7dd2e6c4865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=[\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/delimiter_sample.txt\",\"dbfs:///Volumes/catalog2/database2/volume2/created_folder/cust_1.txt\"],inferSchema=True,header=True,sep=',')\n",
    "print(csv_df1.count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8630619113636402,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_sql_read_extract_Basics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
